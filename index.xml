<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>XP&#39;s Blog</title>
        <link>https://blog.xpgreat.com/</link>
        <description>Recent content on XP&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Fri, 10 Jun 2022 12:27:12 +0200</lastBuildDate><atom:link href="https://blog.xpgreat.com/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>cannot import name ‘container_abcs‘ from ‘torch._six‘错误的解决方法</title>
        <link>https://blog.xpgreat.com/p/torch_six_issue/</link>
        <pubDate>Fri, 10 Jun 2022 12:27:12 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/torch_six_issue/</guid>
        <description>&lt;p&gt;在升级PyTorch 1.9以后用一些老版本的开源代码的时候会出现这种问题。与此类似的还有：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;cannot import name ‘container_abcs‘ from ‘torch._six‘
cannot import name ‘int_classes‘ from ‘torch._six‘
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;解决方法是定位到相应的import的位置，然后把原来的import改成：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# from torch._six import container_abcs as container_abcs&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# from torch._six import int_classes as int_classes&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;TORCH_MAJOR&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__version__&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;TORCH_MINOR&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__version__&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TORCH_MAJOR&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TORCH_MINOR&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch._six&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;container_abcs&lt;/span&gt;
    &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch._six&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;int_classes&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;int_classes&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;collections.abc&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;container_abcs&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;int_classes&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;DIG包，报错环境信息：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;torch 1.11.0
torchvision 0.12.0
torch-sparse 0.6.13
torch-scatter 2.0.9
torch-geometric 1.7.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        </item>
        <item>
        <title>LRP简介</title>
        <link>https://blog.xpgreat.com/p/lrp/</link>
        <pubDate>Sun, 29 Aug 2021 10:56:18 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/lrp/</guid>
        <description>&lt;p&gt;机器学习的可解释性一直以来都是一个大问题，模型中的海量权重和连接关系让机器学习一直被视为黑盒模型。为了解决这个问题，Explainable AI （XAI）是一个前沿的研究方向。关于可解释性的研究，推荐这篇&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/9369420&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;综述&lt;/a&gt;。本文简要介绍解释深度神经网络（DNN）的算法LRP（Layerwise Relevance Propagation）。&lt;/p&gt;
&lt;h2 id=&#34;深度泰勒分解deep-taylor-decomposition&#34;&gt;深度泰勒分解（Deep Taylor Decomposition）&lt;/h2&gt;
&lt;p&gt;在介绍LRP之前，首先需要简单了解一下&lt;a class=&#34;link&#34; href=&#34;https://www.sciencedirect.com/science/article/pii/S0031320316303582&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;深度泰勒分解&lt;/a&gt;的思想方法。只要学过高数的都知道一种近似计算函数值的方法，即泰勒公式：&lt;/p&gt;
&lt;p&gt;$$
f(x) = \sum_0^{+\infin} \frac{f^{(n)}(a)}{n!}(x-a)^n \\= f(a) + \frac{f&#39;(a)}{1!}(x-a) + \frac{f&#39;&#39;(a)}{2!}(x-a)^2 + &amp;hellip;
$$&lt;/p&gt;
&lt;p&gt;类似的，我们可以对一个深度神经网络进行泰勒分解，假定\(f(x)\)是神经网络学习到的预测函数，对它进行一阶泰勒展开，并假设参照点的函数值为0，则：&lt;/p&gt;
&lt;p&gt;$$
f(x) \approx \sum_1^D \frac{\partial f}{\partial x_{(d)}}(x_0)(x_{(d)}-x_{0(d)})
$$&lt;/p&gt;
&lt;p&gt;其中\(d\)是向量\(x\)的对应维度的值。上面的式子可以看做梯度乘以与零点的差，可以用它来构造可解释性heatmap。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/dtd_img.jpeg&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/dtd_img.jpeg&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;所以想要解释模型的预测结果，需要结合模型的属性和输入的数据，在DTD中是将模型梯度和输入数据相乘，而在LRP中是通过模型的权重（也就是各层的梯度）和模型中间结果来计算加权平均，从最后一层传导到输入层，得到模型的解释。&lt;/p&gt;
&lt;h2 id=&#34;lrp&#34;&gt;LRP&lt;/h2&gt;
&lt;p&gt;LRP包括两步，第一步是传统的使用神经网络向后传播计算预测值，在这个过程中保存下所有的中间值；第二步是把神经网络反过来，从后向前依层计算各层的Relevance score，最终传导到最初的输入层，而输入层对应的Relevance score则可以被用来解释模型的预测结果，可以用来绘制heat map、高亮文本等等。LRP的算法过程可以表示如下：&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/lrp.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/lrp.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;code&gt;relprop&lt;/code&gt;使用\(l-1\)层的中间值\(a^{(l-1)}\)、后一层的relevance score \(R^(l)\)和\(l\)层的权重（记为\(f_l\)）。一般有三种方式进行传播：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LRP-0: 最简单的方式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
R_j = \sum_k \frac{a_j w_{jk}}{\sum_{0,i} a_i w_{ik}} R_k
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LRP-\(\epsilon\): 分母上加入了一个微小值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
R_j = \sum_k \frac{a_j w_{jk}}{\epsilon + \sum_{0,i} a_i w_{ik}} R_k
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LRP-\(\gamma\): 计算权重时多加/减一部分正项，以达到偏向正向影响或负向影响的效果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
R_j = \sum_k \frac{a_j (w_{jk}+\gamma w_{jk}^+)}{\sum_{0,i} a_i (w_{ik}+\gamma w_{ik}^+)} R_k
$$&lt;/p&gt;
&lt;p&gt;注意这里面sum的下标\(0,i\)的意思是前一层所有的neuron\(i\)加上\(w_{0k}\)的bias项，\(a_0 = 1\)。LRP的过程可以表示如下图：
&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/lrp_illu.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/lrp_illu.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;LRP的理论证明可以参考这篇&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/9369420&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;综述&lt;/a&gt;的第五章，LRP的本质上等价于特殊情况下的DTD。值得注意的是，上面的三种表达中，LRP-\(\gamma\)和LRP-\(\epsilon\)基于深度整流网络（deep rectifier network），而对于其他类型的激活函数，LRP的表达则更为复杂。&lt;/p&gt;</description>
        </item>
        <item>
        <title>GNN 图神经网络简介</title>
        <link>https://blog.xpgreat.com/p/gnn/</link>
        <pubDate>Fri, 27 Aug 2021 23:40:00 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/gnn/</guid>
        <description>&lt;p&gt;图神经网络是是一种基于图结构的广义神经网络。其实传统的神经网络也是可以处理图数据，只需要进行前期合理的embedding即可，那么为什么还需要GNN呢？GNN其实属于一种embedding算法，它通过在整张图上&lt;strong&gt;传递、转换、聚合&lt;/strong&gt;节点的特征信息，从而生成单节点的embedding。本文简要介绍GNN，力求通俗易懂，需要更详细的研究GNN的话，推荐一篇2019年的综述&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1901.00596&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;A Comprehensive Survey on Graph Neural Networks&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;基本概念&#34;&gt;基本概念&lt;/h2&gt;
&lt;h3 id=&#34;图&#34;&gt;图&lt;/h3&gt;
&lt;p&gt;图是由节点（node，或顶点vertex）和边（edge）构成的一种数据结构，节点可以看作是对象，边可以看作是对象之间的关系。进一步的，每个节点可以有自己的信息，边也可以加上权重和方向以表示关系的不同。所以，最基本的图可以表示为\(V,E\)，其中\(V\)是vertex的集合，\(E\)是edge的集合。简单起见，先不考虑V自身的信息和E的权重和方向。&lt;/p&gt;
&lt;p&gt;图的表示一般有两种方式：一种是表示为节点对:\((a, b)\)，表示\(a, b\)之间有一条边（暂时不考虑方向）。容易发现，这种表示在边很多的时候会重复存储很多节点信息，可能带来空间的浪费。另一种方式是表示为邻接矩阵（adjacency matrix），即矩阵的横轴和纵轴分别代表边的两端，如果位置\((a, b)\的值\(\Lambda_{a, b} = 1\，即\(a, b\)之间有一条边。使用邻接矩阵的好处是在边很多的时候可以节省空间，可以方便的加上边的权重信息（对应位置的值为2，3等等），也可以利用矩阵的结构，方便的进行并行运算（对机器学习来说非常重要）。但也会存在一个问题，即图很稀疏的时候（边很少），会存在大量的0值，浪费空间，所幸常用的库都做了sparse matrix的各种实现，既节省空间，又保持了矩阵的形式。&lt;/p&gt;
&lt;p&gt;一个节点的邻居可以表示为\(N(a)\)，是与该节点之间存在边的所有节点的集合。一个节点的度（degree）是这个节点邻居的个数，即\(card(N(a))\).&lt;/p&gt;
&lt;h3 id=&#34;神经网络&#34;&gt;神经网络&lt;/h3&gt;
&lt;p&gt;神经网络可以看成是多层的加权求和运算，在各个层之间加入激活函数（一般是非线性函数，以提高模型的表示能力），可以写成：&lt;/p&gt;
&lt;p&gt;$$
X_1 = activiation(W_0X_0) \\&lt;br&gt;
X_2 = activiation(W_1X_1) \\&lt;br&gt;
&amp;hellip;\\&lt;br&gt;
X_n = activiation(W_{n-1}X_{n-1}) \\&lt;br&gt;
Y = output_transformation(W_nX_n)
$$&lt;/p&gt;
&lt;p&gt;可以使用训练数据集来训练神经网络，也就是获得一组能反应训练集的特征的权重\(W_{0~n)\)，这样可以用于预测。为优化权重使用梯度下降算法。此外还有很多tricks，比如dropout。&lt;/p&gt;
&lt;h2 id=&#34;gnn&#34;&gt;GNN&lt;/h2&gt;
&lt;p&gt;GNN可以看作是对每一个节点分别使用&lt;strong&gt;同一个&lt;/strong&gt;神经网络来获得这个节点的输出值。在神经网络的每一层里，用该节点自身的邻居的状态来更新该节点的状态，最终状态则是每个节点的输出值。如果用于图的分类问题，可以对全部节点的输出值进行聚合，得到最终的结果。&lt;/p&gt;
&lt;p&gt;对每一层的运算过程我们可以抽象成两步：聚集（aggregate）和合并（combine）。对于一个有\(n\)个节点的图，我们将图的邻接矩阵记为\(\Lambda\in \mathbb N^{n \times n}\)，\(t\)层的各个节点的状态记为\(H \in \mathbb N^{n \times d_t}\)，每个节点的状态可能有多个维度，则这两步可以写成：&lt;/p&gt;
&lt;p&gt;$$
aggregation: Z_t = \Lambda H_{t-1}
combine: H_t = \mathcal C_t(Z_t)
$$&lt;/p&gt;
&lt;p&gt;其中\(\mathcal C_t\)是一个合并方程，对\(Z_t\)的每一行用一个权重矩阵\(W_t \in \mathbb N^{d_{t-1} \times d_t}\)相乘，将状态矩阵转化到新的维度\(H_t \in \mathbb N^{n \times d_t}\)。将所有的步骤合并起来，并在最后加入一个readout function，整个网络可以写成：&lt;/p&gt;
&lt;p&gt;$$
f(\Lambda, H_0) = g(H_T(\Lambda, H_{T-1}(\Lambda, H_{T-2}(\Lambda, &amp;hellip; H_1(\Lambda, H_0)))))
$$&lt;/p&gt;
&lt;p&gt;输入一个邻接矩阵和初始状态，输出一个预测值，当然也可以去掉readout一步，获得每个节点的状态\(H_T\)作为输出。&lt;/p&gt;</description>
        </item>
        <item>
        <title>Hamiltonian Monte Carlo算法</title>
        <link>https://blog.xpgreat.com/p/hmc/</link>
        <pubDate>Tue, 17 Aug 2021 20:34:54 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/hmc/</guid>
        <description>&lt;p&gt;MCMC（Markov-Chain Monte-Carlo）算法用于生成给定分布的样本。在很多时候，我们想计算一个复杂分布的函数期望值，但用解析的，求解积分的方法，是极其困难的，对于有些分布甚至是不可能的。所以我们使用一些采样方法，例如Gibbs sampling，Metropolis-Hasting算法。本文介绍Hamiltonian Monte-Carlo算法。&lt;/p&gt;
&lt;h2 id=&#34;目标和问题&#34;&gt;目标和问题&lt;/h2&gt;
&lt;p&gt;给定一个分布\(\pi(x)\)，计算期望：\(\mathbb E_\pi[f]=\int f(q)\pi(q)dq\)，而直接计算积分是不可行的。因此我们计算期望的一个近似：\(\mathbb E_\pi[f] \approx \frac{1}{N}\sum_i f(x_i)\)，使得\(P(x_i) = \pi(x_i)\)。&lt;/p&gt;
&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;
&lt;p&gt;从上面的式子可以看出，\(f(q)\)对应的\(\pi(q)dq\)越大，它对最终结果的贡献也越大。所以，如果我们可以从高\(\pi(q)dq\)区域采样，效率会更高。这个区域也被称为&lt;strong&gt;typical set&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/hmc_1.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/hmc_1.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			alt=&#34;As q moves farer to mode, the dq rises rapidly, probability decreases. &#34;&gt;
	&lt;/a&gt;
	
	&lt;figcaption&gt;As q moves farer to mode, the dq rises rapidly, probability decreases.&lt;/figcaption&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;\(q\)运动到距离mode越远的地方，\(dq\)急剧上升，同时概率下降。&lt;/p&gt;
&lt;p&gt;但是，当\(q\)是高维数据，\(dq\)的体积（volume）变得相对越来越小，我们采样也不容易落到这个typical set里。在绝大多数的时候，我们随机采样的\(q\)落到\(\pi(q)dq\)非常小的地方，这让我们计算期望十分低效。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/hmc_2.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/hmc_2.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			alt=&#34;HMC%20feb2b437ff5644c898eb089ffd8f36cc/Untitled%201.png&#34;&gt;
	&lt;/a&gt;
	
	&lt;figcaption&gt;HMC%20feb2b437ff5644c898eb089ffd8f36cc/Untitled%201.png&lt;/figcaption&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;metropolis-hasting&#34;&gt;Metropolis-Hasting&lt;/h2&gt;
&lt;p&gt;我们可以利用Markov-chain-Monte-Carlo模型来采样。假设我们有一个转移矩阵\(\mathbb T(q&#39;|q)\)，它定义了从\(q\)转移到\(q&#39;\)的概率密度。所有的MCMC转移必须满足**细致平衡（detailed balanced）**条件，即从\(q\)转移到\(q&#39;\)必须是可逆的：$\pi(q)\mathbb T(q&#39;|q) = \pi(q&#39;)\mathbb T(q|q&#39;)$。然而直接找到一个合适的\(\mathbb T\)十分困难，所以我们利用一个acceptance coefficient \(a(q&#39;|q)=\min{1, \frac{\pi(q&#39;)\mathbb Q(q|q&#39;)}{\pi(q)\mathbb Q(q&#39;|q)}}\)，\(\mathbb T=a\mathbb Q\)，这样细致平衡条件就达到了：&lt;/p&gt;
&lt;p&gt;$$
\pi(q)\mathbb Q(q&#39;|q) a(q&#39;|q) = \min{\pi(q)\mathbb Q(q&#39;|q), \pi(q&#39;)\mathbb Q(q|q&#39;)}=\pi(q&#39;)\mathbb Q(q|q&#39;) a(q|q&#39;)
$$&lt;/p&gt;
&lt;p&gt;如果 \(\mathbb Q(q&#39;|q) = \mathcal N(q&#39;|q,\mathbf \Sigma)\)，这个算法被称为 &lt;strong&gt;Random-Walk Metropolis&lt;/strong&gt;。实践中如果我们可以采样 \(x\sim \mathcal N(0,1)\)并用 \(q&#39; = \mathbf \Sigma ^{0.5} x + q\) 来转化。因为 \(\mathbb Q(q&#39;|q)=\mathbb Q(q|q&#39;)\)，acceptance coefficient 变成了 \(a(q&#39;|q)=\min{1, \frac{\pi(q&#39;)}{\pi(q)}}\)。代码如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;qprime&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sqrt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;normal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;q&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uniform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dist&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;qprime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dist&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;q&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
	&lt;span class=&#34;n&#34;&gt;q&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;qprime&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的方法还是没有解决高维稀疏的问题。高维\(q\) 会导致acceptance coefficient太低（\(\pi(q&#39;)\) is too small）以至于上面的proposal几乎不会被接受。为了解决这个问题，可以考虑使用概率的梯度来更加靠近高\(\pi\)的区域，即typical set。&lt;/p&gt;
&lt;h2 id=&#34;mala-metropolis-adjusted-langevin-algorithm&#34;&gt;MALA (Metropolis-adjusted Langevin Algorithm)&lt;/h2&gt;
&lt;p&gt;MALA 用到了概率方程的梯度和一个随机噪音来更快地逼近typical set。考虑over-damped Langevin Itō diffusion:&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial X}{\partial t} = \nabla \log \pi(X) + \sqrt{2} \frac{\partial W}{\partial t}
$$&lt;/p&gt;
&lt;p&gt;\(W\)是一个标准布朗运动。为了近似Langevin diffusion采样路径，我们可以使用Euler-Maruyama方法，固定时间步\(\tau\)，我们可以更新\(X\)：&lt;/p&gt;
&lt;p&gt;$$
X_{k+1}:= X_k + \tau \nabla \log\pi(X_k) +\sqrt{2\tau} \xi_k
$$&lt;/p&gt;
&lt;p&gt;\(\xi_k\sim \mathcal N(0,I)\)。 这个方法可以被用做proposal方法，acceptance coefficient为:&lt;/p&gt;
&lt;p&gt;$$
a = \min {1, \frac{\pi(X_{k+1})q(X_k|X_{k+1})}{\pi(X_{k})q(X_{k+1}|X_k)}}
$$&lt;/p&gt;
&lt;p&gt;\(q(x&#39;|x)\propto \exp(-\frac{1}{4\tau}||x&#39;-x - \tau\nabla \log\pi(x)||^2_2)\), \(x&#39;|x \sim \mathcal N(x+\tau\nabla\log\pi(x),2\tau)\), 可以从更新等式推导得到.&lt;/p&gt;
&lt;h2 id=&#34;hamiltonian-mc&#34;&gt;Hamiltonian MC&lt;/h2&gt;
&lt;p&gt;更进一步的，我们可以将 Hamiltonian Dynamic 用到 MC 算法上。 Hamiltonian等式：&lt;/p&gt;
&lt;p&gt;$$
\frac {\partial q}{\partial t} = \frac {\partial H}{\partial p}\&lt;br&gt;
\frac {\partial p}{\partial t} = -\frac {\partial H}{\partial q}\&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;\(H=-\log\pi(q,p)=-\log\pi(p|q)-\log\pi(q) =: K(p,q)+V(q)\)被称为能量，\(p\) 是一个辅助动量，\(K,V\)分别是动能和势能。Hamiltonian equation给出一个向量场，类似于地球的引力场。运动一段时间后，\(q\)到达\(q&#39;\)，其轨迹是\(\phi_t(q,p)\)。在这个轨迹上，总能量不变，类似卫星绕地飞行。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/hmc_3.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/hmc_3.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			alt=&#34;HMC%20feb2b437ff5644c898eb089ffd8f36cc/Untitled%202.png&#34;&gt;
	&lt;/a&gt;
	
	&lt;figcaption&gt;HMC%20feb2b437ff5644c898eb089ffd8f36cc/Untitled%202.png&lt;/figcaption&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;现在我们需要设计\(p\) 和 \(\pi(p|q)\)。一个最简单的方法是令 \(p|q\sim\mathcal N(0,1)\)。另外，有两个主流的设计\(p\)的方法：Euclidean-Gaussian Kinetic Energies, \(p|q\sim\mathcal N(0,M)\) and \(M\) is defined as Euclidean metric, or mass in physic, thus \(K(p,q) = 0.5p^TMp+\log(|M|) + const.\)；Riemannian-Gaussian Kinetic Energies, where \(p|q\sim\mathcal N(0,\Sigma(q))\), thus \(K(p,q) = 0.5p^T\Sigma^{-1}(q)p+\log(|\Sigma(q)|) + const.\)。此外还有non-Gaussian kinetic energies，它们的长尾特性可能可以提高模型效果，但是实践上效果往往很差。&lt;/p&gt;
&lt;p&gt;另一个重要的变量是\(t\)。Basically for distribution family of target densities \(\pi_\beta(q)\propto\exp(-|q|^\beta)\), the optimal time is \(T_{optimal}(q,p)\propto (H(q,p))^\frac{2-\beta}{\beta}\), but it should be tuned dynamically in practice.&lt;/p&gt;
&lt;p&gt;我们可以先用\(p\sim\pi(p|q)\)采样，然后对Hamilton&amp;rsquo;s equations积分一段时间来得到轨迹\((q,p)\rightarrow\phi_t(q,p)\)，然后投影消除掉\(p\)得到\(q&#39;\)。但是直接对微分方程积分很困难，实践上，我们可以用symplectic integrator来近似，比如 leapfrog algorithm：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize \(q_0, p_0\).&lt;/li&gt;
&lt;li&gt;for \(0\le n&amp;lt;  T/\epsilon\):
&lt;ol&gt;
&lt;li&gt;\(p_{n+\frac{1}{2}} = p_n + \frac{\epsilon}{2}\frac{\partial p}{\partial t}\)&lt;/li&gt;
&lt;li&gt;\(q_{n+q} = q_n + \epsilon\frac{\partial q}{\partial t}\)&lt;/li&gt;
&lt;li&gt;\(p_{n+1} = p_n + \frac{\epsilon}{2}\frac{\partial p_{n+\frac{1}{2}}}{\partial t}\)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;注意\(\frac{\partial p}{\partial t}\) and \(\frac{\partial q}{\partial t}\) 需要替换为 Hamiltonian equations。我们还可以对Integrator调参来得到更好的结果。&lt;/p&gt;
&lt;p&gt;积分后，我们得到一个新的\(q\) 作为proposal，acceptance coefficient为:&lt;/p&gt;
&lt;p&gt;$$
a(q&#39;,p&#39;|q,p) = \min{1,\frac{\pi(q&#39;,p&#39;)\mathbb Q(q,p|q&#39;,p&#39;)}{\pi(q,p)\mathbb Q(q&#39;,p&#39;|q,p)}}
$$&lt;/p&gt;
&lt;p&gt;\(\mathbb Q(q,p|q_0,p_0)=\delta(q-q_L)\delta(p-p_L)\), \(q_L\) and \(p_L\) are the end point of the trajectory \((q_0,p_0)\rightarrow(q_L,p_L)\). 给定初始状态\((q_0,p_0)\)，这个变换总是可以沿着Hamiltonian equations积分给出轨迹的终点。In the notation above we write \((p&#39;,q&#39;)\) instead of \((p_L,q_L)\) and \((p,q)\) instead of \((p_0,q_0)\), so it&amp;rsquo;s obvious that \(\mathbb Q(q&#39;,p&#39;|q,p) = \delta(q&#39;-q&#39;)\delta(p&#39;-p&#39;) = 1\). However, if we inverse it, starting with \((q_L,p_L)\), it won&amp;rsquo;t go back to \((q_0,p_0)\) (definition of momentum), but go to \((q_{2L},p_{2L})\). So \(\mathbb Q(q,p|q&#39;,p&#39;) = \delta(q-q_{2L})\delta(p-p_{2L}) = 0\) and \(a = 0\), i.e. always reject the proposal. To fix this, we can flip the sign of momentum after we finish integrating: \((q_0,p_0)\rightarrow(q_L,-p_L)\), then inverse: \((q_L,-p_L)\rightarrow(q_0,-(-p_0))\). We also change the definition of \(\mathbb Q\):&lt;/p&gt;
&lt;p&gt;$$
\mathbb Q(q,p|q_0,p_0)=\delta(q-q_L)\delta(p+p_L)
$$&lt;/p&gt;
&lt;p&gt;then we have (note the changes of notation)&lt;/p&gt;
&lt;p&gt;$$
\mathbb Q(q&#39;,-p&#39;|q,p) = \delta(q&#39;-q&#39;)\delta(-p&#39;+p&#39;) = 1 \
\mathbb Q(q,p|q&#39;,-p&#39;) = \delta(q-q)\delta(p+(-p)) = 1
$$&lt;/p&gt;
&lt;p&gt;So we can simplify the acceptance coefficient:&lt;/p&gt;
&lt;p&gt;$$
a(q&#39;,-p&#39;|q,p) = \min{1,\frac{\pi(q&#39;,-p&#39;)\mathbb Q(q,p|q&#39;,-p&#39;)}{\pi(q,p)\mathbb Q(q&#39;,-p&#39;|q,p)}} = \min{1,\frac{\pi(q&#39;,-p&#39;)}{\pi(q,p)}}
$$&lt;/p&gt;
&lt;p&gt;Consider \(\pi(q,p) = \exp(-H(q,p))\),&lt;/p&gt;
&lt;p&gt;$$
a(q&#39;,-p&#39;|q,p)=\min{1, \exp(-H(q&#39;,-p&#39;)+H(q,p)}
$$&lt;/p&gt;
&lt;p&gt;To sum up, first we design a momentum distribution (or kinetic energy), then we sample a momentum, integrate the Hamiltonian equation to get a proposal, compute the acceptance ratio, sample a uniform value and decide if keep the proposal or not. Repeating these steps we will get a sample set efficiently (acceptance ratio is high).&lt;/p&gt;
&lt;h3 id=&#34;no-u-turn-sampler-nuts&#34;&gt;No-U-Turn Sampler (NUTS)&lt;/h3&gt;
&lt;p&gt;NUTS helps to control the integration time \(T\). Intuitively, if we follow the momentum too long, it will run too far away, like an U-Turn, and may stop at a point not far from the start point (the field is a closed loop).&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.notion.so/presentation-3699e0cbcf9046bd9c5bfa209d9f642b&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;presentation&lt;/a&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>Python调用作为参数的函数的方法</title>
        <link>https://blog.xpgreat.com/p/function_parameter_dict/</link>
        <pubDate>Thu, 10 Jun 2021 11:00:00 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/function_parameter_dict/</guid>
        <description>&lt;p&gt;今天在做project的时候遇到一个需求，简单来说是给定一个未知的函数&lt;code&gt;func&lt;/code&gt;和他的参数范围，需要用这些参数重复调用该函数，记录返回值。此时遇到的问题是不知道&lt;code&gt;func&lt;/code&gt;有多少参数和传入参数的顺序，查阅文档后发现可以用参数字典给函数传参，现记录于此。&lt;/p&gt;
&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;
&lt;p&gt;给定一个函数&lt;code&gt;func&lt;/code&gt;，参数名列表&lt;code&gt;param_names&lt;/code&gt;，参数列表&lt;code&gt;params&lt;/code&gt;，多次调用该函数并记录返回值。&lt;/p&gt;
&lt;h2 id=&#34;解决方法&#34;&gt;解决方法&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;func&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
  &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;param_names&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;params&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;res&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;para&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;func&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;param_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;para&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# Output:&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;使用itertoolsproduct快速循环生成参数表&#34;&gt;使用itertools.product快速循环生成参数表&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;params&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;itertools&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;product&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;params&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里的&lt;code&gt;params&lt;/code&gt;是一个字典，key是参数名字，value是参数可选范围，比如&lt;code&gt;&amp;quot;param1&amp;quot; :[0, 1, 2]&lt;/code&gt;。&lt;code&gt;itertools.product&lt;/code&gt;可以接受多个list，需要用&lt;code&gt;*list&lt;/code&gt;的形式解包传入参数。&lt;/p&gt;</description>
        </item>
        <item>
        <title>[论文解读]买方/卖方分析师对基金经理决策的影响</title>
        <link>https://blog.xpgreat.com/p/ssa_bsa/</link>
        <pubDate>Mon, 12 Apr 2021 17:48:40 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/ssa_bsa/</guid>
        <description>&lt;p&gt;在基金经理决策时，他所依赖的信息源主要是分析师，分析师分为三种：卖方分析师（Sell-side Analyst, SSA）、买方分析师（Buy-side Analyst, BSA）和独立第三方分析师（Independent Third-party Analyst, ITA）。基金经理如何权衡这三个来源的信息，决策时使用的权重有什么影响，分析师本身的一些特征（例如经验、偏差、准确度）会对决策结果带来什么影响？Yingmei Cheng et al. 在2006年发表的名为“
Buy-Side Analysts, Sell-Side Analysts, and Investment Decisions of Money Manager”的论文对此作了详细的理论和实证分析。&lt;/p&gt;
&lt;h2 id=&#34;三种分析师&#34;&gt;三种分析师&lt;/h2&gt;
&lt;p&gt;金融分析师应用公开信息和个人的信源，对股票等投资标的物的价格、前景、相关指标进行预测，进而为投资人提供投资建议。SSA为证券经纪商（brokerage firm）工作，为经纪人和他们的客户服务，他们的研究报告和投资建议通常也为大众开放。这些研报也被广泛认为是有投资参考价值的（eg., Elton, Gruber, and Grossman (1986), Stickel(1995), Womack (1996)），但是由于SSA与卖方的关系，他们的研报中可能会存在有利于卖方的偏差（bias）。对于SSA来说，存在两种主要的激励来源：一是对职业道德和名望的追求会促使他们提供客观无偏的报告，事实上很多基金经理会根据分析师过往的表现来决定是否使用他的报告；二是SSA与卖方的利益关系会促使他们提供更乐观的报告以吸引更多的投资，卖方经纪商甚至可能会因此给他们提供额外的报酬。&lt;/p&gt;
&lt;p&gt;BSA为资产管理公司工作，通常是各种基金公司，他们为公司内的基金经理提供内部的预测和推荐。与SSA不同，由于BSA供职于买方，他们的收入与买方是否盈利挂钩，所以他们会尽力给出有利于买方的研报，即更符合真实情况的报告，也就意味着不存在偏差（bias）的问题。然而由于是内部报告，BSA的成本比SSA要高昂的多，且成本会随着要求的精度的上升而迅速上升。&lt;/p&gt;
&lt;p&gt;ITA是独立第三方分析师，他们不供职于买方或卖方，而是生产研报，以一次性的价格卖给基金经理。通常情况下ITA的报告在基金经理决策时只占很小的部分(4%)。&lt;/p&gt;
&lt;h2 id=&#34;bsassa占比的最优平衡问题&#34;&gt;BSA、SSA占比的最优平衡问题&lt;/h2&gt;
&lt;h3 id=&#34;定义&#34;&gt;定义&lt;/h3&gt;
&lt;p&gt;对于一个基金经理来说，他们做出一个投资决策\(y\)带来的收益与该股票真实价格\(v\)挂钩，该过程对他带来的效用（utility）是&lt;/p&gt;
&lt;p&gt;$$
U(y,b) = -\alpha (y-v)^2
$$&lt;/p&gt;
&lt;p&gt;其中\(\alpha &amp;gt; 0\)是一个比例因子，它的大小表达了基金经理对决策与真实的偏差的敏感度，比如管理的基金规模（economy of scale）越大，敏感度就越高，\(\alpha\)也越大。真实价格\(\widetilde v \sim N(0, \sigma_0^2)\)其中\(\sigma_0^2 \rightarrow +\infinity \)，也就是说真实价格近似服从一个均值为零且方差无穷大的正态分布，故基金经理不可能依靠自己的先验知识来进行预测。所以基金经理必须完全依靠分析师来进行投资决策。SSA做出的预测可以写为：&lt;/p&gt;
&lt;p&gt;$$
\widetilde S_{SSA} = B + (\widetilde v + {\widetilde e}_{SSA})
$$&lt;/p&gt;
&lt;p&gt;其中\(\widetilde e_{SSA} \sim N(0, \sigma_{SSA}^2)\)，代表SSA的信号（即预测）的噪音（signal&amp;rsquo;s noise），其方差越大，意味着预测越不可信，所以可以定义\(p = 1 / \sigma_{SSA}^2\)来描述信号的精度，\(p\)越大，信号中包含的真实情况的信息越多。\(B\)代表SSA的bias，可以假设\(B&amp;gt;0\)，因为SSA通常只会加入积极的偏差（optimistic bias），虽然理论上也可以是消极的，但推理和实证研究都表明偏差为正。正如上面提到的，SSA不一定会在每一次都有偏，所以引入一个偏差概率\(\theta\)，\(P(B=b) = \theta, P(B=0) = 1-\theta \)。*（对于一个SSA来说偏差一直保持不变？B应该是一个大于等于零的随机变量。有待思考）* 于是\(B\)的方差\(\Sigma_B^2 = \theta (1-\theta)b^2\) （二元分布）。偏差不能太大，否则SSA的信息会被视为无用，假设\(0 &amp;lt; b \le = \sqrt{1/(2\theta (1-\theta) p)} \)。总的来说，\(p\)越大，\(b\)和\(\Sigma_B^2\)越小，则SSA的报告质量越高。&lt;/p&gt;
&lt;p&gt;BSA做出的预测可以写为：&lt;/p&gt;
&lt;p&gt;$$
\widetilde S_{BSA} = \widetilde v + {\widetilde e}_{BSA}
$$&lt;/p&gt;
&lt;p&gt;与SSA类似地，\(\widetilde e_{BSA} \sim N(0, \sigma_{BSA}^2)\)，\(q = 1 / \sigma_{BSA}^2\)。注意到BSA是不含偏差的。此外，由于BSA是基金公司内部的分析师，所以存在成本，且成本与报告质量有关，记为\(C(q)\)，是一个关于\(q\)的凸增函数，即\(C&#39;(q)&amp;gt;0, C&#39;&#39;(q) &amp;gt; 0\)。&lt;/p&gt;
&lt;h3 id=&#34;成本平衡与分析师权重&#34;&gt;成本平衡与分析师权重&lt;/h3&gt;
&lt;p&gt;基于上面的定义，可以写出基金经理在权衡SSA和BSA时的核心优化问题：&lt;/p&gt;
&lt;p&gt;$$
\max_y E[- \alpha (y - \widetilde v)^2 | \widetilde S_{SSA} = S_{SSA}, \widetilde S_{BSA} = S_{BSA}]
$$&lt;/p&gt;
&lt;p&gt;经证明可得，均衡点的\(y^* = w^*S_{BSA} + (1 - w^*)(S_{SSA} - \theta b)\)，其中\(w^* = \frac{q}{p+q}\)。&lt;/p&gt;
&lt;p&gt;将\(y^*, w^*\)代回效用函数，减掉BSA带来的成本，求导得到最优\(q\)应满足的一阶条件：&lt;/p&gt;
&lt;p&gt;$$
\alpha \left[ \frac{1}{(p+q)^2} + \frac{2p^2b^2\theta (1-\theta)}{(p+q)^3} \right] - C&#39;(q) = 0
$$&lt;/p&gt;
&lt;p&gt;进一步推导可得，\(q^*\)在\(\alpha , b, \Sigma_B^2\)增加，或\(p\)减少时，增加。相应的，\(w^* = \frac{q}{p+q} = 1-\frac{p}{p+q}\)也增加。&lt;/p&gt;
&lt;h2 id=&#34;6条假说&#34;&gt;6条假说&lt;/h2&gt;
&lt;p&gt;由上面的模型推导，作者提出了6条假说，并用实际数据加以了验证：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;H1: BSA的权重随着&lt;strong&gt;SSA的信号质量降低&lt;/strong&gt;而增加。\(p\)&lt;/li&gt;
&lt;li&gt;H2: BSA的权重随着&lt;strong&gt;SSA的偏差增加&lt;/strong&gt;而增加。\(b\)&lt;/li&gt;
&lt;li&gt;H3: BSA的权重随着&lt;strong&gt;SSA的偏差的不确定性增加&lt;/strong&gt;而增加。\(\Sigma_B^2\)&lt;/li&gt;
&lt;li&gt;H4: BSA的权重随着&lt;strong&gt;BSA的信号质量增加&lt;/strong&gt;而增加。\(q\)&lt;/li&gt;
&lt;li&gt;H5: BSA的权重随着&lt;strong&gt;资产管理规模的增加&lt;/strong&gt;而增加。\(\alpha\)&lt;/li&gt;
&lt;li&gt;H6: BSA的权重随着&lt;strong&gt;代理人问题（agency problem）的严重程度减少&lt;/strong&gt;而增加。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于绝大多数基金内的股票，都有多个SSA来cover，同时这些基金也有自己的BSA团队。我们可以认为，SSA的信号就是所有cover
该股的SSA的报告所包含的信息（&lt;em&gt;不考虑有相关性的其他股票的信息？&lt;/em&gt;）。进一步，如果cover的SSA数量越多，可以认为SSA信号所包含的信息越多（如果有herding的情况不一定对），也就是SSA信号的质量越高。作者使用了cover到某一股票的SSA数量作为SSA信号质量\(p\)的一个度量，如果没有SSAcover该股票，则\(p=0\)，基金经理只能全部依赖于BSA。于是有了假设1，BSA的权重随着SSA的信号质量降低而增加。&lt;/p&gt;
&lt;p&gt;为了衡量SSA的偏差，作者取所有相关的SSA的平均预测的误差作为它的度量，尽管不同的SSA有着不同的偏差，但平均预测的误差可以反映出总体的偏差的大小。作者用股价做了归一化，消除了股价的绝对高低带来的偏差的不可比的问题。由此作者提出第二个假设：BSA的权重随着SSA的偏差增加而增加。&lt;/p&gt;
&lt;p&gt;由上面对偏差的估计的讨论可以得出，我们可以用SSA的平均预测的误差的方差作为SSA的偏差的不确定性的一个度量。而平均预测的误差的方差也正是平均预测的方差，故作者用它来衡量偏差的不确定性。作者提出假设3: BSA的权重随着SSA的偏差的不确定性增加而增加。&lt;/p&gt;
&lt;p&gt;BSA的信号的质量直接影响到基金经理的权重选择。影响其质量的一个重要因素，且该因素不会影响到SSA，是BSA的经验。一般的我们可以假定，BSA的经验越丰富，给出的信号质量越高。于是有了假设4: BSA的权重随着BSA的信号质量增加而增加。&lt;/p&gt;
&lt;p&gt;基金的规模也会影响基金经理的选择，在上面的效用函数中，\(\alpha\)越大，基金经理越希望获得与实际相近的预测，也就会更多的参考BSA的无偏的报告。一个用来衡量\(\alpha\)的变量则是基金或管理资金的规模。即假设5: BSA的权重随着资产管理规模的增加而增加。&lt;/p&gt;
&lt;p&gt;最后，基金经理和客户间的代理人问题也会产生影响。为了减轻代理人问题，可以给基金经理performance的激励，他们的\(\alpha\)会增大，会提高BSA的占比以追求更准确的预测。所以作者提出第六个假设：BSA的权重随着代理人问题（agency problem）的严重程度减少而增加。&lt;/p&gt;
&lt;h2 id=&#34;实证研究&#34;&gt;实证研究&lt;/h2&gt;
&lt;p&gt;为了验证上述的六个假设，作者使用了Thomson Financial/Nelson Information&amp;rsquo;s Directory of Fund Managers作为数据源，提取出了2000—2002年三年的数据，在每年中都有超过1800个来自全球的组织的数据，他们一共管理超过2万亿美元的资产。该数据集每年发布一版，主要聚焦于复杂的机构投资者（而不是小投资者），所以他们提供误导性数据以短期获利的动机比较低。作者选择的样本是seperate account，即brokerage和financial advisor为高净值、pension和其他机构投资者提供分离的账户，每个投资人都在自己的账户中独立拥有相同的股票set，而不是像一般的open-end的共同基金一样所有投资者拥有一个池子里的一部分。该数据集主要包括基金portfolio的特点、投资风格、benchmark、return。另外，它还提供了关于基金经理和基金BSA的特点信息，包括他们的经验和费用结构。数据集的统计特征如图所示：&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/img/ssa_bsa_1.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/img/ssa_bsa_1.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			alt=&#34;统计特征表&#34;&gt;
	&lt;/a&gt;
	
	&lt;figcaption&gt;统计特征表&lt;/figcaption&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;其中对于股票的资本规模、风险、PE，作者用S&amp;amp;P500为参照。Equity Turnover（股票周转率）表示了基金成分股票交易的频率，即该基金的管理的积极性。Sector Emphasis（行业重点）表示基金是否更改其行业重点。在后面的研究中，作者将Value和Growth &amp;amp; Value合并成一类Value-typed。&lt;/p&gt;
&lt;p&gt;这个数据集最大的特点就是，它提供了基金经理决策的来源信息（后四行），In-house对应BSA，Street Analyst对应SSA，Other对应ITA。从数据中可以归纳出至少三种BSA有利于基金经理的渠道：1. 在很少SSA cover到某个股票时（例如小盘股），BSA可以提供分析报告，此时BSA填补了SSA的空白（fill the void）；2. 基金经理不信任SSA，转而使用BSA的报告，此时BSA替换了（replace/substitute）SSA；3. BSA可能使用SSA的报告作为参考来发表自己的报告，此时BSA报告辅助了（complement）SSA的报告。值得注意的是，BSA的平均和中位占比远高于SSA，证明了BSA对基金经理的重要性。&lt;/p&gt;
&lt;p&gt;最后一行描述了基金公司整体的决策过程，分为三类：1. 集中制（centralized），基金公司可以集中公司内所有的BSA，他们生产的研报所有基金经理都可以使用，以增加整个公司的研究资源的使用效率；2. 多委员会制（multi commitee），基金公司内由多个不同的委员会分别负责投资的各个方面（例如研究、投资组合审查），但是数据表明，单个公司的多个基金仍然依赖一个委员会进行投资决策，因此这些基金使用BSA的方法也是一样的；3. 相对独立（relative autonomous），同公司内的不同基金经理的使用BSA和SSA的方式不同，使用这种决策过程的公司占比仅10%～12%，除去其中仅管理一个基金的公司后占比不到8%。&lt;/p&gt;
&lt;p&gt;接下来，作者从什么影响BSA/SSA占比，和BSA/SSA占比对收益的影响两个方面入手，展开了回归分析，以证明上面的六个假说。此外，作者还对分析的鲁棒性做了测试，以证明上述分析的可靠性，篇幅比较长在这就不做展示了，请参阅原论文。&lt;/p&gt;</description>
        </item>
        <item>
        <title>AMPlibraryagent异常访问Onedrive下载</title>
        <link>https://blog.xpgreat.com/p/amp_onedrive/</link>
        <pubDate>Sat, 10 Apr 2021 11:19:05 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/amp_onedrive/</guid>
        <description>&lt;p&gt;先放方法：用任务监视器定位到AMPlibraryagent的路径，然后使用OneDrive的“应用管理”，停止响应AMPlibraryagent的请求。&lt;/p&gt;
&lt;p&gt;今天在正常使用MacBook时，偶然发现OneDrive经常会异常同步，反复释放空间无果。查看OneDrive的日志发现，是一个名叫AMPlibraryagent的应用请求的。查询得知，AMPLibraryAgent（The media library agent for Music.app and TV.app）是Apple Music和Apple TV的相关进程，属于系统进程，从它下手的可能性不大。&lt;/p&gt;
&lt;p&gt;尝试很多方法后，终于发现可以通过onedrive设置里面的“管理应用”来停止响应特定应用的请求&lt;a class=&#34;link&#34; href=&#34;https://www.reddit.com/r/mac/comments/kcsayt/disable_amplibraryagent/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;🔗&lt;/a&gt;。于是有了如下步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;任务监视器定位到AMPlibraryagent的路径。&lt;/li&gt;
&lt;li&gt;Onedrive &amp;gt; 首选项 &amp;gt; 应用管理 &amp;gt; +&lt;/li&gt;
&lt;li&gt;弹出的Finder里 Cmd + Shift + G, 输入AMPlibraryagent的路径，选择它和AMPArtworkAgent，停止OneDrive对他们的响应。&lt;/li&gt;
&lt;li&gt;ok&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
        <item>
        <title>搭建真分布式Hadoop集群指南</title>
        <link>https://blog.xpgreat.com/p/hadoop_aws/</link>
        <pubDate>Mon, 29 Mar 2021 10:39:10 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/hadoop_aws/</guid>
        <description>&lt;p&gt;搭建一个Hadoop环境是学习大数据技术的第一步，这个环境可以是单机的，也可以是伪集群的，但最贴近实际生产环境的还是真集群的版本。本文提供一个基于AWS搭建Hadoop的简要指南，基于AWS EC2 Ubuntu 20。如果你也用的是AWS的免费套餐，几个注意的地方：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;小心流量使用情况，免费的仅包括了15G，请设置使用提醒。&lt;/li&gt;
&lt;li&gt;选择机器配置的时候注意选择免费套餐内的，否则也会扣费。&lt;/li&gt;
&lt;li&gt;免费套餐包括EC2每个月750h/台的使用时间，开多台实例的话注意不要超时，可以随用随开，不用就关。&lt;/li&gt;
&lt;li&gt;关闭后再启动公有IP会变，注意配置问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;初始化通用环境&#34;&gt;初始化通用环境&lt;/h2&gt;
&lt;p&gt;先启动一个实例，安装通用的Java和Hadoop，然后利用AWS的映像功能克隆出其他的集群成员。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;sudo apt-get update -y
sudo apt-get upgrade -y
sudo apt install default-jre &lt;span class=&#34;c1&#34;&gt;#可以先java看一下提示，换成最新版&lt;/span&gt;
wget https://apache.mirror.digionline.de/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz &lt;span class=&#34;c1&#34;&gt;#改成最新版&lt;/span&gt;
sudo tar zxvf hadoop-* -C /usr/local
sudo mv /usr/local/hadoop-* /usr/local/hadoop​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;添加环境变量&lt;code&gt;sudo vim ~/.bashrc​&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;JAVA_HOME&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/usr/lib/jvm/java-11-openjdk-amd64
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;PATH&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$PATH&lt;/span&gt;:&lt;span class=&#34;nv&#34;&gt;$JAVA_HOME&lt;/span&gt;/bin
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/usr/local/hadoop
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;PATH&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$PATH&lt;/span&gt;:&lt;span class=&#34;nv&#34;&gt;$HADOOP_HOME&lt;/span&gt;/bin
&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;HADOOP_CONF_DIR&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/usr/local/hadoop/etc/hadoop​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更新环境变量&lt;code&gt;source ~/.bashrc&lt;/code&gt;，测试能否用java和hadoop。&lt;/p&gt;
&lt;p&gt;上面的搞定后，克隆两个实例。&lt;/p&gt;
&lt;h2 id=&#34;配置ssh&#34;&gt;配置SSH&lt;/h2&gt;
&lt;p&gt;在每个实例上创建key：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;ssh-keygen -f ~/.ssh/id_rsa -t rsa -P &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在NameNode上的&lt;code&gt;~/.ssh/config&lt;/code&gt;写入：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;Host namenode
	HostName &amp;lt;private IP or public DNS of namenode instance&amp;gt;
	User ubuntu
Host datanode1 
	HostName &amp;lt;private IP or public DNS of datanode1 instance&amp;gt; 
	User ubuntu
Host datanode2 
	HostName &amp;lt;private IP or public DNS of datanode2 instance&amp;gt; 
	User ubuntu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;把datanode的公钥拷贝到namenode的&lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt;里，换行粘贴就行。&lt;/p&gt;
&lt;h2 id=&#34;配置hadoop&#34;&gt;配置Hadoop&lt;/h2&gt;
&lt;h3 id=&#34;在namenode和datanode上执行&#34;&gt;在NameNode和DataNode上执行：&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;$HADOOP_CONF_DIR&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在&lt;code&gt;core-site.xml&lt;/code&gt;中插入，并确认9000端口开放：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.defaultFS&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;hdfs://[namenode public dns name or private IP]:9000&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在&lt;code&gt;yarn-site.xml&lt;/code&gt;中插入：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt;

&lt;span class=&#34;c&#34;&gt;&amp;lt;!-- Site specific YARN configuration properties --&amp;gt;&lt;/span&gt;

  &lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.nodemanager.aux-services&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;mapreduce_shuffle&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.resourcemanager.hostname&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;[namenode public dns name or private IP]&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在&lt;code&gt;mapred-site.xml&lt;/code&gt;中插入，并确认54311端口开放：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.jobtracker.address&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;[namenode public dns name or private IP]:54311&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.framework.name&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;yarn&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在&lt;code&gt;hdoop-env.sh&lt;/code&gt;中将将语句&lt;code&gt;export JAVA_HOME=$JAVA_HOME&lt;/code&gt;修改为&lt;code&gt;export JAVA_HOME=/usr/java/jdk1.8.0_101&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;仅在namenode上执行&#34;&gt;仅在NameNode上执行：&lt;/h3&gt;
&lt;p&gt;修改&lt;code&gt;/etc/hosts&lt;/code&gt;，加入：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&amp;lt;namenode ip&amp;gt; &amp;lt;namenode_hostname&amp;gt;
&amp;lt;datanode1 ip&amp;gt; &amp;lt;datanode1_hostname&amp;gt;
&amp;lt;datanode2 ip&amp;gt; &amp;lt;datanode2_hostname&amp;gt;
127.0.0.1 localhost
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在&lt;code&gt;hdfs-site.xml&lt;/code&gt;插入：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.replication&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;3&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.namenode.name.dir&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;file:///usr/local/hadoop/data/hdfs/namenode&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;新建文件路径：&lt;code&gt;sudo mkdir -p $HADOOP_HOME/data/hdfs/namenode​&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;HADOOP_CONF_DIR&lt;/code&gt;创建一个名称为&lt;code&gt;masters&lt;/code&gt;的文件，写入&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&amp;lt;namenode_hostname&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;创建一个名称为&lt;code&gt;workers&lt;/code&gt;的文件，写入&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&amp;lt;datanode1_hostname&amp;gt;
&amp;lt;datanode2_hostname&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;修改权限：&lt;code&gt;sudo chown -R ubuntu $HADOOP_HOME​&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;仅在datanode上执行&#34;&gt;仅在DataNode上执行：&lt;/h3&gt;
&lt;p&gt;在&lt;code&gt;$HADOOP_CONF_DIR/hdfs-site.xml&lt;/code&gt;中插入：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.replication&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;3&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.datanode.data.dir&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;file:///usr/local/hadoop/data/hdfs/datanode&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;执行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;sudo mkdir -p &lt;span class=&#34;nv&#34;&gt;$HADOOP_HOME&lt;/span&gt;/data/hdfs/datanode
sudo chown -R ubuntu &lt;span class=&#34;nv&#34;&gt;$HADOOP_HOME&lt;/span&gt;​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;初始化hadoop集群&#34;&gt;初始化Hadoop集群&lt;/h2&gt;
&lt;p&gt;在NameNode上执行：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;hdfs namenode -format &lt;span class=&#34;c1&#34;&gt;# 格式化文件系统&lt;/span&gt;
&lt;span class=&#34;nv&#34;&gt;$HADOOP_HOME&lt;/span&gt;/sbin/start-dfs.sh​ &lt;span class=&#34;c1&#34;&gt;# 启动dfs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;没问题后初始化yarn：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;nv&#34;&gt;$HADOOP_HOME&lt;/span&gt;/sbin/start-yarn.sh​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;执行完后运行&lt;code&gt;jps&lt;/code&gt;查看运行情况，确认服务都起来了。&lt;/p&gt;
&lt;p&gt;一切顺利的话之后都可以通过&lt;code&gt;$HADOOP_HOME/sbin/start-all.sh​&lt;/code&gt;和&lt;code&gt;$HADOOP_HOME/sbin/start-all.sh​&lt;/code&gt;一键启停集群。&lt;/p&gt;
&lt;p&gt;获取集群slave的状态报告：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;hdfs dfsadmin -report
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;刷新集群节点：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;hdfs dfsadmin -refreshNodes​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;成功后可以访问&lt;code&gt;&amp;lt;namenode public DNS or public IP&amp;gt;:9870 &lt;/code&gt;来使用hadoop的web UI，但要确认9870端口是开放的。&lt;/p&gt;
&lt;h2 id=&#34;优化&#34;&gt;优化&lt;/h2&gt;
&lt;p&gt;对于AWS免费的1G RAM的机器，在&lt;code&gt;yarn-site.xml&lt;/code&gt;中加入：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.nodemanager.resource.memory-mb&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;768&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.scheduler.maximum-allocation-mb&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;768&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.scheduler.minimum-allocation-mb&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;64&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.nodemanager.vmem-check-enabled&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;false&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在&lt;code&gt;mapred-site.xml&lt;/code&gt;中配置：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.app.mapreduce.am.resource.mb&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;256&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.map.memory.mb&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;128&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapreduce.reduce.memory.mb&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
	&lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;128&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;持续更新--要点和troubleshooting&#34;&gt;持续更新&amp;ndash;要点和troubleshooting&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;配置ssh权限时要有耐心，这直接决定了你的集群能不能互相通讯。用AWS的时候最好用私有ip配置，否则重启后公有ip会改变，又要重新配置一遍。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重要的配置文件（在&lt;code&gt;$HADOOP_HOME/etc/hadoop/&lt;/code&gt;下）：&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;core-site.xml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yarn-site.xml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mapred-site.xml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hdfs-site.xml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hadoop-env.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Hadoop3.0后web端端口改成了&lt;strong&gt;9870&lt;/strong&gt;，不是原来的50070！！！！！！不要被网上老的教程误导！&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cluster ID 冲突&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;nv&#34;&gt;$HADOOP_HOME&lt;/span&gt;/sbin/stop-all.sh
sudo rm -rf /tmp/hadoop-ubuntu
sudo rm -rf &lt;span class=&#34;nv&#34;&gt;$HADOOP_HOMEdata&lt;/span&gt;/hdfs/namenode   &lt;span class=&#34;c1&#34;&gt;# directory mentioned in hdfs-site.xml for namenode&lt;/span&gt;
sudo mkdir -p &lt;span class=&#34;nv&#34;&gt;$HADOOP_HOME&lt;/span&gt;/data/hdfs/namenode
sudo chown -R ubuntu &lt;span class=&#34;nv&#34;&gt;$HADOOP_HOME&lt;/span&gt;
hdfs namenode -format -force
&lt;span class=&#34;nv&#34;&gt;$HADOOP_HOME&lt;/span&gt;/sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;
&lt;p&gt;ERROR:无法创建 java.io.IOException: Cannot create directory /usr/local/hadoop/data/hdfs/namenode/current
解决方法：改权限
sudo chmod -R a+w /usr/local/hadoop/&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ERROR: JAVA_HOME is not set and could not be found.
针对这个错误，网上好多都说了java的路径设置有问题，但没有指出具体的修改方法，其实是hadoop里面hadoop-env.sh文件里面的java路径设置不对，hadoop-env.sh在hadoop/etc/hadoop目录下，具体的修改办法如下：&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo vim hadoop/etc/hadoop/hdoop-env.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;将语句      &lt;code&gt;export JAVA_HOME=$JAVA_HOME&lt;/code&gt;
修改为      &lt;code&gt;export JAVA_HOME=/usr/java/jdk1.8.0_101&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;ERROR: 找不到或无法加载主类org.apache.hadoop.mapreduce.v2.app.MRAppMaster
在命令行下输入&lt;code&gt;hadoop classpath&lt;/code&gt;，并将返回的地址复制。
编辑&lt;code&gt;yarn-site.xml&lt;/code&gt;, 添加如下内容&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
        &lt;span class=&#34;nt&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;yarn.application.classpath&lt;span class=&#34;nt&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
        &lt;span class=&#34;nt&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;输入刚才返回的Hadoop classpath路径&lt;span class=&#34;nt&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在所有的Master和Slave节点进行如上设置，设置完毕后重启Hadoop集群。&lt;/p&gt;</description>
        </item>
        <item>
        <title>利用sklearn构建tfidf向量</title>
        <link>https://blog.xpgreat.com/p/tfidf_python/</link>
        <pubDate>Tue, 24 Nov 2020 11:22:02 +0800</pubDate>
        
        <guid>https://blog.xpgreat.com/p/tfidf_python/</guid>
        <description>&lt;p&gt;在自然语言处理中，第一步需要面对的就是词向量特征的提取。语言的特征提取在sklearn模块中有相当完善的方法和模块，本文先利用CountVectorizer提取词汇，再用TfidfTransformer计算TFIDF向量。之所以选择CountVectorizer而不自行写一个代码，是因为在使用时维度很容易超过10w，产生的bag-of-words向量特别稀疏，需要耗费极大的内存，而sklearn实现了一个稀疏矩阵的存储形式，可以极大的加速和降低消耗。&lt;/p&gt;
&lt;h2 id=&#34;构建bag-of-words词汇矩阵&#34;&gt;构建Bag-of-words词汇矩阵&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;CountVectorizer&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#测试用字符串list&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;s_l&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Relevant words for each class or cluster are identified by computing a relevancy score rc for every word ti based on the documents in the class&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;s1&#34;&gt;&amp;#39;or cluster and then selecting the highest scoring&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;s1&#34;&gt;&amp;#39;words. These scores can be computed either by-89&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;s1&#34;&gt;&amp;#39;aggregating the raw tf-idf features of all documents&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;s1&#34;&gt;&amp;#39;in the group (Section 2.3.1), by aggregating these&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;s1&#34;&gt;&amp;#39;features weighted by some classifier’s parameters&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;s1&#34;&gt;&amp;#39;(Section 2.3.2), or directly by computing a score&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;s1&#34;&gt;&amp;#39;for each word depending on the number of documents it occurs in from this class relative to other&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;s1&#34;&gt;&amp;#39;classes (Section 2.3.3).&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#初始化CountVectorizer，token_pattern后可以自定义提取单词的规则，不写则默认取纯字母单词，这里取的是字母、数字和-的组合&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;count_vect&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;CountVectorizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;token_pattern&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;[a-zA-Z0-9\-]+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#这里传入的参数必须是iteratable的，比如这里的list&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;X_train_counts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count_vect&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s_l&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_train_counts&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plain&#34; data-lang=&#34;plain&#34;&gt;&amp;lt;9x62 sparse matrix of type &amp;#39;&amp;lt;class &amp;#39;numpy.int64&amp;#39;&amp;gt;&amp;#39;
	with 95 stored elements in Compressed Sparse Row format&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;显示出这里的&lt;code&gt;X_train_counts&lt;/code&gt;是一个&lt;code&gt;9*62&lt;/code&gt;的稀疏矩阵。&lt;/p&gt;
&lt;p&gt;稀疏化显示&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_train_counts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;todense&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plain&#34; data-lang=&#34;plain&#34;&gt;matrix([[0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0,
         1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
         0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1],
        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
        [0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
         0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
         1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0],
        [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
       dtype=int64)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;查看词汇&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;count_vect&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vocabulary_&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plain&#34; data-lang=&#34;plain&#34;&gt;{&amp;#39;relevant&amp;#39;: 44,
 &amp;#39;words&amp;#39;: 61,
 &amp;#39;for&amp;#39;: 26,
 &amp;#39;each&amp;#39;: 22,
...
 &amp;#39;occurs&amp;#39;: 34,
 &amp;#39;from&amp;#39;: 27,
 &amp;#39;this&amp;#39;: 56,
 &amp;#39;relative&amp;#39;: 42,
 &amp;#39;to&amp;#39;: 58,
 &amp;#39;other&amp;#39;: 38,
 &amp;#39;classes&amp;#39;: 14}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;左边是单词，右边是索引，可以用&lt;code&gt;count_vect.vocabulary_.get(&#39;relative&#39;)&lt;/code&gt;这种形式来查找索引号，&lt;code&gt;list(filter(lambda x: x[1] == 6, count_vect.vocabulary_.items()))[0][0]&lt;/code&gt;获取索引为6的word，其实&lt;code&gt;count_vect.vocabulary_.items()&lt;/code&gt;本质上就是字典。&lt;/p&gt;
&lt;h2 id=&#34;构建tfidf向量矩阵&#34;&gt;构建tfidf向量矩阵&lt;/h2&gt;
&lt;p&gt;Term Frequency-Inverse Document Frequency，词频-逆文件频率，是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一单词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。即&lt;strong&gt;一个词语在一篇文件中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文件&lt;/strong&gt;。一般做归一化处理，公式如下：&lt;/p&gt;
&lt;p&gt;$$
tf(t_i)=\frac{在某一文件中词t_i出现的次数}{该文件中所有的词数目} \\ \ \\ idf(t_i)=\log(\frac{数据集的文件总数}{包含词t_i的文件数})=\log\frac{|N|}{|\{k\in N:t_i\in k\}|}
$$&lt;/p&gt;
&lt;p&gt;有时为了保证\(idf\)分母不为零，可以在分母上加一。&lt;/p&gt;
&lt;p&gt;使用sklearn的TfidfTransformer实现，非常方便。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TfidfTransformer&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;tf_idf_transformer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TfidfTransformer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;tf_idf_matrix&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tf_idf_transformer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_train_counts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#传入bag-of-words矩阵即可&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plain&#34; data-lang=&#34;plain&#34;&gt;matrix([[0.        , 0.        , 0.        , 0.17690932, 0.        ,
         0.        , 0.        , 0.20945535, 0.20945535, 0.        ,
         0.13590618, 0.        , 0.        , 0.35381864, 0.        ,
         0.        , 0.17690932, 0.        , 0.17690932, 0.        ,
         0.        , 0.15381755, 0.17690932, 0.        , 0.20945535,
         0.        , 0.35381864, 0.        , 0.        , 0.        ,
         0.20945535, 0.15381755, 0.        , 0.        , 0.        ,
         0.        , 0.17690932, 0.15381755, 0.        , 0.        ,
         0.        , 0.20945535, 0.        , 0.20945535, 0.20945535,
         0.        , 0.17690932, 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.24254304, 0.        ,
         0.        , 0.        , 0.20945535, 0.        , 0.        ,
         0.17690932, 0.17690932],
        ...
        [0.        , 0.35681845, 0.7136369 , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.48588431,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.35681845,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        , 0.        , 0.        , 0.        ,
         0.        , 0.        ]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        </item>
        <item>
        <title>论文解读：Exploring text datasets by visualizing relevant words</title>
        <link>https://blog.xpgreat.com/p/explore_text_dataset/</link>
        <pubDate>Thu, 19 Nov 2020 15:14:26 +0800</pubDate>
        
        <guid>https://blog.xpgreat.com/p/explore_text_dataset/</guid>
        <description>&lt;p&gt;在做机器学习使用新数据集时，我们首先要知道它的特点，一个可以快速可靠地深入了解所选文档的内容，并区分它们类别的工具十分重要。 在这篇论文，作者从文本集合中提取“相关词”，以概括属于某个类别（或在unlabeled数据集下为聚类的组）的文档的内容，并在词云中可视化它们。 作者比较了三种提取相关单词的方法，并使用两个数据集验证了模型的可用性。&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1707.05261.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;论文链接&lt;/a&gt;，&lt;a class=&#34;link&#34; href=&#34;https://github.com/cod3licious/textcatvis&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;论文代码&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;目标&#34;&gt;目标&lt;/h2&gt;
&lt;p&gt;提供一个文件数据集，输出一组词云，展示最能够区分每个文件类别的词。这里的数据集可以是标注的或是未标注的，对未标注的数据集作者先进行了聚类。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/image-20201119154330899.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/image-20201119154330899.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			alt=&#34;image-20201119154330899&#34;&gt;
	&lt;/a&gt;
	
	&lt;figcaption&gt;image-20201119154330899&lt;/figcaption&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;上图是作者论文中的图片，五个词云分别表示最能代表论文五个部分（Abstract, Introduction, Methods, Results, Discussion，即文件类别）的词。&lt;/p&gt;
&lt;h2 id=&#34;方法&#34;&gt;方法&lt;/h2&gt;
&lt;h3 id=&#34;数据预处理和特征提取&#34;&gt;数据预处理和特征提取&lt;/h3&gt;
&lt;p&gt;首先对所有文件进行1.小写化 2. 删除非字母数字的字符。然后逐段用tfidf转换为bag-of-words（BOW）特征向量\(\mathbf x_k \in \mathbf R^T \forall k \in \{1,\dots,N\}\)，其中\(k\)是文件编号，\(T\)是该文件包含的词汇数（vocabulary，即去重词数），\(N\)是文件数。表示文件\(k\)的特征向量里对应单词\(t_i\)的值\(\mathbf x_{ki} = tf(t_i) idf(t_i)\)。&lt;/p&gt;
&lt;h4 id=&#34;tf-idf&#34;&gt;TF-IDF&lt;/h4&gt;
&lt;p&gt;Term Frequency-Inverse Document Frequency，词频-逆文件频率，是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一单词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。即&lt;strong&gt;一个词语在一篇文件中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文件&lt;/strong&gt;。一般做归一化处理，公式如下：&lt;/p&gt;
&lt;p&gt;$$
tf(t_i)=\frac{在某一文件中词t_i出现的次数}{该文件中所有的词数目} \\ \ \\ idf(t_i)=\log(\frac{数据集的文件总数}{包含词t_i的文件数})=\log\frac{|N|}{|\{k\in N:t_i\in k\}|}
$$&lt;/p&gt;
&lt;p&gt;有时为了保证\(idf\)分母不为零，可以在分母上加一。&lt;/p&gt;
&lt;h3 id=&#34;未标注数据集聚类&#34;&gt;未标注数据集聚类&lt;/h3&gt;
&lt;p&gt;对于未标注的数据集，首先要聚类成多个cluster，然后针对每个cluster计算相关单词。作者采用的是DBSCAN（density-based spatial clustering of applications with noise，https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf），首先把tfidf向量用线性PCA降维到250维，然后对特征做交叉，再计算向量间的距离（1 - cosine similarity）。对于聚类时使用的距离门槛（threshold），作者经过实验推荐最低cosine similarity为0.45.&lt;/p&gt;
&lt;h3 id=&#34;计算相关单词relevant-word&#34;&gt;计算相关单词（relevant word）&lt;/h3&gt;
&lt;p&gt;确定相关单词分为两步，第一步是计算描述相关性的relevance score \(r_c\)，第二步时根据\(r_c\)给所有单词排序，越高代表越相关。为了计算\(r_c\)，作者比较了三种方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过汇总组中所有文档的原始tf-idf特征；&lt;/li&gt;
&lt;li&gt;通过汇总由某些分类器（SVM+LRP）加权的这些特征；&lt;/li&gt;
&lt;li&gt;分别计算每个单词在该类和其他类出现的文件数量，进而求出\(r_c\)。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;突出tfidf特征法salient-tf-idf-features&#34;&gt;突出tfidf特征法（Salient tf-idf features）&lt;/h4&gt;
&lt;p&gt;最直接简单的方法，对类别\(c\)计算\(r_c\)的公式为：&lt;/p&gt;
&lt;p&gt;$$
r_c (t_i) = \sum_{k:y_k=c}\mathbf x_{ki}=\sum_{k:y_k=c}tf_k(t_i)idf_k(t_i)
$$&lt;/p&gt;
&lt;p&gt;即将所有属于类别\(c\)的文件\(k\)中的单词\(t_i\)的tf-idf值加起来作为这个类别中该单词的relevance score。这样分数最高的单词在该类别的文件中出现频率最高，且不会在大多数其他文件里出现（否则idf值会很低）。然而，一个可能的问题是，高分词在其他类别的文件中出现频率也差不多，这对idf的值影响不大，但意味着在其他类别中该词也会被判定为relevant word，这与relevant word的定义矛盾（最能用来区分不同类别的词）。&lt;/p&gt;
&lt;h4 id=&#34;svmlrpdecomposed-classifier-scores&#34;&gt;SVM+LRP（Decomposed classifier scores）&lt;/h4&gt;
&lt;p&gt;分为两步，第一步找到一个线性分类器，利用tfidf特征将文件分类。即对类\(c\)，找到\(\mathbf w_c\)，最优化分类器：&lt;/p&gt;
&lt;p&gt;$$
\hat y_k  = \mathop{\arg \max}_{c} \ b_c + \mathbf w_c^T\mathbf x_k
$$&lt;/p&gt;
&lt;p&gt;优化好分类器后，计算relevance score：&lt;/p&gt;
&lt;p&gt;$$
r_c(t_i) = \sum_{k:y_k=c}\left( \mathbf w_{ci} \mathbf x_{ki} + \frac{b_c}{T}\right)
$$&lt;/p&gt;
&lt;p&gt;其中\(T\)是词汇数。&lt;/p&gt;
&lt;p&gt;在优化分类器时，作者选择了SVM，并提出，也可以用其他的线性分类器比如逻辑回归，甚至DNN。作者强调，只有在分类器精确度相当高的时候，这个方法找出来的相关词汇才有意义，所以分类器的精确度比简便更加重要。&lt;/p&gt;
&lt;h5 id=&#34;layerwise-relevance-propagationlrp&#34;&gt;Layerwise relevance propagation（LRP）&lt;/h5&gt;
&lt;p&gt;是一个计算相关性，并将相关性逐层向后传播的过程。首先将网络模型看成一个拓扑图结构，在计算一个节点 a 和输入的节点之间的相关性时，将 a 点的数值作为相关性，并且计算与 a 点相连的上一层节点在生成 a 点时所占的权重，将 a 的相关性逐层向后传播，直到输入层。作者用下图的例子告诉了我们：&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/image-20201119212624565.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/image-20201119212624565.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			alt=&#34;image-20201119212624565&#34;&gt;
	&lt;/a&gt;
	
	&lt;figcaption&gt;image-20201119212624565&lt;/figcaption&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;如果要计算 \(v_1\) 和 \(u_1\) 之间的相关性，首先计算 \(v_1\)和 \(z_1\), \(z_2\) 之间的相关性，再将 \(v_1\) 和\(z_1\), \(z_2\) 的相关性传递到 \(u_1\), 从而求得 \(v_1\) 和 \(u_1\) 之间的相关性。&lt;/p&gt;
&lt;h5 id=&#34;support-vector-machine-svm支持向量机&#34;&gt;Support vector machine （SVM，支持向量机）&lt;/h5&gt;
&lt;p&gt;是一种二分类模型，它的基本模型是定义在特征空间上的&lt;strong&gt;间隔最大&lt;/strong&gt;的&lt;strong&gt;线性分类器&lt;/strong&gt;，间隔最大使它有别于感知机；SVM还包括&lt;strong&gt;核技巧&lt;/strong&gt;，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。&lt;/p&gt;
&lt;p&gt;SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。如下图所示，\(\mathbf w \mathbf x + b = 0\)即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/image-20201119213025858.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/image-20201119213025858.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			alt=&#34;image-20201119213025858&#34;&gt;
	&lt;/a&gt;
	
	&lt;figcaption&gt;image-20201119213025858&lt;/figcaption&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;SVM有一个重要性质：&lt;strong&gt;训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;算法如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;：训练数据集 \(T=\{(\mathbf x_1,y_1),(\mathbf x_2,y_2),\dots,(\mathbf x_n,y_n)\}\)其中，\(\mathbf x_i \in \mathbf R^n, y_i \in \{1,-1\},i = 1,2,\dots,n\)；&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;输出&lt;/strong&gt;：分离超平面和分类决策函数&lt;/p&gt;
&lt;p&gt;（1）选择惩罚参数\(C &amp;gt; 0\) ，构造并求解凸二次规划问题&lt;/p&gt;
&lt;p&gt;$$
\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(\mathbf x_i\mathbf x_j) - \sum_{i=1}^N\alpha_i\\ s.t.\ \sum_{i=1}^N\alpha_iy_i=0\\0\le \alpha_i\le C, i=1,2,\dots,N
$$&lt;/p&gt;
&lt;p&gt;得到最优解\(\mathbf\alpha^*=(\alpha_1^*,\alpha_2^*,\dots,\alpha_N^*)^T\)&lt;/p&gt;
&lt;p&gt;（2）计算&lt;/p&gt;
&lt;p&gt;$$
\mathbf {\omega} ^* = \sum_{i=1}^N\alpha_i^*y_i\mathbf x_i
$$&lt;/p&gt;
&lt;p&gt;选择\(\mathbf \alpha^*\)的一个分量\(\mathbf \alpha_j^*\)满足条件\(0&amp;lt;\mathbf \alpha_j^* &amp;lt; C\)，计算&lt;/p&gt;
&lt;p&gt;$$
b^* = y_j - \sum_{i=1}^N\alpha_i^*y_i(\mathbf x_i\mathbf x_j)
$$&lt;/p&gt;
&lt;p&gt;（3）求分离超平面&lt;/p&gt;
&lt;p&gt;$$
\mathbf \omega ^* \mathbf x + b^* = 0
$$&lt;/p&gt;
&lt;p&gt;分类决策函数：&lt;/p&gt;
&lt;p&gt;$$
f(\mathbf x) = sign(\mathbf\omega^*\mathbf x +b^*)
$$&lt;/p&gt;
&lt;h4 id=&#34;词-文件数量法distinctive-words&#34;&gt;词-文件数量法（Distinctive words）&lt;/h4&gt;
&lt;p&gt;作者引入了两个概念：TPR（True Positive Rate，真阳性率）和FPR（假阳性率）。定义分别为&lt;/p&gt;
&lt;p&gt;$$
TPR_c(t_i) = \frac{类别为c且t_i的tfidf值大于0的文件数}{类别为c的文件数} = \frac {|\{k:y_k=c\ \land \ \mathbf x_{ki} &amp;gt; 0\}|}{|\{k:y_k=c\}|} \\ FPR_c(t_i) = mean(\{TPR_l(t_i):l\ne c\})+std(\{TPR_l(t_i):l\ne c\})
$$&lt;/p&gt;
&lt;p&gt;最终目的是找出在目标类中出现频繁（TPR高），且在非目标类中出现不频繁（FPR低）的词，需要一个度量这一性质的函数。一种直接的方法是直接算差值：&lt;/p&gt;
&lt;p&gt;$$
r_c\_diff(t_i) = \max \ \{TPR_c(t_i)-FPR_c(t_i),0\}
$$&lt;/p&gt;
&lt;p&gt;这种方法存在一个问题，即只考虑了两个率的绝对差值，未考虑相对差，即如果1. TPR=0.9，FPR=0.8；2. TPR=0.2，FPR=0.1，这种量度会认为他们一样重要，但显然前者对于后者，TPR和FPR相差过近。因此引入基于除法的算法：&lt;/p&gt;
&lt;p&gt;$$
r_c\_quot(t_i) = \frac{\min\{\max\{z_c(t_i),1\},4\} -1}{3} \\ where\ z_c(t_i) = \frac{TPR_c(t_i)}{\max\{FPR_c(t_i),\epsilon\}}, \epsilon是一个极小值，用来避免除数为零
$$&lt;/p&gt;
&lt;p&gt;综合二者，得到既考虑相对又考虑绝对差异的度量：&lt;/p&gt;
&lt;p&gt;$$
r_c\_dist(t_i) = 0.5(r_c\_diff(t_i)+r_c\_quot(t_i))
$$&lt;/p&gt;
&lt;p&gt;rc-TPR-FPR关系图：&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/image-20201119232608223.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/image-20201119232608223.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			alt=&#34;image-20201119232608223&#34;&gt;
	&lt;/a&gt;
	
	&lt;figcaption&gt;image-20201119232608223&lt;/figcaption&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;作者用scientific publications（标记的，主题、格式分类）和New York Times article（未标记的，寻找热度主题）两个数据集验证了算法的可行性。该方法快速且可靠，更重要的是，即使许多cluster仅包含很少的样本，也有可能可以识别出relevant words，而用分类算法很难达到这个效果。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/31886934&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zhuanlan.zhihu.com/p/31886934&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.sohu.com/a/150681957_114778&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.sohu.com/a/150681957_114778&lt;/a&gt;&lt;/p&gt;</description>
        </item>
        <item>
        <title>Hive窗口函数总结</title>
        <link>https://blog.xpgreat.com/p/hive_windowfunc/</link>
        <pubDate>Thu, 29 Oct 2020 11:01:26 +0800</pubDate>
        
        <guid>https://blog.xpgreat.com/p/hive_windowfunc/</guid>
        <description>&lt;p&gt;窗口函数是对Hive的一项增强，用来更方便地分析离线数据。窗口函数的使用场景非常之多，包括去重、排名、分组求和等等。本文希望尽可能全面的归纳窗口函数的用法，以便日后的查阅。&lt;/p&gt;
&lt;h2 id=&#34;窗口函数&#34;&gt;窗口函数&lt;/h2&gt;
&lt;h3 id=&#34;基本用法模式&#34;&gt;基本用法模式&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;FUNCTION&amp;gt;(&amp;lt;params&amp;gt;) OVER (&amp;lt;window&amp;gt;)&lt;/code&gt;, 表示对window内的元素进行function操作，这里的window可以理解为分组，例如&lt;code&gt;partition by col1 order by col2 desc &lt;/code&gt;，即”相同的col1分为一组，按照每一行对应的col2的值倒序排序“。function即对每一组采取的操作，比如取平均值，这样可以得到对每一个不同的&lt;code&gt;col1&lt;/code&gt;的&lt;code&gt;col2&lt;/code&gt;的平均值的表，即：&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/image-20201029113941871.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/image-20201029113941871.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			alt=&#34;image-20201029113941871&#34;&gt;
	&lt;/a&gt;
	
	&lt;figcaption&gt;image-20201029113941871&lt;/figcaption&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;该操作的hive语句：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span class=&#34;k&#34;&gt;SELECT&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; 
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;AVG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;over&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;partition&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;by&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;order&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;by&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;k&#34;&gt;desc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;n&#34;&gt;test_table&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;where&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;...];&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;lead--lag&#34;&gt;Lead &amp;amp; Lag&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;LEAD(col, n, DEFAULT)&lt;/code&gt;: 用于统计窗口内往下第&lt;code&gt;n&lt;/code&gt;行值。&lt;code&gt;col&lt;/code&gt;指定列名，&lt;code&gt;DEFAULT&lt;/code&gt;指定如果往下&lt;code&gt;n&lt;/code&gt;行没有值了的替换值，如不指定则是&lt;code&gt;NULL&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;LAG(col, n, DEFAULT)&lt;/code&gt;: 用于统计窗口内往上第&lt;code&gt;n&lt;/code&gt;行值。&lt;code&gt;col&lt;/code&gt;指定列名，&lt;code&gt;DEFAULT&lt;/code&gt;指定如果往上&lt;code&gt;n&lt;/code&gt;行没有值了的替换值，如不指定则是&lt;code&gt;NULL&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;firstvalue--lastvalue&#34;&gt;FirstValue &amp;amp; LastValue&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;FIRST_VALUE(col, NO_NULL)&lt;/code&gt;: 用于统计窗口内截止到当前行的第1行值。&lt;code&gt;col&lt;/code&gt;指定列名，&lt;code&gt;NO_NULL&lt;/code&gt;指定是否跳过空值，默认&lt;code&gt;TRUE&lt;/code&gt;跳过。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;LAST_VALUE(col, NO_NULL)&lt;/code&gt;: 用于统计窗口内截止到当前行的最后一行值。&lt;code&gt;col&lt;/code&gt;指定列名，&lt;code&gt;NO_NULL&lt;/code&gt;指定是否跳过空值，默认&lt;code&gt;TRUE&lt;/code&gt;跳过。&lt;/p&gt;
&lt;h3 id=&#34;aggregation-functions&#34;&gt;Aggregation Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;COUNT(col)&lt;/code&gt;：计数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SUM(col)&lt;/code&gt;：求和&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MIN(col)&lt;/code&gt;：求最小值&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MAX(col)&lt;/code&gt;：求最大值&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AVG(col)&lt;/code&gt;：求平均值&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ranking-functions&#34;&gt;Ranking Functions&lt;/h3&gt;
&lt;p&gt;注意排序已经在window语句中执行了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ROW_NUMBER()&lt;/code&gt;：求该行在window中的行数，从1开始，遇到重复值的按窗口出现的顺序递增排列。常用于取前n个记录，比如取用户最近一次冒泡时间。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RANK()&lt;/code&gt;：求该行在window中的排名，重复值的名次相同，但会留下空位，比如两个第一后面是第三。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DENSE_RANK()&lt;/code&gt;：求该行在window中的排名，重复值的名次相同，&lt;strong&gt;不&lt;/strong&gt;会留下空位，比如两个第一后面是第二。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CUME_RANK()&lt;/code&gt;：小于等于当前值的行数占比。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PERCENTILE_RANK()&lt;/code&gt;：(该行的RANK值-1)/(窗口总行数-1)，百分数排名。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NTILE(n)&lt;/code&gt;：将窗口按顺序切成n片，如果切出来的结果不均匀，分界处的行归入上一片。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;窗口子句&#34;&gt;窗口子句&lt;/h2&gt;
&lt;p&gt;窗口子句可以用来更精细的描述窗口，注意有几个函数是不支持窗口子句的：&lt;code&gt;Rank&lt;/code&gt;, &lt;code&gt;NTile&lt;/code&gt;,&lt;code&gt;DenseRank&lt;/code&gt;,&lt;code&gt;CumeDisk&lt;/code&gt;,&lt;code&gt;PercentRank&lt;/code&gt;,&lt;code&gt;Lead&lt;/code&gt;,&lt;code&gt;Lag&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;子句&lt;/th&gt;
&lt;th&gt;意义&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;PRECEDING&lt;/td&gt;
&lt;td&gt;往前&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FOLLOWING&lt;/td&gt;
&lt;td&gt;往后&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CURRENT ROW&lt;/td&gt;
&lt;td&gt;当前行&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UNBOUNDED&lt;/td&gt;
&lt;td&gt;起点（一般结合PRECEDING，FOLLOWING使用）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UNBOUNDED PRECEDING&lt;/td&gt;
&lt;td&gt;表示该窗口最前面的行（起点）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UNBOUNDED FOLLOWING&lt;/td&gt;
&lt;td&gt;表示该窗口最后面的行（终点）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;用法实例：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW&lt;/code&gt;：从该窗口的起点到当前行&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ROWS BETWEEN 2 PRECEDING AND 1 FOLLOWING&lt;/code&gt;：从前2行到后1行&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ROWS BETWEEN 2 PRECEDING AND 1 CURRENT ROW&lt;/code&gt;：从前2行到当前行&lt;/p&gt;</description>
        </item>
        <item>
        <title>机器学习的工作流程</title>
        <link>https://blog.xpgreat.com/p/workflow_ml/</link>
        <pubDate>Sun, 15 Mar 2020 16:25:12 +0800</pubDate>
        
        <guid>https://blog.xpgreat.com/p/workflow_ml/</guid>
        <description>&lt;p&gt;最近在阅读François Chollet写的Deep Learning with Python. 书中有很多经验性的技巧和归纳，其中4.5写的Machine Learning的工作流程归纳的很好，看完深有启发，摘录于此。&lt;/p&gt;
&lt;p&gt;机器学习的标准流程可以分为六个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定义问题&lt;/li&gt;
&lt;li&gt;确定衡量模型的标准&lt;/li&gt;
&lt;li&gt;确定评估方式&lt;/li&gt;
&lt;li&gt;建立一个比理论平均更好的简单模型&lt;/li&gt;
&lt;li&gt;扩展模型，使其过拟合&lt;/li&gt;
&lt;li&gt;优化模型&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一般的机器学习方面的研究主要关注第6步，但最好按照标准流程来做。&lt;/p&gt;
&lt;h2 id=&#34;定义问题&#34;&gt;定义问题&lt;/h2&gt;
&lt;p&gt;首先思考，可用的数据有哪些，希望从这些数据中得到什么信息，希望用这些信息预测什么。需要注意预测的内容必须要从training data中能得到信息的，比如要为影评做情绪分类，必须要有影评和它对应的情绪标签作training data才行。&lt;/p&gt;
&lt;p&gt;考虑问题的类型：二元分类？多元分类？标量回归？向量回归？多元多标签分类？聚类？&amp;hellip;&amp;hellip;总之明确问题的类型，有助于后面选择模型。&lt;/p&gt;
&lt;p&gt;明确输入和输出。时刻记住在使用机器学习的时候，存在两个假设：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;输出可以通过输入预测得到。&lt;/li&gt;
&lt;li&gt;已有的数据足够多以提取出输入和输出的关系。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;比如如果做一个模型预测股票价格的走势，在仅有历史价格数据的时候是不够的，因为历史价格不包含未来价格的信息。机器学习只能利用training data中存在的模式(pattern)。&lt;/p&gt;
&lt;p&gt;另外如果这个问题是不平稳的(Nonstationary)，机器学习也不适用。比如做推荐服装的模型，不同季节对不同服装的喜好程度是不同的，如果用八月的数据训练预测九月的情况，模型效果肯定很差。这时则要采取变通，比如使用多年的数据训练，添加月份标签等。&lt;/p&gt;
&lt;h2 id=&#34;确定衡量模型的标准&#34;&gt;确定衡量模型的标准&lt;/h2&gt;
&lt;p&gt;考虑清楚问题并估计可以建模之后，需要确定一个衡量标准，来判断模型是否成功。比如预测值的准确度，客户留存度，正确分类的比例等等。需要注意的是很多时候标准是根据问题自定的，这就需要建模者有经验。可以在&lt;a class=&#34;link&#34; href=&#34;kaggle.com&#34; &gt;Kaggle&lt;/a&gt;之类的竞赛网站上拓展视野。&lt;/p&gt;
&lt;h2 id=&#34;确定评估方式&#34;&gt;确定评估方式&lt;/h2&gt;
&lt;p&gt;确定一个评估方式来衡量进度也是很重要的。一般来说有三种方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用一个hold-out validation set，即将数据集分出一部分专门用来验证模型效果。在有大量数据时可以使用这个方法。&lt;/li&gt;
&lt;li&gt;K-fold Cross Validation， 将原始数据分成K组(K-Fold)，将每个子集数据分别做一次验证集，其余的K-1组子集数据作为训练集，这样会得到K个模型。这K个模型分别在验证集中评估结果，最后的误差取平均就得到交叉验证误差。在数据量不足以做第一种方法时可以选择。但这种方法计算需求很大。&lt;/li&gt;
&lt;li&gt;循环K-fold validation，每次都进行数据随机排列，然后进行完整的k-fold。适用于数据量很小且对精度要求高的情况。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一般选第一个即可。&lt;/p&gt;
&lt;h2 id=&#34;建立一个比理论平均更好的简单模型&#34;&gt;建立一个比理论平均更好的简单模型&lt;/h2&gt;
&lt;p&gt;这一步的目的是较快的找到比理论平均好的简单模型，通常可以进行简单的尝试，比如在MNIST数字分类时用最简单的feed feedforward 模型尝试，得到精确度高于0.1（完全随机分类的平均精度），这样就可以在这个基础上进行下一步的优化了。需要注意的是不是所有时候都能够找得到这样的简单模型的。如果在多次尝试后仍找不到，需要考虑是否是前面的两个假设不成立，即根本不存在输出和输入之间的明确关系，或者这种关系在已有的数据集中不明确。&lt;/p&gt;
&lt;p&gt;如果顺利找到这种简单模型，下一步需要考虑三个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最后一层的激活函数。将模型网络中的倒数第二层输出组合形成最终输出，比如在二元分类时可以使用relu激活，得到一个0-1之间的数字作为选择一类的概率。&lt;/li&gt;
&lt;li&gt;loss function，即一个可微的函数，用来衡量模型的精度。可微性是必要的因为使用梯度下降法优化时需要求微分。&lt;/li&gt;
&lt;li&gt;优化方法，比如Adam，RmsProp等。通常可以先尝试用这两种的默认学习率（learning rate）优化。&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Problem type&lt;/th&gt;
&lt;th&gt;Last-layer activation&lt;/th&gt;
&lt;th&gt;Loss function&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Binary classification&lt;/td&gt;
&lt;td&gt;&lt;code&gt;sigmoid&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;binary_crossentropy&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Multiclass, single-label classification&lt;/td&gt;
&lt;td&gt;&lt;code&gt;softmax&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;categorical_crossentropy&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Multiclass, multilabel classification&lt;/td&gt;
&lt;td&gt;&lt;code&gt;sigmoid&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;binary_crossentropy&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Regression to arbitrary values&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;&lt;code&gt;mse&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Regression to values between 0 and 1&lt;/td&gt;
&lt;td&gt;&lt;code&gt;sigmoid&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;mse&lt;/code&gt; or &lt;code&gt;binary_crossentropy&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;扩展模型&#34;&gt;扩展模型&lt;/h2&gt;
&lt;p&gt;这一步将模型扩展，使其过拟合，因为上一步得到的简单模型可能没有足够的层数，没有足够多的神经元等等。机器学习问题其实就是平衡拟合能力和泛化能力的问题，我们的目标是找到二者平衡的一个点。既不能让它欠拟合，也不能让它过拟合。一般的做法是先让模型过拟合，然后再进行细节优化。扩展模型有三个方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;增加层数。&lt;/li&gt;
&lt;li&gt;增加单层神经元个数。&lt;/li&gt;
&lt;li&gt;训练更多的epochs。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果你发现验证的精度开始下降了，这时候就说明模型已经过拟合，这一步也可以结束了。&lt;/p&gt;
&lt;h2 id=&#34;优化模型&#34;&gt;优化模型&lt;/h2&gt;
&lt;p&gt;最复杂也最耗时的一步，大部分研究的重点。大致逻辑是对模型做小修小改，把数据放进去跑，看模型验证精度的变化，直到优化到不能再优化为止。一般可以尝试的方向有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;加入Dropout。&lt;/li&gt;
&lt;li&gt;改变结构，比如增加或删除一层。&lt;/li&gt;
&lt;li&gt;L1/L2正则化（Regularization）。&lt;/li&gt;
&lt;li&gt;更改不同的超参数（hyperparameter），比如学习率、每层神经元个数等等。&lt;/li&gt;
&lt;li&gt;增加或删除feature。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;如果多次依据验证精度调整模型，会把验证集的信息泄露给模型（相当于验证集也参与了模型的构建和优化）。少量的次数还可以接受，太多次后会让优化后的模型没有说服力。&lt;/p&gt;
&lt;p&gt;当你获得一个满意的优化后的模型，可以使用所有的数据（training 和 validation set）来最后训练一次，再使用test data做最后的测试。如果最后测试结果显著不如validation的结果，则需要考虑是否overfitting，或者时评估方式（第三步）选择不对，可以尝试换一个试试。&lt;/p&gt;</description>
        </item>
        <item>
        <title>安装TensorFlow时踩的一个小坑</title>
        <link>https://blog.xpgreat.com/p/tip_inst_tf/</link>
        <pubDate>Sun, 15 Mar 2020 15:22:49 +0800</pubDate>
        
        <guid>https://blog.xpgreat.com/p/tip_inst_tf/</guid>
        <description>&lt;p&gt;不记得之前安装的是什么版本，这次（2020/3）让&lt;code&gt;pip&lt;/code&gt;自动安装选择的是&lt;code&gt;2.1.0&lt;/code&gt;，但这个版本安装后使用&lt;code&gt;import tensorflow&lt;/code&gt;会报错，如下：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Traceback (most recent call last):
  File &amp;quot;[...]\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py&amp;quot;, line 58, in &amp;lt;module&amp;gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File &amp;quot;[...]\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py&amp;quot;, line 28, in &amp;lt;module&amp;gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File &amp;quot;[...]\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py&amp;quot;, line 24, in swig_import_helper
    _mod = imp.load_module(&#39;_pywrap_tensorflow_internal&#39;, fp, pathname, description)
  File &amp;quot;[...]\lib\imp.py&amp;quot;, line 242, in load_module
    return load_dynamic(name, filename, file)
  File &amp;quot;[...]\lib\imp.py&amp;quot;, line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;, line 1, in &amp;lt;module&amp;gt;
  File &amp;quot;[...]\lib\site-packages\tensorflow\__init__.py&amp;quot;, line 101, in &amp;lt;module&amp;gt;
    from tensorflow_core import *
  File &amp;quot;[...]\lib\site-packages\tensorflow_core\__init__.py&amp;quot;, line 40, in &amp;lt;module&amp;gt;
    from tensorflow.python.tools import module_util as _module_util
  File &amp;quot;[...]\lib\site-packages\tensorflow\__init__.py&amp;quot;, line 50, in __getattr__
    module = self._load()
  File &amp;quot;[...]\lib\site-packages\tensorflow\__init__.py&amp;quot;, line 44, in _load
    module = _importlib.import_module(self.__name__)
  File &amp;quot;[...]\lib\importlib\__init__.py&amp;quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &amp;quot;[...]\lib\site-packages\tensorflow_core\python\__init__.py&amp;quot;, line 49, in &amp;lt;module&amp;gt;
    from tensorflow.python import pywrap_tensorflow
  File &amp;quot;[...]\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py&amp;quot;, line 74, in &amp;lt;module&amp;gt;
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File &amp;quot;[...]\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py&amp;quot;, line 58, in &amp;lt;module&amp;gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File &amp;quot;[...]\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py&amp;quot;, line 28, in &amp;lt;module&amp;gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File &amp;quot;[...]\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py&amp;quot;, line 24, in swig_import_helper
    _mod = imp.load_module(&#39;_pywrap_tensorflow_internal&#39;, fp, pathname, description)
  File &amp;quot;[...]\lib\imp.py&amp;quot;, line 242, in load_module
    return load_dynamic(name, filename, file)
  File &amp;quot;[...]\lib\imp.py&amp;quot;, line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 找不到指定的模块。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;报错十分奇怪，提示缺少DLL，但在网上找到的信息只有需要&lt;code&gt;system32&lt;/code&gt;里的&lt;code&gt;msvc140p.dll&lt;/code&gt;可能是问题所在，如果缺少可以安装&lt;code&gt;Microsoft Visual C++ 2015 Redistributable&lt;/code&gt;解决。在检查后发现这个文件已经存在。尝试 &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qq_42580947/article/details/104175457&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://blog.csdn.net/qq_42580947/article/details/104175457&lt;/a&gt; 上的方法后才解决。问题出在版本上，换成&lt;code&gt;2.0.0&lt;/code&gt;正常运行。&lt;code&gt;pip install tensorflow==2.0.0&lt;/code&gt;。另附上国内比较好用的&lt;code&gt;pip&lt;/code&gt;源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;清华大学：https://pypi.tuna.tsinghua.edu.cn/simple/&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;中科大：https://pypi.mirrors.ustc.edu.cn/simple/&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用的时候在命令后面加上&lt;code&gt;-i [源]&lt;/code&gt;即可。&lt;/p&gt;</description>
        </item>
        <item>
        <title>在Jupyer Notebook里安装包</title>
        <link>https://blog.xpgreat.com/p/jupyter_install_package/</link>
        <pubDate>Mon, 09 Dec 2019 22:01:11 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/jupyter_install_package/</guid>
        <description>&lt;p&gt;一个方便的小技巧，可以很快的安装缺失的包。&lt;/p&gt;
&lt;p&gt;可以使用pip或conda来安装包，注意如果环境不同包是不能通用的，在Windows下启动Anaconda Prompt的时候要注意标题后面括号内的环境是不是所需要的。&lt;/p&gt;
&lt;p&gt;以numpy为例，使用pip：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import sys
!{sys.executable} -m pip install numpy
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用conda：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import sys
!conda install --yes --prefix {sys.prefix} numpy
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>让程序在后台稳定运行的方法</title>
        <link>https://blog.xpgreat.com/p/keep_running_after-disconnect/</link>
        <pubDate>Tue, 16 Jul 2019 16:21:51 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/keep_running_after-disconnect/</guid>
        <description>&lt;p&gt;在使用SSH链接远程虚拟机的时候经常会出现网络连接不稳定而导致前台运行的代码终止运行的情况，怎样能让程序稳定地在后台运行，不受断线的影响？IBM Developer上的这篇文章很有帮助，转载方便以后查阅。&lt;/p&gt;
&lt;p&gt;(转自&lt;a class=&#34;link&#34; href=&#34;https://www.ibm.com/developerworks/cn/linux/l-cn-nohup/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Linux 技巧：让进程在后台运行更可靠的几种方法 - 申毅&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;我们经常会碰到这样的问题，用 telnet/ssh 登录了远程的 Linux 服务器，运行了一些耗时较长的任务， 结果却由于网络的不稳定导致任务中途失败。如何让命令提交后不受本地关闭终端窗口/网络断开连接的干扰呢？下面举了一些例子， 您可以针对不同的场景选择不同的方式来处理这个问题。&lt;/p&gt;
&lt;h2 id=&#34;nohupsetsid&#34;&gt;nohup/setsid/&amp;amp;&lt;/h2&gt;
&lt;h3 id=&#34;场景&#34;&gt;场景：&lt;/h3&gt;
&lt;p&gt;如果只是临时有一个命令需要长时间运行，什么方法能最简便的保证它在后台稳定运行呢？&lt;/p&gt;
&lt;h3 id=&#34;解决方法&#34;&gt;解决方法：&lt;/h3&gt;
&lt;p&gt;我们知道，当用户注销（logout）或者网络断开时，终端会收到 HUP（hangup）信号从而关闭其所有子进程。因此，我们的解决办法就有两种途径：要么让进程忽略 HUP 信号，要么让进程运行在新的会话里从而成为不属于此终端的子进程。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;hangup 名称的来由&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在 Unix 的早期版本中，每个终端都会通过 modem 和系统通讯。当用户 logout 时，modem 就会挂断（hang up）电话。 同理，当 modem 断开连接时，就会给终端发送 hangup 信号来通知其关闭所有子进程。&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;1-nohup&#34;&gt;1. nohup&lt;/h4&gt;
&lt;p&gt;nohup 无疑是我们首先想到的办法。顾名思义，nohup 的用途就是让提交的命令忽略 hangup 信号。让我们先来看一下 nohup 的帮助信息：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;NOHUP(1)                        User Commands                        NOHUP(1)
 
NAME
       nohup - run a command immune to hangups, with output to a non-tty
 
SYNOPSIS
       nohup COMMAND [ARG]...
       nohup OPTION
 
DESCRIPTION
       Run COMMAND, ignoring hangup signals.
 
       --help display this help and exit
 
       --version
              output version information and exit
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可见，nohup 的使用是十分方便的，只需在要处理的命令前加上 nohup 即可，标准输出和标准错误缺省会被重定向到 nohup.out 文件中。一般我们可在结尾加上**&amp;quot;&amp;amp;&amp;quot;**来将命令同时放入后台运行，也可用&amp;quot;&lt;code&gt;&amp;gt;filename 2&amp;gt;&amp;amp;1&lt;/code&gt;&amp;ldquo;来更改缺省的重定向文件名。&lt;/p&gt;
&lt;p&gt;nohup 示例&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@pvcent107 ~]# nohup ping www.ibm.com &amp;amp;
[1] 3059
nohup: appending output to `nohup.out&#39;
[root@pvcent107 ~]# ps -ef |grep 3059
root      3059   984  0 21:06 pts/3    00:00:00 ping www.ibm.com
root      3067   984  0 21:06 pts/3    00:00:00 grep 3059
[root@pvcent107 ~]#
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;2-setsid&#34;&gt;2. setsid&lt;/h4&gt;
&lt;p&gt;nohup 无疑能通过忽略 HUP 信号来使我们的进程避免中途被中断，但如果我们换个角度思考，如果我们的进程不属于接受 HUP 信号的终端的子进程，那么自然也就不会受到 HUP 信号的影响了。setsid 就能帮助我们做到这一点。让我们先来看一下 setsid 的帮助信息：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SETSID(8)                 Linux Programmer’s Manual                 SETSID(8)
 
NAME
       setsid - run a program in a new session
 
SYNOPSIS
       setsid program [ arg ... ]
 
DESCRIPTION
       setsid runs a program in a new session.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可见 setsid 的使用也是非常方便的，也只需在要处理的命令前加上 setsid 即可。&lt;/p&gt;
&lt;p&gt;setsid 示例&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@pvcent107 ~]# setsid ping www.ibm.com
[root@pvcent107 ~]# ps -ef |grep www.ibm.com
root     31094     1  0 07:28 ?        00:00:00 ping www.ibm.com
root     31102 29217  0 07:29 pts/4    00:00:00 grep www.ibm.com
[root@pvcent107 ~]#
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;值得注意的是，上例中我们的进程 ID(PID)为31094，而它的父 ID（PPID）为1（即为 init 进程 ID），并不是当前终端的进程 ID。请将此例与nohup 例中的父 ID 做比较。&lt;/p&gt;
&lt;h4 id=&#34;3-&#34;&gt;3. &amp;amp;&lt;/h4&gt;
&lt;p&gt;这里还有一个关于 subshell 的小技巧。我们知道，将一个或多个命名包含在“()”中就能让这些命令在子 shell 中运行中，从而扩展出很多有趣的功能，我们现在要讨论的就是其中之一。&lt;/p&gt;
&lt;p&gt;当我们将&amp;rdquo;&amp;amp;&amp;ldquo;也放入“()”内之后，我们就会发现所提交的作业并不在作业列表中，也就是说，是无法通过jobs来查看的。让我们来看看为什么这样就能躲过 HUP 信号的影响吧。&lt;/p&gt;
&lt;p&gt;subshell 示例&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@pvcent107 ~]# (ping www.ibm.com &amp;amp;)
[root@pvcent107 ~]# ps -ef |grep www.ibm.com
root     16270     1  0 14:13 pts/4    00:00:00 ping www.ibm.com
root     16278 15362  0 14:13 pts/4    00:00:00 grep www.ibm.com
[root@pvcent107 ~]#
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从上例中可以看出，新提交的进程的父 ID（PPID）为1（init 进程的 PID），并不是当前终端的进程 ID。因此并不属于当前终端的子进程，从而也就不会受到当前终端的 HUP 信号的影响了。&lt;/p&gt;
&lt;h2 id=&#34;disown&#34;&gt;disown&lt;/h2&gt;
&lt;h3 id=&#34;场景-1&#34;&gt;场景：&lt;/h3&gt;
&lt;p&gt;我们已经知道，如果事先在命令前加上 nohup 或者 setsid 就可以避免 HUP 信号的影响。但是如果我们未加任何处理就已经提交了命令，该如何补救才能让它避免 HUP 信号的影响呢？&lt;/p&gt;
&lt;h3 id=&#34;解决方法-1&#34;&gt;解决方法：&lt;/h3&gt;
&lt;p&gt;这时想加 nohup 或者 setsid 已经为时已晚，只能通过作业调度和 disown 来解决这个问题了。让我们来看一下 disown 的帮助信息：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;disown [-ar] [-h] [jobspec ...]
    Without options, each jobspec is  removed  from  the  table  of
    active  jobs.   If  the -h option is given, each jobspec is not
    removed from the table, but is marked so  that  SIGHUP  is  not
    sent  to the job if the shell receives a SIGHUP.  If no jobspec
    is present, and neither the -a nor the -r option  is  supplied,
    the  current  job  is  used.  If no jobspec is supplied, the -a
    option means to remove or mark all jobs; the -r option  without
    a  jobspec  argument  restricts operation to running jobs.  The
    return value is 0 unless a jobspec does  not  specify  a  valid
    job.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看出，我们可以用如下方式来达成我们的目的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用&lt;code&gt;disown -h jobspec&lt;/code&gt; 来使某个作业忽略HUP信号。&lt;/li&gt;
&lt;li&gt;用&lt;code&gt;disown -ah&lt;/code&gt; 来使所有的作业都忽略HUP信号。&lt;/li&gt;
&lt;li&gt;用&lt;code&gt;disown -rh&lt;/code&gt; 来使正在运行的作业忽略HUP信号。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;需要注意的是，当使用过 disown 之后，会将把目标作业从作业列表中移除，我们将不能再使用jobs来查看它，但是依然能够用ps -ef查找到它。&lt;/p&gt;
&lt;p&gt;但是还有一个问题，这种方法的操作对象是作业，如果我们在运行命令时在结尾加了&amp;rdquo;&amp;amp;&amp;ldquo;来使它成为一个作业并在后台运行，那么就万事大吉了，我们可以通过jobs命令来得到所有作业的列表。但是如果并没有把当前命令作为作业来运行，如何才能得到它的作业号呢？答案就是用 CTRL-z（按住Ctrl键的同时按住z键）了！&lt;/p&gt;
&lt;p&gt;CTRL-z 的用途就是将当前进程挂起（Suspend），然后我们就可以用jobs命令来查询它的作业号，再用&lt;code&gt;bg jobspec&lt;/code&gt;来将它放入后台并继续运行。需要注意的是，如果挂起会影响当前进程的运行结果，请慎用此方法。&lt;/p&gt;
&lt;p&gt;disown 示例1（如果提交命令时已经用“&amp;amp;”将命令放入后台运行，则可以直接使用“disown”）&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@pvcent107 build]# cp -r testLargeFile largeFile &amp;amp;
[1] 4825
[root@pvcent107 build]# jobs
[1]+  Running                 cp -i -r testLargeFile largeFile &amp;amp;
[root@pvcent107 build]# disown -h %1
[root@pvcent107 build]# ps -ef |grep largeFile
root      4825   968  1 09:46 pts/4    00:00:00 cp -i -r testLargeFile largeFile
root      4853   968  0 09:46 pts/4    00:00:00 grep largeFile
[root@pvcent107 build]# logout
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;disown 示例2（如果提交命令时未使用“&amp;amp;”将命令放入后台运行，可使用 CTRL-z 和“bg”将其放入后台，再使用“disown”）&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@pvcent107 build]# cp -r testLargeFile largeFile2
 
[1]+  Stopped                 cp -i -r testLargeFile largeFile2
[root@pvcent107 build]# bg %1
[1]+ cp -i -r testLargeFile largeFile2 &amp;amp;
[root@pvcent107 build]# jobs
[1]+  Running                 cp -i -r testLargeFile largeFile2 &amp;amp;
[root@pvcent107 build]# disown -h %1
[root@pvcent107 build]# ps -ef |grep largeFile2
root      5790  5577  1 10:04 pts/3    00:00:00 cp -i -r testLargeFile largeFile2
root      5824  5577  0 10:05 pts/3    00:00:00 grep largeFile2
[root@pvcent107 build]#
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;灵活运用-ctrl-z&#34;&gt;灵活运用 CTRL-z&lt;/h3&gt;
&lt;p&gt;在我们的日常工作中，我们可以用 CTRL-z 来将当前进程挂起到后台暂停运行，执行一些别的操作，然后再用 fg 来将挂起的进程重新放回前台（也可用 bg 来将挂起的进程放在后台）继续运行。这样我们就可以在一个终端内灵活切换运行多个任务，这一点在调试代码时尤为有用。因为将代码编辑器挂起到后台再重新放回时，光标定位仍然停留在上次挂起时的位置，避免了重新定位的麻烦。&lt;/p&gt;
&lt;h2 id=&#34;screen&#34;&gt;screen&lt;/h2&gt;
&lt;h3 id=&#34;场景-2&#34;&gt;场景：&lt;/h3&gt;
&lt;p&gt;我们已经知道了如何让进程免受 HUP 信号的影响，但是如果有大量这种命令需要在稳定的后台里运行，如何避免对每条命令都做这样的操作呢？&lt;/p&gt;
&lt;h3 id=&#34;解决方法-2&#34;&gt;解决方法：&lt;/h3&gt;
&lt;p&gt;此时最方便的方法就是 screen 了。简单的说，screen 提供了 ANSI/VT100 的终端模拟器，使它能够在一个真实终端下运行多个全屏的伪终端。screen 的参数很多，具有很强大的功能，我们在此仅介绍其常用功能以及简要分析一下为什么使用 screen 能够避免 HUP 信号的影响。我们先看一下 screen 的帮助信息：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SCREEN(1)                                                           SCREEN(1)
 
NAME
       screen - screen manager with VT100/ANSI terminal emulation
 
SYNOPSIS
       screen [ -options ] [ cmd [ args ] ]
       screen -r [[pid.]tty[.host]]
       screen -r sessionowner/[[pid.]tty[.host]]
 
DESCRIPTION
       Screen  is  a  full-screen  window manager that multiplexes a physical
       terminal between several  processes  (typically  interactive  shells).
       Each  virtual  terminal provides the functions of a DEC VT100 terminal
       and, in addition, several control functions from the  ISO  6429  (ECMA
       48,  ANSI  X3.64)  and ISO 2022 standards (e.g. insert/delete line and
       support for multiple character sets).  There is a  scrollback  history
       buffer  for  each virtual terminal and a copy-and-paste mechanism that
       allows moving text regions between windows.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用 screen 很方便，有以下几个常用选项：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用&lt;code&gt;screen -dmS session name&lt;/code&gt; 来建立一个处于断开模式下的会话（并指定其会话名）。&lt;/li&gt;
&lt;li&gt;用&lt;code&gt;screen -list&lt;/code&gt; 来列出所有会话。&lt;/li&gt;
&lt;li&gt;用&lt;code&gt;screen -r session name&lt;/code&gt; 来重新连接指定会话。&lt;/li&gt;
&lt;li&gt;用快捷键CTRL-a d 来暂时断开当前会话。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;screen 示例&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@pvcent107 ~]# screen -dmS Urumchi
[root@pvcent107 ~]# screen -list
There is a screen on:
        12842.Urumchi   (Detached)
1 Socket in /tmp/screens/S-root.
 
[root@pvcent107 ~]# screen -r Urumchi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当我们用“-r”连接到 screen 会话后，我们就可以在这个伪终端里面为所欲为，再也不用担心 HUP 信号会对我们的进程造成影响，也不用给每个命令前都加上“nohup”或者“setsid”了。这是为什么呢？让我来看一下下面两个例子吧。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;未使用 screen 时新进程的进程树&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@pvcent107 ~]# ping www.google.com &amp;amp;
[1] 9499
[root@pvcent107 ~]# pstree -H 9499
init─┬─Xvnc
     ├─acpid
     ├─atd
     ├─2*[sendmail] 
     ├─sshd─┬─sshd───bash───pstree
     │       └─sshd───bash───ping
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们可以看出，未使用 screen 时我们所处的 bash 是 sshd 的子进程，当 ssh 断开连接时，HUP 信号自然会影响到它下面的所有子进程（包括我们新建立的 ping 进程）。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;使用了 screen 后新进程的进程树&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[root@pvcent107 ~]# screen -r Urumchi
[root@pvcent107 ~]# ping www.ibm.com &amp;amp;
[1] 9488
[root@pvcent107 ~]# pstree -H 9488
init─┬─Xvnc
     ├─acpid
     ├─atd
     ├─screen───bash───ping
     ├─2*[sendmail]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而使用了 screen 后就不同了，此时 bash 是 screen 的子进程，而 screen 是 init（PID为1）的子进程。那么当 ssh 断开连接时，HUP 信号自然不会影响到 screen 下面的子进程了。&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;现在几种方法已经介绍完毕，我们可以根据不同的场景来选择不同的方案。nohup/setsid 无疑是临时需要时最方便的方法，disown 能帮助我们来事后补救当前已经在运行了的作业，而 screen 则是在大批量操作时不二的选择了。&lt;/p&gt;</description>
        </item>
        <item>
        <title>投资对象的资本结构无关性</title>
        <link>https://blog.xpgreat.com/p/irrelevance_invest/</link>
        <pubDate>Sat, 08 Jun 2019 11:23:53 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/irrelevance_invest/</guid>
        <description>&lt;p&gt;本文介绍了公司的杠杆效应（Leverage Effect）和Modigliani-Miller定理，以阐述投资对象的资本结构与价值的无关性。&lt;/p&gt;
&lt;h2 id=&#34;杠杆效应&#34;&gt;杠杆效应&lt;/h2&gt;
&lt;p&gt;一个公司的资本结构是总资产（Total capital, T) = 股权（Equity, E）+ 债务（Debt, D）。定义：\(r_T\)是总资产回报率，毛利 = \(r_T * (E+D)\)；净利 = 毛利 - 债务利息 = \(r_T * (E+D) - r_D * D := r_E * E\)。则：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
r_E * E &amp;amp;= r_T * (E+D) - r_D * D
\\r_E &amp;amp;= r_T + (r_T - r_D) * \frac{D}{E}
\\r_E &amp;amp;= r_T + (r_T - r_D) * DR
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;如果\(r_T &amp;gt; r_B\)，\(r_E\)随着负债率（Debt-to-equity ratio）上升而上升。这样看来，公司应尽可能地提高负债率以提高利润率，然而这与常识似乎有矛盾。原因在于没有考虑回报率的不确定性，即他们的方差。用期望和方差的眼光来看，假设债务的利息是确定的，可以得到：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;E(r_E) = E(r_T) * E+(E(r_T) - r_D) * D
\\&amp;amp;Var(r_E) = (D+1)^2 * Var(r_T)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;这两个式子分别表示杠杆效应和杠杆风险（Leverage Risk）。可以看出更高负债率意味着更高的股权回报率和更高的风险，且风险升高的速度更快，这是一个用高风险换高收益的典型例子。此外，当负债率升高到足够高的时候，对债权人的风险上升，债务的利息也会上升，进一步地降低回报率。那么按照一般的想法，一定存在一个均衡点，也就是一个最好的负债率，事实如此吗？&lt;/p&gt;
&lt;h2 id=&#34;modigliani-miller定理&#34;&gt;Modigliani-Miller定理&lt;/h2&gt;
&lt;p&gt;由经济学家弗兰科·莫迪利安尼和默顿·米勒提出，它是现代资本结构理论的基础。该定理认为，在不考虑税，破产成本，信息不对称并且假设在有效市场里面，企业价值不会因为企业融资方式（资本结构）改变而改变。也即是说，不论公司选择发行股票或者卖债券，或是采用不同的红利政策，都不会影响企业价值。因此莫迪尼亚尼-米勒定理也被称为资本结构无关原理。&lt;/p&gt;
&lt;p&gt;假设一个投资人投资了一个公司\(\alpha\)的股份，\(\widetilde{R}\)表示未扣除利息的公司收益，\(r * D\)表示确定的需要公司支付的债务利息（假定利率等于市场无风险利率）。此时投资人的收入为\(\alpha (\widetilde R - r * D)\)，投资头寸为\(\alpha * E_M\)（\(E_M\)是公司股权的市值。）&lt;/p&gt;
&lt;p&gt;这个投资人有自己的偏好，他更想投资一个零负债的公司，他打算出售一部分的股票投资到无风险项目中，假设新的投资比例为\(\beta\)，出售股票回流\((\alpha - \beta) * E_M\)。此时投资人的收入为：来自投资公司\(\beta (\widetilde R - r * D)\)，来自无风险投资\(r(\alpha - \beta)E_M\)，投资头寸分别为\(\beta * E_M\)和\((\alpha - \beta) * E_M\)。&lt;/p&gt;
&lt;p&gt;当\(\beta\)为多少的时候，投资人的收入和投资一个无负债公司的收入是一样的？当投资人要承担的公司债务利息部分与投资人无风险投资的利息相等的时候：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\beta * r * D &amp;amp;= r(\alpha - \beta)E_M
\\\beta * r * DR * E_M &amp;amp;= r(\alpha - \beta)E_M
\\\beta * DR &amp;amp;= \alpha - \beta
\\\beta &amp;amp;= \frac{\alpha}{1+DR}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;即投资人把投资比例调至\(\frac{\alpha}{1+DR}\)时，等价于投资了一个零负债的公司。所以，公司的负债比例与投资人能否按自己偏好进行投资没有关系，因为可以用这种方法创建完全符合投资人的投资组合。&lt;/p&gt;</description>
        </item>
        <item>
        <title>中短期电价预测模型简介</title>
        <link>https://blog.xpgreat.com/p/epf_models/</link>
        <pubDate>Sat, 25 May 2019 17:35:01 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/epf_models/</guid>
        <description>&lt;p&gt;从1981年的智利电力体制改革之后，英美等国先后进行了电力工业重组，这些努力导致了电力工业的市场化，促成了电力市场的形成，实现了资源的合理分配和社会效益的最大化。准确的电力价格对发电商、用户、监管者具有重要的意义。本文将常用的电价预测（EPF, Electricity Price Forecasting）模型进行了分类，并加以简介。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;本文主要参考了&lt;a class=&#34;link&#34; href=&#34;https://www.sciencedirect.com/science/article/pii/S0169207014001083&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;’Rafał Weron, Electricity price forecasting: A review of the state-of-the-art with a look into the future, International Journal of Forecasting, Volume 30, Issue 4, 2014, Pages 1030-1081, ISSN 0169-2070‘&lt;/a&gt;&lt;/strong&gt; 写成博文以整理思路。&lt;/p&gt;
&lt;h2 id=&#34;电力市场模式简介&#34;&gt;电力市场模式简介&lt;/h2&gt;
&lt;p&gt;首先简单介绍一下电力市场的运作模式。电力市场主要由三方构成：供应商、用户、管理者。供应商负责发电，用户买电用电，管理者维护市场秩序。由于电力是一种很特别的商品，它无法大规模储存，只能即发即用，很容易造成某一时刻的供给和需求不平衡，这种不平衡反应到价格上面，所以电价极不稳定。早期的电力市场是高度管制的，管理机构制定发电量、电价等等，十分稳定，但效率很差，浪费资源，也不利于电力市场的发展。市场化（自由化）后的电力市场引入竞争，很多供应商参与竞价，提高效率促进发展。目前市场化的电力市场基本分为两种模式，日前（Day-ahead）市场和日内（Intraday）市场。日前市场中供应商在前一天向清算机构报告自己的供应量和价格，在某一个时间点（比如18:00）清算机构整合所有供应商的信息，确定下一天的电价，一般是以小时为单位。日内市场与股票市场类似，通常是供应商提前一段时间竞价，比如12:00的时候竞价12:15-12:30的电价，一般以15分钟为单位。日内市场相较日前市场而言更加的不稳定。&lt;/p&gt;
&lt;h2 id=&#34;电价预测的分类&#34;&gt;电价预测的分类&lt;/h2&gt;
&lt;p&gt;可以分为长期、中期、短期预测。但它们之间尚没有明确的界限。长期一般研究几个月、一个季度到几年的范围，一般用于决定投资、长期协议和其他战略行动。中期一般研究几个星期到一个月的范围，主要用于资源分配、报表分析、风险管理、金融计算等。短期一般研究几分钟到几个星期的范围，用于降低生产成本、提高发电厂利润等。中短期预测使用的方法基本类似，与长期预测的不同。本文只介绍中短期的模型，长期预测日后再写。&lt;/p&gt;
&lt;h2 id=&#34;中短期电价预测的六个类型&#34;&gt;中短期电价预测的六个类型&lt;/h2&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/EPFmodelsclass.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/EPFmodelsclass.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;中短期电价预测模型众多，大体可以分为六个类型：Multi-agent, Fundamental, Reduced-form, Statistical, Computational intelligence 以及它们的组合。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-agent模型运用经济学、博弈论等学科知识，模拟电力市场的各方行为，匹配供给和需求预测价格。&lt;/li&gt;
&lt;li&gt;Fundamental模型通过模拟重要物理和经济因素对电价的影响来描述价格动态变化。&lt;/li&gt;
&lt;li&gt;Reduced-form模型描述电价随时间推移的统计特性，最终目标是金融衍生工具的评估和风险管理。&lt;/li&gt;
&lt;li&gt;Statistical模型直接运用统计学和计量经济学的方法根据前期数据进行预测。&lt;/li&gt;
&lt;li&gt;Computational intelligence模型运用机器学习结合了学习、进化、模糊化的特性，可以灵活的适应复杂的动态模型。&lt;/li&gt;
&lt;li&gt;组合模型在上述模型中进行组合，目标是结合各模型的优势，弱化劣势，取长补短。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面详细介绍前五类模型，包括该类型的优势劣势以及例子，重点是Computational intelligence模型。&lt;/p&gt;
&lt;h2 id=&#34;multi-agent模型&#34;&gt;Multi-agent模型&lt;/h2&gt;
&lt;p&gt;预测电价在过去是一项直接但困难的任务。它通常涉及中期和长期时间范围，并涉及将需求预测与供给相匹配，供给是通过依运营成本排列计划生产量得到的。这些基于成本的模型（Production-cost model，PCM）能够以一小时一小时的速度预测价格。然而，他们忽略了战略招标策略，包括市场的力量。它们适用于受监管的市场，价格不确定性很小、结构稳定、没有博弈的情况，但不适合竞争激烈的电力市场。均衡（Equilibrium）（博弈论）方法可被视为PCM的一般化方法，并结合战略投标策略进行修正。这些模型对于预测没有价格历史，但已知供应成本和市场集中度的市场的价格水平特别有用。另一方面，越来越流行的基于agent的仿真技术可以解决静态均衡模型忽略的电力市场特征。&lt;/p&gt;
&lt;p&gt;Nash-Cournot framework，均衡方法的一种。在Nash-Cournot框架中，电力被视为同质商品，市场均衡通过供应商的容量设定来确定。但是这些模型预测的价格往往高于现实中的价格。&lt;/p&gt;
&lt;p&gt;Supply function equilibrium，将价格建模为公司投标与供给（和需求）曲线的均衡。 计算供给函数均衡（SFE）需要求解一组微分方程，而不是Nash-Cournot框架中出现的典型代数方程组。 因此，这些模型在其数字易处理性方面具有相当大的限制。&lt;/p&gt;
&lt;p&gt;Strategic production-cost models，是传统生产成本模型（PCM）的衍生。 战略PCM（SPCM）将代理商（agent）的出价策略也考虑在内。 每个agent都试图最大化自己的利润，同时考虑其成本结构和竞争对手的预期行为，通过战略参数建模，该参数代表每个供应商生产水平的剩余需求函数的斜率。与前两个模型相比，SPCM的计算效率很高，这让它可用于实时的价格分析。&lt;/p&gt;
&lt;p&gt;Agent-based simulation models，是一类计算结构和规则，用于模拟自主代理（无论是个人还是集体实体，如组织或团体）的行为和交互，最终目标是评估它们对整个系统的影响。&lt;/p&gt;
&lt;p&gt;这类模型极其灵活，但这种灵活要求模型使用者自己确定很多变量，比如参与者是谁，他们可能用的策略是什么，他们互相交互的方式等。另外，multi-agent模型更多地关注的是质量问题而不是定量结果，比如他们可以预测价格是否高于边际成本，以及这个是否会影响参与者的收益；但是如果要求得到更定量的结论，比如高精度的预测价格，这类模型会产生问题。&lt;/p&gt;
&lt;h2 id=&#34;fundamental模型&#34;&gt;Fundamental模型&lt;/h2&gt;
&lt;p&gt;试图捕捉电力生产和交易中存在的基本物理量和经济指标的关系。 假设基本因素（负载，天气条件，系统参数等）之间的功能相互关联，并且通常通过Statistical，Reduced-form或Computational Intelligence独立地对基本输入进行建模和预测。 此外，文献中考虑的许多EPF方法是基于时间序列、回归和神经网络模型的混合解决方案，其使用一些Fundamental的因素 - 例如负载，燃料价格，风力或温度 - 作为输入变量。主要分为两个子类：parameter rich models 和 parsimonious structural models。&lt;/p&gt;
&lt;p&gt;Parameter-rich fundamental models，通常被开发为专有的内部产品，因此，它们的细节不会公开披露。 公布的大部分结果都与水电主导电力市场有关。顾名思义，此类模型运用很多输入参数，Vahviläinen and Pyykkönen (2005) 提出了一个模型，使用了27个标量参数（13个气候参数、4个需求和10个供给参数），并使用了29个公式来描述参数之间的关系。&lt;/p&gt;
&lt;p&gt;Parsimonious structural models是parameter-rich模型的简化，可以追溯到Barlow (2002)提出的由对市场供给和需求曲线的实证研究得出的现货价格过程，运用了Box–Cox
transformation的逆。&lt;/p&gt;
&lt;p&gt;这类模型面对的挑战主要有两点，第一是数据难以获得，根据市场的不同，关于工厂容量、成本、需求模式和运输能力等数据不一定可以供研究人员建模使用。第二是在建模的时候应用一些关于市场的假设，所以数据的随机性变化会对模型的可信度产生较大影响。另外由于这类模型使用的数据一般是在较长时间范围内获取的，所以更适合用于中期预测而不是短期价格预测。所以在应用这类模型的时候存在着很多风险。&lt;/p&gt;
&lt;h2 id=&#34;reduced-form模型&#34;&gt;Reduced-form模型&lt;/h2&gt;
&lt;p&gt;金融风格的Reduced-form（定量，随机）价格动态模型的一个共同特征是，它们的主要目的不是提供准确的每小时的价格预测，而是复制每日电价的主要特征，如未来时间点的边际分布、价格动态以及商品价格之间的相关性。这些模型是衍生品定价和风险管理系统的核心。如果选择的价格过程不适合描述电价的主要属性，则模型的结果可能不可靠。同时，如果模型过于复杂，计算负担将阻止其在交易部门中的使用（Weron，2006）。&lt;/p&gt;
&lt;p&gt;Jump-diffusion models
Markov regime-switching models&lt;/p&gt;
&lt;p&gt;通常此类模型不会准确地预测每小时价格，但可以发现电力现货价格的主要特征，通常是在每日的时间尺度下。这些模型简化但合理地描述了价格动态，并且通常用于衍生品定价和风险分析（参见Benth et al.，2008; Eydeland＆Wolyniec，2003）。 有趣的是，当涉及到波动性或价格飙升的预测时，简化模型的表现相当不错。&lt;/p&gt;
&lt;h2 id=&#34;statistical模型&#34;&gt;Statistical模型&lt;/h2&gt;
&lt;p&gt;统计模型运用数学方法组合过去的电价数据和/或外部数据，一般是生产数据、需求数据、天气数据。一般分为两类：加法模型和乘法模型。加法模型把各个参数加起来，乘法模型则是乘起来，这两种模型可以相互转换，比如对乘法模型求对数。统计模型最大的优势在于它很好解释变量的影响，方便理解模型内部的行为。然而统计模型也因为它不好支持非线性拟合而受到批评。&lt;/p&gt;
&lt;p&gt;Similar-day使用和预测当天相似的那天的数据来预测，它可以使用比如需求量、天气等数据来判断两天的相似程度。 可以使用指数平滑（exponential smoothing methods）来提高精确度，但这种模型在与其他模型比较中精确度不是很理想。&lt;/p&gt;
&lt;p&gt;回归模型是使用最广泛的一种，很好理解，这里不做过多描述。值得一提的是在建模的时候很多人使用小波变换（wavelet），可以提高精确度。&lt;/p&gt;
&lt;p&gt;AR-MA类型的时间序列分析模型，基本的时间序列分析模型。这种模型对于平稳的时间序列数据的表现比较好，所以产生了差分ARMA（又称ARIMA，AutoRegressive Integrated Moving
Average）模型，它将数据（n阶）差分化，即把\(X_t\)变化成\(X_t - X_{t-1}\)以获取平稳的时间序列（一阶差分）。另外的还可以做季节差分（SARIMA，seasonal ARIMA）等。&lt;/p&gt;
&lt;p&gt;ARX类型的时间序列分析模型，X代表eXgenous，即输入外部数据的AR模型，因为电价也受很多外源因素的影响。由此衍生了ARX, ARMAX, ARIMAX, SARIMAX等模型。&lt;/p&gt;
&lt;p&gt;Threshold autoregressive models (TAR), SETAR(Self Exciting TAR), STAR(Smooth Transition AR), LSTAR(Logistic STAR), TARX(TAR eXgenous)等等。&lt;/p&gt;
&lt;p&gt;Heteroskedasticity and GARCH-type models，异方差性和GARCH类模型。包括AutoRegressive Conditional Heteroskedastic (ARCH), Generalized ARCH (GARCH), (S)AR(IMA)-GARCH等等。&lt;/p&gt;
&lt;p&gt;统计型模型的精确度不仅取决于模型的选取、算法的使用，也取决于获取的数据的质量。对于处理价格激增，统计模型的效果不是很好，尤其是对于只是用价格作为数据的模型，对于价格spike的识别可以用以下方法：recursive filters (Cartea &amp;amp; Figueroa, 2005; Weron, 2008), variable price thresholds (Trück, Weron, &amp;amp; Wolff, 2007), &lt;del&gt;&lt;del&gt;fixed price change thresholds (Bierbrauer et al., 2004)&lt;/del&gt;&lt;/del&gt;（不推荐，因为忽视了长期和季节性变化）, regime-switching classification (RSC;Janczura et al., 2013), and wavelet filtering (Stevenson, 2001;Weron, 2006)。&lt;/p&gt;
&lt;h2 id=&#34;computational-intelligence-models&#34;&gt;Computational intelligence models&lt;/h2&gt;
&lt;p&gt;CI（有的文献中也称AI）模型是受自然事物启发产生的计算方法，结合了学习、进化、模糊化的元素，可以灵活的适应复杂的动态模型，解决传统模型不能解决的问题。&lt;/p&gt;
&lt;h3 id=&#34;ci模型的分类&#34;&gt;CI模型的分类&lt;/h3&gt;
&lt;p&gt;每一个的CI模型都可以根据它的架构和学习算法分类，架构表示神经元的连接方式，学习算法表示模型怎样根据训练数据调整优化它的各项权重。在EPF的背景下，它也可以依据输出向量的维度分类，输出一维或多维。一维的一般用于预测下一小时电价 (e.g. Gonzalez, San Roque, &amp;amp; Garcia-Gonzalez, 2005; Mandal, Senjyu, &amp;amp; Funabashi, 2006)，h小时后电价 (e.g. Amjady, 2006; Hu et al., 2008; Rodriguez &amp;amp; Anders, 2004)等等。多维的一般用于一次性预测接下来几个小时的电价，不如前者常见(e.g. Yamin, Shahidehpour, &amp;amp; Li, 2004)。根据神经元的拓扑结构可以分为两类，前馈网络（feed-forward networks）和递归网络（recurrent networks），前者不含循环而后者有。前馈网络更多的被用于预测价格而递归网络往往被用于分类(Jain, Mao, &amp;amp; Mohiuddin, 1996; Rutkowski, 2008)。&lt;/p&gt;
&lt;p&gt;此外CI模型还可以被用于区间预测（interval forecasting，与置信区间confidence interval有区别），在这里不展开说了。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/ANN20190526214838.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/ANN20190526214838.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;前馈神经网络&#34;&gt;前馈神经网络&lt;/h3&gt;
&lt;p&gt;最简单的前馈神经网络只包含一个输入层，一个输出层，中间由一个感知神经元连接，它等价于一个线性回归的模型。通过增加中间神经元（隐藏层）的层数和个数，可以做到拟合非线性的关系（MLP, Multi Layer Perceptron）。每个神经元里都有一个激活函数，可以将输入进行一些处理，常用的有ReLU，tanh，softmax等等。在激活函数中可以使用径向基函数（Raidal Basis Function），构成一类特殊的模型。径向基通常使用高斯核函数（Gaussian Kernel）。&lt;/p&gt;
&lt;p&gt;简单的MLP和RBF模型通常被当作测试对比模型，真正应用的模型大多是他们与其他模型的混合模型，比如Shafie-Khah et al. (2011) 提出了一个 wavelet-ARIMA-RBF模型，结合了小波变换、ARIMA和RBF的优缺点。研究指出，RBF更适合研究局部数据的特点，MLP适合描述整体的数据趋势。&lt;/p&gt;
&lt;h3 id=&#34;递归神经网络&#34;&gt;递归神经网络&lt;/h3&gt;
&lt;p&gt;详见另一篇博文：&lt;a class=&#34;link&#34; href=&#34;https://blog.xpgreat.com/post/rnn_lstm_gru/&#34; &gt;递归神经网络&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前馈神经网络存在一个问题，它不能描述相邻两个数据之间的关系，RRN则可以很好的解决这一点，前面cell的输出是后面cell的输入，或者有一个贯穿整个模型的状态记忆量。&lt;/p&gt;
&lt;h3 id=&#34;模糊神经网络&#34;&gt;模糊神经网络&lt;/h3&gt;
&lt;p&gt;模糊逻辑是通常的布尔逻辑的推广，因为它的输入值可以不止为0或1，还可以是某些定性范围。例如，温度可以是低，中或高。模糊逻辑允许从模糊或噪声的输入推导出输出，重要的是，不需要指定输入到输出的精确映射。模糊神经网络则是应用了模糊逻辑推广的神经网络，一般还要使用一个去模糊化函数以获取精确结果。&lt;/p&gt;
&lt;h3 id=&#34;支持向量机&#34;&gt;支持向量机&lt;/h3&gt;
&lt;p&gt;向量机的讲解推荐&lt;a class=&#34;link&#34; href=&#34;https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E9%80%9A%E4%BF%97%E5%AF%BC%E8%AE%BA%EF%BC%88%E7%90%86%E8%A7%A3SVM%E7%9A%84%E4%B8%89%E5%B1%82%E5%A2%83%E7%95%8C%EF%BC%89LaTeX%E6%9C%80%E6%96%B0%E7%89%88_2015.1.9.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;支持向量机通俗导论（理解SVM的三层境界）&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;与ANN不同，SVM通过把数据非线性投影到高维度，再对投影后的数据使用简单的线性函数以拟合数据的非线性关系，而ANN是通过复杂的函数来拟合非线性关系。SVM的一个优点是它得出的是全局的最值，而不是ANN给出的局部最值（因为一般使用梯度下降优化）。&lt;/p&gt;
&lt;h3 id=&#34;优势和劣势&#34;&gt;优势和劣势&lt;/h3&gt;
&lt;p&gt;主要的优势是可以拟合复杂的、非线性的关系，因此它通常比纯统计学的模型效果要更好一些。但同时它不太灵活，这是它的主要缺陷。另外，CI模型种类繁多，存在很多变种，而且不同模型间难以比较（参数、输入数据等经常不同），参数、激活函数选择众多，所以很难选择一个最好的模型。&lt;/p&gt;
&lt;h2 id=&#34;后记&#34;&gt;后记&lt;/h2&gt;
&lt;p&gt;电价预测是一个涉及广泛的研究领域，永远都无法找到一个最令人满意的预测方法，数据精度、模型调参、模型间的组合多种多样······不管是预测什么东西都是这样，也正是因为其中存在的不确定性和创造性，这个领域才如此的有吸引力。&lt;/p&gt;</description>
        </item>
        <item>
        <title>递归神经网络</title>
        <link>https://blog.xpgreat.com/p/rnn_lstm_gru/</link>
        <pubDate>Tue, 14 May 2019 10:48:15 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/rnn_lstm_gru/</guid>
        <description>&lt;p&gt;递归神经网络是神经网络的一种，与其他神经网络不同点在于，它可以很好地处理序列数据，即前面的数据的输入与后面的输出是有关系的，比如一句话的语义理解。本文介绍了递归神经网络及其两个变种——LSTM和GRU，目的是梳理一下自己所理解的RRN，也希望能够给初次接触RRN的同学一点帮助。&lt;/p&gt;
&lt;h2 id=&#34;其他神经网络的不足为什么需要rrn&#34;&gt;其他神经网络的不足——为什么需要RRN？&lt;/h2&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/1_SL8FESMwzSy6QTrcIzcRYw.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/1_SL8FESMwzSy6QTrcIzcRYw.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;传统神经网络的结构大多为输入-隐藏层（多个）-输出，即给定一组输入数据，输出结果，隐藏层像一个黑盒子，通过各种运算把输入的数据算成需要的结果。输入不同的数据运行两次，它们的结果是完全无关的。而有的时候我们需要处理一个序列，比如一个句子，它是单词的序列，要理解这句话中各个单词的意思，必须要结合语境，也就是要结合其他的单词的意思，一般情况下要结合这个单词前面的单词。例如前面两个词是我、吃，那么后面的一个词很大概率是一个表示食物的名词。&lt;/p&gt;
&lt;p&gt;为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就诞生了。&lt;/p&gt;
&lt;h2 id=&#34;rnn的结构&#34;&gt;RNN的结构&lt;/h2&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/v2-b0175ebd3419f9a11a3d0d8b00e28675_r.jpg&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/v2-b0175ebd3419f9a11a3d0d8b00e28675_r.jpg&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;循环神经网络也由输入层、隐藏层和输出层构成，这是神经网络的基本结构，不同的是它的隐藏层（也叫cell）可以连续的接收输入\(x\)，输出\(o\)，而贯穿其中的\(s\)可以存储这个网络的状态，也就是“记忆”到的前面发生的事情，利用它可以更加精准的预测后面的输出。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/RNN20190514210119.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/RNN20190514210119.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;
一个最简单的RNN结构如图所示，每一个cell输入一个\(x\)和一个\(s\)，输出两个同样的\(h\)，通过下列算法算出：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;s_t = \tanh (Ws_{t-1} + Ux_t)
\\&amp;amp;h_t = Vs_t
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;这种简单的结构虽然可以达到记忆的效果，然而存在着一个致命的问题：&lt;strong&gt;梯度爆炸&lt;/strong&gt;和&lt;strong&gt;梯度消失&lt;/strong&gt;。即在多次循环后，利用梯度下降法调整，在求导的时候会出现\(\frac{\partial Loss}{\partial w_i} = \frac{\partial Loss}{\partial f_t}\frac{\partial f_t}{\partial f_{t-1}}&amp;hellip;\frac{\partial f_i}{\partial w_i} \)，如果\(\frac{\partial f_t}{\partial f_{t-1}}\)部分大于或小于1，会出现类似1.01的99次方（或0.99的99次方）的问题，使得权重过大或过小，无法使用梯度下降。这一问题的根源就在于\(W\)和\(s\)之间的乘法运算。为了解决这一问题，&lt;a class=&#34;link&#34; href=&#34;https://blog.xpgreat.com/file/lstm.pdf&#34; &gt;Hochreiter, Schmidhuber（1997)&lt;/a&gt;提出了LSTM。&lt;/p&gt;
&lt;h2 id=&#34;lstm&#34;&gt;LSTM&lt;/h2&gt;
&lt;p&gt;LSTM全称Long Short Time Memory RRN，长短期记忆循环神经网络，它本质上也是一个循环神经网络，只是cell不一样而已，如下图：
&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/LSTM20190514210208.png&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/LSTM20190514210208.png&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;LSTM的关键之处在于cell的状态，也就是图中贯穿顶部的那条水平线。Cell的状态像是一条传送带，它贯穿整条链，其中只发生一些小的线性作用。信息流过这条线而不改变是非常容易的。但是，LSTM也有能力移除或增加信息到cell状态中，由被称为门的结构精细控制。门是一种让信息可选地通过的方法。它们由一个sigmoid(\( S(t)={\frac {1}{1+e^{-t}}}\))神经网络层和一个点乘操作组成。这里的sigmoid函数取0-1的值，充当了开关的作用，控制影响的程度。&lt;/p&gt;
&lt;h3 id=&#34;运行过程&#34;&gt;运行过程&lt;/h3&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/v2-d7a679e66345c250ebcf9d53ba5be867_r.jpg&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/v2-d7a679e66345c250ebcf9d53ba5be867_r.jpg&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;
从左往右看这幅图，首先第一步是决定我们需要从cell状态中扔掉什么样的信息。这个决策由一个称为“遗忘门（forget gate）”的sigmoid层决定。输入 \(h_{t-1}\) 和 \(x_t\) ，输出一个0和1之间的数。1代表“完全保留这个值”，而0代表“完全扔掉这个值”。比如对于一个基于上文预测最后一个词的语言模型。cell的状态可能包含当前主题的信息，来预测下一个准确的词。而当我们得到一个新的语言主题的时候，我们会想要遗忘旧的主题的记忆，应用新的语言主题的信息来预测准确的词。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/v2-535a2df5db8ac715ee373209d7cbd346_r.jpg&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/v2-535a2df5db8ac715ee373209d7cbd346_r.jpg&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;
第二步是决定我们需要在cell state里存储什么样的信息。这个问题有两个部分。第一，一个sigmoid层调用“输入门（input gate）”以决定哪些数据是需要更新的。然后，一个\(tanh\)层为新的候选值创建一个向量 \(\tilde{C}_t\) ，这些值能够加入state中。下一步，我们要将这两个部分合并以创建对state的更新。&lt;/p&gt;
&lt;p&gt;比如还是语言模型，可以表示为想要把新的语言主题的信息加入到cell state中，以替代我们要遗忘的旧的记忆信息。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/v2-f2dccb5533133c5ff61a58a0e1a752ab_r.jpg&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/v2-f2dccb5533133c5ff61a58a0e1a752ab_r.jpg&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;
在决定需要遗忘和需要加入的记忆之后，就可以更新旧的cell state \(C_{t-1}\) 到新的cell state \(C_t\) 了。在这一步，我们把旧的state  \(C_{t-1}\) 与 \(f_t\) 相乘，遗忘我们先前决定遗忘的东西，然后我们加上 \(i_t * \tilde{C}_t\) ，这可以理解为新的记忆信息，当然，这里体现了对状态值的更新度是有限制的，我们可以把 \(i_t\)  当成一个权重。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/v2-9b77a873ce830ea4a3bdb0385fe275ef_r.jpg&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/v2-9b77a873ce830ea4a3bdb0385fe275ef_r.jpg&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;
最后，我们需要决定要输出的东西。这个输出基于我们的cell state，但会是一个过滤后的值。首先，我们运行一个sigmoid层，这个也就是输出门（output gate），以决定cell state中的那个部分是我们将要输出的。然后我们把cell state放进\(tanh\)（将数值压到-1和1之间），最后将它与sigmoid门的输出相乘，这样我们就只输出了我们想要的部分了。&lt;/p&gt;
&lt;p&gt;在LSTM中，状态\(S\)通过累加的方式来计算，\(S_t = \sum_{\tau=1}^t \Delta S_{\tau}\)，这样就不会是一直复合函数的形式了，它的导数也不是乘积的形式，这样就不会发生梯度消失的情况了。（具体论文：An Empirical Exploration of Recurrent Network Architectures 和 Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling）&lt;/p&gt;
&lt;h2 id=&#34;gru&#34;&gt;GRU&lt;/h2&gt;
&lt;p&gt;GRU全称Gated Recurrent Unit，循环门单元。和LSTM一样，他也只是个cell的种类，本质上是RNN，由 &lt;a class=&#34;link&#34; href=&#34;https://blog.xpgreat.com/file/gru.pdf&#34; &gt;Cho, et al. (2014)&lt;/a&gt;提出。它将LSTM的遗忘门和输入门组合成了一个新的更新门，合并了cell state和hidden state，比LSTM更加简单（尽管稍微难理解一些）。&lt;/p&gt;
&lt;p&gt;&lt;figure 
	&gt;
	&lt;a href=&#34;https://blog.xpgreat.com/media/v2-f2716bc289734d8b545926b38a224692_r.jpg&#34; &gt;
		&lt;img src=&#34;https://blog.xpgreat.com/media/v2-f2716bc289734d8b545926b38a224692_r.jpg&#34;
			
			
			
			loading=&#34;lazy&#34;
			&gt;
	&lt;/a&gt;
	
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;从左往右看，前两个门是reset gate和update gate，计算方法：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;r_t = \sigma (W^rx_t + U^rh_{t-1})
\\&amp;amp;z_t = \sigma (W^zx_t + U^zh_{t-1})
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;reset gate控制在计算候选状态时使用多少前序的状态，update gate则表示计算新状态时候选状态和原状态各取多少比例（注意特殊的“1-”门，表示把\(z_t\)当作一个比例）。&lt;/p&gt;
&lt;h2 id=&#34;总结和比较&#34;&gt;总结和比较&lt;/h2&gt;
&lt;p&gt;RRN是神经网络的一个种类，LSTM和GRU是两种特殊的cell。与最基本的cell结构相比，LSTM和GRU都很好的解决了梯度爆炸和梯度消失的问题。对比LSTM，GRU的参数更少，因而训练稍快或需要更少的数据来泛化。另一方面，如果有足够的数据，LSTM的强大表达能力可能会产生更好的结果。&lt;a class=&#34;link&#34; href=&#34;https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1503.04069.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Greff, et al. (2015)&lt;/a&gt;对流行的变种做了一个很好的比较，发现它们都是一样的。&lt;a class=&#34;link&#34; href=&#34;https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/jozefowicz15.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jozefowicz, et al.(2015)&lt;/a&gt;测试了超过一万中RNN结构，发现某些任务情形下，有些比LSTM工作得更好，但那也是比较特殊的时候。&lt;/p&gt;
&lt;p&gt;总的来说，没有通用的最好的模型，只能通过具体的数据和问题来选择模型。如果研究的数据序列会相互影响，比如做词语预测，那么LSTM-RRN和GRU-RRN不失为一种很好的选择。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;参考：&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/34203833&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;深入理解lstm及其变种gru&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Understanding LSTM Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>英文论文写作句型</title>
        <link>https://blog.xpgreat.com/p/eng_thesis_voc/</link>
        <pubDate>Wed, 08 May 2019 22:13:27 +0200</pubDate>
        
        <guid>https://blog.xpgreat.com/p/eng_thesis_voc/</guid>
        <description>&lt;p&gt;最近终于开始写毕业论文了，要求用英语，从网上搜集了一下英语论文的句型，整理后发布到博客，方便查阅。&lt;/p&gt;
&lt;h2 id=&#34;链接&#34;&gt;链接&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://writingcenter.unc.edu/tips-and-tools/thesis-statements/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Thesis Statements from UNC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://blog.sciencenet.cn/home.php?mod=space&amp;amp;uid=46212&amp;amp;do=blog&amp;amp;id=349932&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;如何提高英文的科研写作能力&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.zhihu.com/question/23684933/answer/125467391&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;如何开始写英文论文？ - 李忆非的回答 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.zhihu.com/question/23684933/answer/617397421&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;如何开始写英文论文？ - 肉山大猫王的回答 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.xpgreat.com/file/report_template_eng.pdf&#34; &gt;English Report Template&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;beginning&#34;&gt;Beginning&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;In this paper, we focus on the need for&lt;/li&gt;
&lt;li&gt;This paper proceeds as follow.&lt;/li&gt;
&lt;li&gt;The structure of the paper is as follows.&lt;/li&gt;
&lt;li&gt;In this paper, we shall first briefly introduce fuzzy sets and related concepts&lt;/li&gt;
&lt;li&gt;To begin with we will provide a brief background on the&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;This will be followed by a description of the fuzzy nature of the problem and a detailed presentation of how the required membership functions are defined.&lt;/li&gt;
&lt;li&gt;Details on xx and xx are discussed in later sections.&lt;/li&gt;
&lt;li&gt;In the next section, after a statement of the basic problem, various situations involving possibility knowledge are investigated: first, an entirely possibility model is proposed; then the cases of a fuzzy service time with stochastic arrivals and non fuzzy service rule is studied; lastly, fuzzy service rule are considered.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;review&#34;&gt;Review&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;This review is followed by an introduction.&lt;/li&gt;
&lt;li&gt;A brief summary of some of the relevant concepts in xxx and xxx is presented in Section 2.&lt;/li&gt;
&lt;li&gt;In the next section, a brief review of the &amp;hellip;. is given.&lt;/li&gt;
&lt;li&gt;In the next section, a short review of &amp;hellip; is given with special regard to &amp;hellip;&lt;/li&gt;
&lt;li&gt;Section 2 reviews relevant research related to xx.&lt;/li&gt;
&lt;li&gt;Section 1.1 briefly surveys the motivation for a methodology of action, while 1.2 looks at the difficulties posed by the complexity of systems and outlines the need for development of possibility methods.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;body&#34;&gt;Body&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Section 1 defines the notion of robustness, and argues for its importance.&lt;/li&gt;
&lt;li&gt;Section 1 devoted to the basic aspects of the FLC decision making logic.&lt;/li&gt;
&lt;li&gt;Section 2 gives the background of the problem which includes xxx&lt;/li&gt;
&lt;li&gt;Section 2 discusses some problems with and approaches to, natural language understanding.&lt;/li&gt;
&lt;li&gt;Section 2 explains how flexibility which often &amp;hellip; can be expressed in terms of fuzzy time window&lt;/li&gt;
&lt;li&gt;Section 3 discusses the aspects of fuzzy set theory that are used in the &amp;hellip;&lt;/li&gt;
&lt;li&gt;Section 3 describes the system itself in a general way, including the ….. and also discusses how to evaluate system performance.&lt;/li&gt;
&lt;li&gt;Section 3 describes a new measure of xx.&lt;/li&gt;
&lt;li&gt;Section 3 demonstrates the use of fuzzy possibility theory in the analysis of xx.&lt;/li&gt;
&lt;li&gt;Section 3 is a fine description of fuzzy formulation of human decision.&lt;/li&gt;
&lt;li&gt;Section 3, is developed to the modeling and processing of fuzzy decision rules&lt;/li&gt;
&lt;li&gt;The main idea of the FLC is described in Section 3 while Section 4 describes the xx strategies.&lt;/li&gt;
&lt;li&gt;Section 3 and 4 show experimental studies for verifying the proposed model.&lt;/li&gt;
&lt;li&gt;Section 4 discusses a previous fuzzy set based approach to cost variance investigation.&lt;/li&gt;
&lt;li&gt;Section 4 gives a specific example of xxx.&lt;/li&gt;
&lt;li&gt;Section 4 is the experimental study to make a fuzzy model of memory process.&lt;/li&gt;
&lt;li&gt;Section 4 contains a discussion of the implication of the results of Section 2 and 3.&lt;/li&gt;
&lt;li&gt;Section 4 applies this fuzzy measure to the analysis of xx and illustrate its use on experimental data.&lt;/li&gt;
&lt;li&gt;Section 5 presents the primary results of the paper: a fuzzy set model ..&lt;/li&gt;
&lt;li&gt;Section 5 contains some conclusions plus some ideas for further work.&lt;/li&gt;
&lt;li&gt;Section 6 illustrates the model with an example.&lt;/li&gt;
&lt;li&gt;Various ways of justification and the reasons for their choice are discussed very briefly in Section 2.&lt;/li&gt;
&lt;li&gt;In Section 2 are presented the block diagram expression of a whole model of human DM system&lt;/li&gt;
&lt;li&gt;In Section 2 we shall list a collection of basic assumptions which a &amp;hellip; scheme must satisfy.&lt;/li&gt;
&lt;li&gt;In Section 2 of this paper, we present representation and uniqueness theorems for the fundamental measurement of fuzziness when the domain of discourse is order dense.&lt;/li&gt;
&lt;li&gt;In Section 3, we describe the preliminary results of an empirical study currently in progress to verify the measurement model and to construct membership functions.&lt;/li&gt;
&lt;li&gt;In Section 5 is analyzed the inference process through the two kinds of inference experiments&amp;hellip;
This Section&lt;/li&gt;
&lt;li&gt;In this section, the characteristics and environment under which MRP is designed are described.&lt;/li&gt;
&lt;li&gt;We will provide in this section basic terminologies and notations which are necessary for the understanding of subsequent results.Next Section&lt;/li&gt;
&lt;li&gt;The next section describes the mathematics that goes into the computer implementation of such fuzzy logic statements.&lt;/li&gt;
&lt;li&gt;However, it is cumbersome for this purpose and in practical applications the formulae were rearranged and simplified as discussed in the next section.&lt;/li&gt;
&lt;li&gt;The three components will be described in the next two section, and an example of xx analysis of a computer information system will then illustrate their use.&lt;/li&gt;
&lt;li&gt;We can interpret the results of Experiments I and II as in the following sections.&lt;/li&gt;
&lt;li&gt;The next section summarizes the method in a from that is useful for arguments based on xx&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;This paper concludes with a discussion of future research consideration in section 5.&lt;/li&gt;
&lt;li&gt;Section 5 summarizes the results of this investigation.&lt;/li&gt;
&lt;li&gt;Section 5 gives the conclusions and future directions of research.&lt;/li&gt;
&lt;li&gt;Section 7 provides a summary and a discussion of some extensions of the paper.&lt;/li&gt;
&lt;li&gt;Finally, conclusions and future work are summarized&lt;/li&gt;
&lt;li&gt;The basic questions posed above are then discussed and conclusions are drawn.&lt;/li&gt;
&lt;li&gt;Section 7 is the conclusion of the paper.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;chapter-0-abstract&#34;&gt;Chapter 0. Abstract&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A basic problem in the design of xx is presented by the choice of a xx rate for the measurement of experimental variables.&lt;/li&gt;
&lt;li&gt;This paper examines a new measure of xx in xx based on fuzzy mathematics which overcomes the difficulties found in other xx measures.&lt;/li&gt;
&lt;li&gt;This paper describes a system for the analysis of the xx.&lt;/li&gt;
&lt;li&gt;The method involves the construction of xx from fuzzy relations.&lt;/li&gt;
&lt;li&gt;The procedure is useful in analyzing how groups reach a decision.&lt;/li&gt;
&lt;li&gt;The technique used is to employ a newly developed and versatile xx algorithm.&lt;/li&gt;
&lt;li&gt;The usefulness of xx is also considered.&lt;/li&gt;
&lt;li&gt;A brief methodology used in xx is discussed.&lt;/li&gt;
&lt;li&gt;The analysis is useful in xx and xx problem.&lt;/li&gt;
&lt;li&gt;A model is developed for a xx analysis using fuzzy matrices.&lt;/li&gt;
&lt;li&gt;Algorithms to combine these estimates and produce a xx are presented and justified.&lt;/li&gt;
&lt;li&gt;The use of the method is discussed and an example is given.&lt;/li&gt;
&lt;li&gt;Results of an experimental applications of this xx analysis procedure are given to illustrate the proposed technique.&lt;/li&gt;
&lt;li&gt;This paper analyses problems in&lt;/li&gt;
&lt;li&gt;This paper outlines the functions carried out by &amp;hellip;&lt;/li&gt;
&lt;li&gt;This paper includes an illustration of the &amp;hellip;&lt;/li&gt;
&lt;li&gt;This paper provides an overview and information useful for approaching&lt;/li&gt;
&lt;li&gt;Emphasis is placed on the construction of a criterion function by which the xx in achieving a hierarchical system of objectives are evaluated.&lt;/li&gt;
&lt;li&gt;The main emphasis is placed on the problem of xx&lt;/li&gt;
&lt;li&gt;Our proposed model is verified through experimental study.&lt;/li&gt;
&lt;li&gt;The experimental results reveal interesting examples of fuzzy phases of: xx, xx&lt;/li&gt;
&lt;li&gt;The compatibility of a project in terms of cost, and xx are likewise represented by linguistic variables.&lt;/li&gt;
&lt;li&gt;A didactic example is included to illustrate the computational procedure&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;chapter-1-introduction&#34;&gt;Chapter 1. Introduction&lt;/h2&gt;
&lt;h3 id=&#34;time&#34;&gt;Time&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Over the course of the past 30 years, .. has emerged form intuitive&lt;/li&gt;
&lt;li&gt;Technological revolutions have recently hit the industrial world&lt;/li&gt;
&lt;li&gt;The advent of &amp;hellip; systems for has had a significant impact on the&lt;/li&gt;
&lt;li&gt;The development of &amp;hellip; is explored&lt;/li&gt;
&lt;li&gt;During the past decade, the theory of fuzzy sets has developed in a variety of directions&lt;/li&gt;
&lt;li&gt;The concept of xx was investigated quite intensively in recent years&lt;/li&gt;
&lt;li&gt;There has been a turning point in &amp;hellip; methodology in accordance with the advent of &amp;hellip;&lt;/li&gt;
&lt;li&gt;A major concern in &amp;hellip; today is to continue to improve&amp;hellip;&lt;/li&gt;
&lt;li&gt;A xx is a latecomer in the part representation arena.&lt;/li&gt;
&lt;li&gt;At the time of this writing, there is still no standard way of xx&lt;/li&gt;
&lt;li&gt;Although a lot of effort is being spent on improving these weaknesses, the efficient and effective method has yet to be developed.&lt;/li&gt;
&lt;li&gt;The pioneer work can be traced to xx [1965].&lt;/li&gt;
&lt;li&gt;To date, none of the methods developed is perfect and all are far from ready to be used in commercial systems.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;objective--goal--purpose&#34;&gt;Objective / Goal / Purpose&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The purpose of the inference engine can be outlined as follows:&lt;/li&gt;
&lt;li&gt;The ultimate goal of the xx system is to allow the non experts to utilize the existing knowledge in the area of manual handling of loads, and to provide intelligent, computer aided instruction for xxx.&lt;/li&gt;
&lt;li&gt;The paper concerns the development of a xx&lt;/li&gt;
&lt;li&gt;The scope of this research lies in&lt;/li&gt;
&lt;li&gt;The main theme of the paper is the application of rule based decision making.&lt;/li&gt;
&lt;li&gt;These objectives are to be met with such thoroughness and confidence as to permit &amp;hellip;&lt;/li&gt;
&lt;li&gt;The objectives of the &amp;hellip; operations study are as follows:&lt;/li&gt;
&lt;li&gt;The primary purpose/consideration/objective of&lt;/li&gt;
&lt;li&gt;The ultimate goal of this concept is to provide&lt;/li&gt;
&lt;li&gt;The main objective of such a &amp;hellip; system is to&lt;/li&gt;
&lt;li&gt;The aim of this paper is to provide methods to construct such probability distribution.&lt;/li&gt;
&lt;li&gt;In order to achieve these objectives, an xx must meet the following requirements:&lt;/li&gt;
&lt;li&gt;In order to take advantage of their similarity&lt;/li&gt;
&lt;li&gt;more research is still required before final goal of &amp;hellip; can be completed&lt;/li&gt;
&lt;li&gt;In this trial, the objective is to generate&amp;hellip;&lt;/li&gt;
&lt;li&gt;for the sake of concentrating on &amp;hellip; research issues&lt;/li&gt;
&lt;li&gt;A major goal of this report is to extend the utilization of a recently developed procedure for the xx.&lt;/li&gt;
&lt;li&gt;For an illustrative purpose, four well known OR problems are studied in presence of fuzzy data: xx.&lt;/li&gt;
&lt;li&gt;A major thrust of the paper is to discuss approaches and strategies for structuring ..methods&lt;/li&gt;
&lt;li&gt;This illustration points out the need to specify&lt;/li&gt;
&lt;li&gt;The ultimate goal is both descriptive and prescriptive.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;chapter-2-literature-review&#34;&gt;Chapter 2. Literature Review&lt;/h2&gt;
&lt;ol start=&#34;23&#34;&gt;
&lt;li&gt;A wealth of information is to be found in the statistics literature, for example, regarding xx&lt;/li&gt;
&lt;li&gt;A considerable amount of research has been done .. during the last decade&lt;/li&gt;
&lt;li&gt;A great number of studies report on the treatment of uncertainties associated with xx.&lt;/li&gt;
&lt;li&gt;There is considerable amount of literature on planning&lt;/li&gt;
&lt;li&gt;However, these studies do not provide much attention to uncertainty in xx.&lt;/li&gt;
&lt;li&gt;Since then, the subject has been extensively explored and it is still under investigation as well in
methodological aspects as in concrete applications.&lt;/li&gt;
&lt;li&gt;Many research studies have been carried out on this topic.&lt;/li&gt;
&lt;li&gt;Problem of xx draws recently more and more attention of system analysis.&lt;/li&gt;
&lt;li&gt;Attempts to resolve this dilemma have resulted in the development of&lt;/li&gt;
&lt;li&gt;Many complex processes unfortunately, do not yield to this design procedure and have, therefore, not yet been automated.&lt;/li&gt;
&lt;li&gt;Most of the methods developed so far are deterministic and /or probabilistic in nature.&lt;/li&gt;
&lt;li&gt;The central issue in all these studies is to&lt;/li&gt;
&lt;li&gt;The problem of xx has been studied by other investigators, however, these studies have been based upon classical statistical approaches.&lt;/li&gt;
&lt;li&gt;Applied &amp;hellip; techniques to&lt;/li&gt;
&lt;li&gt;Characterized the &amp;hellip; system as&lt;/li&gt;
&lt;li&gt;Developed an algorithm to&lt;/li&gt;
&lt;li&gt;Developed a system called &amp;hellip; which&lt;/li&gt;
&lt;li&gt;Uses an iterative algorithm to deduce&lt;/li&gt;
&lt;li&gt;Emphasized the need to&lt;/li&gt;
&lt;li&gt;Identifies six key issues surrounding high technology&lt;/li&gt;
&lt;li&gt;A comprehensive study of the&amp;hellip; has been undertaken&lt;/li&gt;
&lt;li&gt;Much work has been reported recently in these filed&lt;/li&gt;
&lt;li&gt;Proposed/Presented/State that/Described/Illustrated/
Indicated/Has shown / showed/Address/Highlights&lt;/li&gt;
&lt;li&gt;Point out that the problem of&lt;/li&gt;
&lt;li&gt;A study on &amp;hellip;was done / developed by []&lt;/li&gt;
&lt;li&gt;Previous work, such as [] and [], deal only with&lt;/li&gt;
&lt;li&gt;The approach taken by [] is&lt;/li&gt;
&lt;li&gt;The system developed by [] consists&lt;/li&gt;
&lt;li&gt;A paper relevant to this research was published by []&lt;/li&gt;
&lt;li&gt;[]&amp;rsquo;s model requires consideration of&amp;hellip;&lt;/li&gt;
&lt;li&gt;[]&#39; model draws attention to evolution in human development&lt;/li&gt;
&lt;li&gt;[]&amp;rsquo;s model focuses on&amp;hellip;&lt;/li&gt;
&lt;li&gt;Little research has been conducted in applying &amp;hellip; to&lt;/li&gt;
&lt;li&gt;The published information that is relevant to this research&amp;hellip;&lt;/li&gt;
&lt;li&gt;This study further shows that&lt;/li&gt;
&lt;li&gt;Their work is based on the principle of&lt;/li&gt;
&lt;li&gt;More history of &amp;hellip; can be found in xx et al. [1979].&lt;/li&gt;
&lt;li&gt;Studies have been completed to established&lt;/li&gt;
&lt;li&gt;The &amp;hellip;studies indicated that&lt;/li&gt;
&lt;li&gt;Though application of xx in the filed of xx has proliferated in recent years, effort in analyzing xx, especially xx, is lacking.
Problem / Issue / Question&lt;/li&gt;
&lt;li&gt;Unfortunately, real-world engineering problems such as manufacturing planning do not fit well with this narrowly defined model. They tend to span broad activities and require consideration of multiple aspects.&lt;/li&gt;
&lt;li&gt;Remedy / solve / alleviate these problems&lt;/li&gt;
&lt;li&gt;&amp;hellip; is a difficult problem, yet to be adequately resolved&lt;/li&gt;
&lt;li&gt;Two major problems have yet to be addressed&lt;/li&gt;
&lt;li&gt;An unanswered question&lt;/li&gt;
&lt;li&gt;This problem in essence involves using x to obtain a solution.&lt;/li&gt;
&lt;li&gt;An additional research issue to be tackled is &amp;hellip;.&lt;/li&gt;
&lt;li&gt;Some important issues in developing a &amp;hellip; system are discussed&lt;/li&gt;
&lt;li&gt;The three prime issues can be summarized:&lt;/li&gt;
&lt;li&gt;The situation leads to the problem of how to determine the &amp;hellip;&lt;/li&gt;
&lt;li&gt;There have been many attempts to&lt;/li&gt;
&lt;li&gt;It is expected to be serious barrier to&lt;/li&gt;
&lt;li&gt;It offers a simple solution in a limited domain for a complex&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
        <item>
        <title>机器学习中的矩阵求导</title>
        <link>https://blog.xpgreat.com/p/linalg_in_ml/</link>
        <pubDate>Thu, 17 Jan 2019 21:47:14 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/linalg_in_ml/</guid>
        <description>&lt;p&gt;在学习机器学习的算法时，推导过程中往往会涉及矩阵或向量求导以及一些其他的线性代数的知识。比如在推导LDA算法的时候，就是把Fisher&amp;rsquo;s Criterion求导后置零得到的结果。另外在优化的时候经常会使用到梯度下降法，这与矩阵向量求导也是分不开的。&lt;/p&gt;
&lt;h2 id=&#34;记号&#34;&gt;记号&lt;/h2&gt;
&lt;p&gt;在这篇文章中，向量统一为列向量，用\(\mathbf x\)表示，矩阵用\(X\)大写字母表示，标量用\(x\)表示。求导使用\(\frac{\partial f(\mathbf x)}{\partial \mathbf x}\)表示。&lt;/p&gt;
&lt;h2 id=&#34;线性代数的一些变换&#34;&gt;线性代数的一些变换&lt;/h2&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;1.\ (AB)^{-1} = B^{-1}A^{-1}\\
&amp;amp;2.\ (A^T)^{-1} = (A^{-1})^T\\
&amp;amp;3.\ AB = C \Leftrightarrow DAB = DC\\
&amp;amp;4.\ \left[
\begin{matrix}
a &amp;amp; b\\
c &amp;amp; d
\end{matrix}
\right]^{-1} = \frac{1}{ad-bc}\left[
\begin{matrix}
d &amp;amp; -b\\
-c &amp;amp; a
\end{matrix}
\right] \\
&amp;amp;5.\ (A+B)^T = A^T + B^T\\
&amp;amp;6.\ (AB)^T = B^TA^T \\
&amp;amp;7.\ A = A^{\frac{1}{2}}A^{\frac{1}{2}} \\
&amp;amp;8.\ A^0 = I
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;正交矩阵&#34;&gt;正交矩阵&lt;/h3&gt;
&lt;p&gt;正交矩阵（orthogonal matrix）是一个方块矩阵，其元素为实数，而且行与列皆为正交的单位向量，使得该矩阵的转置矩阵为其逆矩阵：\(S^{-1} = S^T\)。比如恒等变换：&lt;/p&gt;
&lt;p&gt;$$
\left[
\begin{matrix}
1 &amp;amp; 0\\
0 &amp;amp; 1
\end{matrix}
\right]
$$&lt;/p&gt;
&lt;p&gt;和旋转\(\alpha \)：&lt;/p&gt;
&lt;p&gt;$$
\left[
\begin{matrix}
\cos \alpha &amp;amp; -\sin \alpha \\
\sin \alpha &amp;amp; \cos \alpha
\end{matrix}
\right]
$$&lt;/p&gt;
&lt;h3 id=&#34;eigenvalue特征值&#34;&gt;Eigenvalue（特征值）&lt;/h3&gt;
&lt;p&gt;若&lt;/p&gt;
&lt;p&gt;$$
A\mathbf u = \lambda \mathbf u
$$&lt;/p&gt;
&lt;p&gt;则\(\lambda\)是特征值，\(\mathbf u\)是其对应的特征向量。&lt;/p&gt;
&lt;p&gt;由此可以推出特征值分解，即对于对称矩阵\(A\)，可以被分解为：&lt;/p&gt;
&lt;p&gt;$$
A = U \Lambda U^T
$$&lt;/p&gt;
&lt;p&gt;其中\(U\)是正交矩阵，它的列是\(A\)的特征向量，\(\Lambda\)是对角矩阵，是对应的\(A\)的特征值。&lt;/p&gt;
&lt;h2 id=&#34;变量多次出现的求导方法&#34;&gt;变量多次出现的求导方法&lt;/h2&gt;
&lt;p&gt;若在函数表达式中，某个变量出现了多次，可以单独计算函数对自变量的每一次出现的导数，再把结果加起来。&lt;/p&gt;
&lt;p&gt;举例（该规则对向量和矩阵也是成立的，这里先用标量举一个简单的例子）：假设函数表达式是\(f(x)=(2x+1)x+x^2\)，可以先把三个\(x\)看成三个不同的变量，即把\(f\)的表达式看成\((2x_1+1)x_2+x_3^2\)，然后分别计算\(\partial f / \partial x_1 = 2x_2\),\(\partial f / \partial x_2 = 2x_1 + 1\)，和\(\partial f / \partial x_3 = 2x_3\)，最后总的导数就是这三项加起来：\(2x_2 + 2x_1 + 1 + 2x_3\)，此时再把\(x\)的下标抹掉并化简，就得到\(6x+1\)。熟悉这个过程之后，可以省掉添加下标再移除的过程。&lt;/p&gt;
&lt;p&gt;这个方法和多层神经网络算法中的反向传递（Back Propagation）的思想方法是一样的。&lt;/p&gt;
&lt;h2 id=&#34;最常用的四个矩阵向量求导公式&#34;&gt;最常用的四个矩阵向量求导公式&lt;/h2&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;1.\ \frac{\partial A\mathbf x}{\partial \mathbf x} = A \\
&amp;amp;2.\ \frac{\partial \mathbf x^T \mathbf a}{\partial \mathbf x} = \frac{\partial \mathbf a^T \mathbf x}{\partial \mathbf x} = \mathbf a \\
&amp;amp;3.\ \frac{\partial \mathbf ||x||^2}{\partial \mathbf x} = \frac{\partial \mathbf x^T \mathbf x}{\partial \mathbf x} = 2 \mathbf x \\
&amp;amp;4.\ \frac{\partial \mathbf x^T A \mathbf x}{\partial \mathbf x} = (A + A^T)\mathbf x
\end{aligned}
$$&lt;/p&gt;
&lt;h2 id=&#34;推导过程&#34;&gt;推导过程&lt;/h2&gt;
&lt;p&gt;最基础的出发点是，对矩阵或者向量求导，就是对其中的每一个元素关于每一个自变量中的元素进行求导，最后写成一个矩阵的形式，所以落脚点在单个元素上。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;公式1&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;容易看出\(A\mathbf x\)是一个向量，对其中一个元素进行求导，注意下标，求和符号内只有一项包含\(x_k\)，其他的可以直接舍弃：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\partial}{\partial x_k} (A\mathbf x)_{i} &amp;amp;= \frac{\partial}{\partial x_k} \sum_{j=1}^{n} A_{ij}x_j \\
&amp;amp;= A_{ik}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;所以可得在求导结果矩阵中每一行每一列的元素都等于\(A\)在这个位置上的元素，q.e.d.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;公式2&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\(\mathbf x^T \mathbf a\)和\(\mathbf a^T \mathbf x\)都是标量。证明过程和1相似：&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial}{\partial x_i} \mathbf x^T \mathbf a = \frac{\partial}{\partial x_i} \sum_{j=1}^{n} x_ja_j = a_i \\
\frac{\partial}{\partial x_i} \mathbf a^T \mathbf x = \frac{\partial}{\partial x_i} \sum_{j=1}^{n} a_jx_j = a_i
$$&lt;/p&gt;
&lt;p&gt;这个公式是最基本的，但极其重要和常用。&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;公式3&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;证明过程也十分简单：&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial \mathbf ||x||^2}{\partial x_i} = \frac{\partial}{\partial x_i} \sum_j x_j^2 = \frac{\partial}{\partial x_i} x_i^2 = 2x_i
$$&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;公式4&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;可以看出结果是一个标量。应用前面所说的&lt;strong&gt;变量多次出现的求导方法&lt;/strong&gt;，先把后面的\(A \mathbf x\)看成整体，对前一个\(\mathbf x\)求导，运用公式2，可得：&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial \mathbf x^T (A \mathbf x&#39;)}{\partial \mathbf x} = A \mathbf x&#39;
$$&lt;/p&gt;
&lt;p&gt;类似的对后面的\(\mathbf x\)求导，可得：&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial (\mathbf x&#39;^T A) \mathbf x}{\partial \mathbf x} = (\mathbf x&#39;^T A)^T = A^T\mathbf x&#39;
$$&lt;/p&gt;
&lt;p&gt;去掉一撇，加起来，可得：&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial \mathbf x^T A \mathbf x}{\partial \mathbf x} = A \mathbf x + A^T\mathbf x = (A + A^T)\mathbf x
$$&lt;/p&gt;
&lt;p&gt;q.e.d.&lt;/p&gt;
&lt;h2 id=&#34;应用举例&#34;&gt;应用举例&lt;/h2&gt;
&lt;p&gt;LDA算法的推导过程中需要解\(\mathbf w\)：&lt;/p&gt;
&lt;p&gt;$$
\underset{\mathbf w}{\operatorname{argmax}} \frac{\mathbf w^TS_B\mathbf w}{\mathbf w^TS_W\mathbf w}
$$&lt;/p&gt;
&lt;p&gt;其中\(S_B\)和\(S_W\)都是对称矩阵。容易发现分式上下都是标量。要求最大值，对\(\mathbf w\)求导，置零即可。先运用商的导数公式：&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial}{\partial \mathbf w}\frac{\mathbf w^TS_B\mathbf w}{\mathbf w^TS_W\mathbf w} = \frac{\mathbf w^TS_W\mathbf w(\frac{\partial}{\partial \mathbf w}\mathbf w^TS_B\mathbf w) - \mathbf w^TS_B\mathbf w(\frac{\partial}{\partial \mathbf w}\mathbf w^TS_W\mathbf w)}{(\mathbf w^TS_W\mathbf w)^2}
$$&lt;/p&gt;
&lt;p&gt;求分子上的导数，运用上面的公式4：&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial}{\partial \mathbf w}\mathbf w^TS_B\mathbf w = (S_B + S_B^T)\mathbf w
$$&lt;/p&gt;
&lt;p&gt;由于\(S_B\)是对称的，\(S_B^T = S_B\),&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial}{\partial \mathbf w}\mathbf w^TS_B\mathbf w = 2S_B\mathbf w
$$&lt;/p&gt;
&lt;p&gt;对另一个倒数类似，置零可得：&lt;/p&gt;
&lt;p&gt;$$
\frac{(\mathbf w^TS_W\mathbf w)2S_B\mathbf w - (\mathbf w^TS_B\mathbf w)2S_W\mathbf w}{(\mathbf w^TS_W\mathbf w)^2} \overset{!}{=} 0
$$&lt;/p&gt;
&lt;p&gt;即：&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
(\mathbf w^TS_B\mathbf w)2S_W\mathbf w &amp;amp;= (\mathbf w^TS_W\mathbf w)2S_B\mathbf w \\
S_W\mathbf w &amp;amp;= \frac{\mathbf w^TS_W\mathbf w}{\mathbf w^TS_B\mathbf w}S_B\mathbf w \\
S_W\mathbf w &amp;amp;= S_B\mathbf w \lambda \\
\mathbf w &amp;amp;= S_W^{-1} S_B\mathbf w \lambda
\end{aligned}
$$&lt;/p&gt;
&lt;h2 id=&#34;另请参阅&#34;&gt;另请参阅&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://atmos.washington.edu/~dennis/MatrixCalculus.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Matrix Cookbook&lt;/a&gt;，很好的一本公式集，适合查阅。&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://gwthomas.github.io/docs/math4ml.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Mathematics for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://blog.xpgreat.com/file/matrixvectorderivativesformachinelearning.pdf&#34; &gt;机器学习中的矩阵、向量求导&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        <item>
        <title>使用MathJax时的注意事项</title>
        <link>https://blog.xpgreat.com/p/mathjax_tips/</link>
        <pubDate>Thu, 17 Jan 2019 17:28:35 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/mathjax_tips/</guid>
        <description>&lt;p&gt;记录一些在使用MathJax写公式时踩过的坑，持续更新。本来放在另一篇里的，内容越来越多，单独成篇。&lt;/p&gt;
&lt;h2 id=&#34;用-无法输入行内公式&#34;&gt;用$ $无法输入行内公式&lt;/h2&gt;
&lt;p&gt;MathJax默认禁用了这种方式，你可以使用&lt;code&gt;\\( 你的公式 \\)&lt;/code&gt;的方法，也可以参考它的&lt;a class=&#34;link&#34; href=&#34;https://github.com/mathjax/MathJax/blob/master/config/default.js&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;default.js&lt;/a&gt;自行配置（在295行）。行间公式默认可以用&lt;code&gt;\\[ 你的公式 \\]&lt;/code&gt;或者传统的TeX写法&lt;code&gt;$$ 你的公式 $$&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;输入下标出错&#34;&gt;输入下标出错&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;x_{abc}&lt;/code&gt;这样使用的时候会莫名其妙的报错，试了一下把&lt;code&gt;_&lt;/code&gt;转义：&lt;code&gt;x\_{abc}&lt;/code&gt;，就能用了。&lt;/p&gt;
&lt;h2 id=&#34;公式不能换行&#34;&gt;公式不能换行&lt;/h2&gt;
&lt;p&gt;因为md解析的时候把&lt;code&gt;\\&lt;/code&gt;解析成了一个&lt;code&gt;\&lt;/code&gt;，解决方法是用四根&lt;code&gt;\\\\ &lt;/code&gt;换行（最后要加一个半角空格）。（&lt;del&gt;也是醉了&lt;/del&gt;）&lt;/p&gt;
&lt;h2 id=&#34;公式对齐&#34;&gt;公式对齐&lt;/h2&gt;
&lt;p&gt;直接上代码：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;\begin{aligned}
&amp;amp;\nabla_{\mathbf x}A\mathbf x = A \\\\ 
&amp;amp;\frac{\partial \mathbf x^T \mathbf a}{\partial \mathbf x} = \frac{\partial \mathbf a^T \mathbf x}{\partial \mathbf x} = \mathbf a \\\\ 
\end{aligned}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;效果：
$$
\begin{aligned}
&amp;amp;\nabla_{\mathbf x}A\mathbf x = A \\
&amp;amp;\frac{\partial \mathbf x^T \mathbf a}{\partial \mathbf x} = \frac{\partial \mathbf a^T \mathbf x}{\partial \mathbf x} = \mathbf a \\
\end{aligned}
$$&lt;/p&gt;
&lt;h2 id=&#34;自定义运算名比如argmax&#34;&gt;自定义运算名，比如argmax&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;\\underset{\mathbf w}{\operatorname{argmax}} \frac{\mathbf w^TS_B\mathbf w}{\mathbf w^TS_B\mathbf w} 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;效果：&lt;/p&gt;
&lt;p&gt;$$
\underset{\mathbf w}{\operatorname{argmax}} \frac{\mathbf w^TS_B\mathbf w}{\mathbf w^TS_B\mathbf w}
$$&lt;/p&gt;
</description>
        </item>
        <item>
        <title>认知算法小结</title>
        <link>https://blog.xpgreat.com/p/ca_short_summary/</link>
        <pubDate>Fri, 28 Dec 2018 20:57:51 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/ca_short_summary/</guid>
        <description>&lt;p&gt;在认知算法课里学习了一些算法，比如NCC，感知算法，LDA等等，小结一下。&lt;/p&gt;
&lt;h2 id=&#34;nearest-centroid-classifier&#34;&gt;Nearest Centroid Classifier&lt;/h2&gt;
&lt;p&gt;NCC. 最简单的一个归类算法。输入多个\((\mathbf x, y)\)，其中\(\mathbf x\)表示特征，\(y\)表示所属类别，比如\( y \in \{ -1,1 \} \)。将每一类的\(\mathbf x\)取平均，得到一个向量，称为原型（Prototype）。预测一个新的\(\mathbf x\)是属于哪一类的时候，计算并比较它与每一类的原型的相似度（距离），归为最相似的那一类中。比如\(y = -1： (0, 1); y = 1: (1, 0)\)，输入\(\mathbf x = (0, 2)\)，计算它与\((0, 1)\)和\((1, 0)\)的距离并比较，容易得出离前者更近，所以归入\(y = -1\)组。&lt;/p&gt;
&lt;p&gt;基于这一思想，可以推导出一个函数\(f(\mathbf x)\)，当\(\mathbf x\)属于一个组的时候函数值为正，属于另一个组的时候函数值为负。略去推导过程，给出结果：&lt;/p&gt;
&lt;p&gt;$$
f(\mathbf x) = {\mathbf w}^T \mathbf x - \beta
$$&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;$$
{\mathbf w} = \mathbf w_{-1} - \mathbf w_{+1}
\\ \beta = \frac{1}{2} \left( \mathbf w_{-1}^T \mathbf w_{-1} - \mathbf w_{+1}^T \mathbf w_{+1} \right)
$$&lt;/p&gt;
&lt;h2 id=&#34;感知算法&#34;&gt;感知算法&lt;/h2&gt;
&lt;p&gt;和NCC类似，输入一组数据和他们的类别，输出一个\(\mathbf w\)。&lt;/p&gt;
&lt;p&gt;引入一个概念&lt;strong&gt;感知错误&lt;/strong&gt;（Perceptron Error），衡量一个\(\mathbf w\)的好坏。定义为：&lt;/p&gt;
&lt;p&gt;$$
E_p(\mathbf w) = - \sum_{m \in M} \mathbf w^T \mathbf x_m y_m
$$&lt;/p&gt;
&lt;p&gt;其中\(M\)是所有分类错误的数据的index。为了获得一个最优的\(\mathbf w\)，将上面的\(E_p(\mathbf w)\)最小化即可。在这里使用了&lt;a class=&#34;link&#34; href=&#34;https://zh.wikipedia.org/zh-hans/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;随机梯度下降法&lt;/a&gt;（Stochastic Gradient Descent）。&lt;/p&gt;
&lt;p&gt;$$
\mathbf w^{new} = \mathbf w^{old} - \eta \nabla E_p(\mathbf w^{old}) = \mathbf w^{old} + \eta \mathbf x_m y_m
$$&lt;/p&gt;
&lt;p&gt;其中最初的\(\mathbf w\)可以随机取得，\(\eta\)是学习率（learning rate），反应变化的速率，由于梯度下降法的特点，\(\eta\)不能太大也不能太小，太大可能得不出结果，太小的话会收敛太慢。&lt;/p&gt;
&lt;p&gt;使用这个方法时，可以设定一个跳出条件，比如相邻两次的变化量小于某个值，让模型重复随机取值、计算即可。&lt;/p&gt;
&lt;h2 id=&#34;linear-discriminant-analysis&#34;&gt;Linear Discriminant Analysis&lt;/h2&gt;
&lt;p&gt;LDA，线性判别分析，是后续很多算法的基础，十分重要。和前面两个算法的目的一样，为了找出\(\mathbf w\)和\(\beta\)。&lt;/p&gt;
&lt;p&gt;引入一个概念，费希尔标准（Fisher Criterion），用来衡量两个类别的分离程度：&lt;/p&gt;
&lt;p&gt;$$
F = \frac{between \ class \ variance}{within \ class \ variance} = \frac{({\mathbf w_{+1} - \mathbf w_{-1}})^2}{\sigma_{+1}^2 + \sigma_{-1}^2}
$$&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;$$
\mathbf w_i = \frac{1}{N_i}\sum_{j=1}^{N_i}\mathbf x_{ji},
\\ \sigma_i^2 = \frac{1}{N_i} \sum_{j=1}^{N_i}(x_{ji} - \mathbf w_i)^2
$$&lt;/p&gt;
&lt;p&gt;为了找到最优的\((\mathbf w , \beta)\)组合，只需要最大化组间variance，最小化组内variance即可，即最大化\(F\)。求导、推导过程略过，计算方法如下：&lt;/p&gt;
&lt;p&gt;首先用上面的方法计算\(\mathbf w_i\)。&lt;/p&gt;
&lt;p&gt;计算组内方差矩阵：&lt;/p&gt;
&lt;p&gt;$$
S_W = \frac{1}{N_{-}} \sum_{i \in y_{-}} (\mathbf x_i - \mathbf w_{-}) (\mathbf x_i - \mathbf w_{-})^T + \frac{1}{N_{+}} \sum_{i \in y_{+}} (\mathbf x_i - \mathbf w_{+}) (\mathbf x_i - \mathbf w_{+})^T
$$&lt;/p&gt;
&lt;p&gt;算出结果：&lt;/p&gt;
&lt;p&gt;$$
\mathbf w = S_W^{-1}(\mathbf w_{+} - \mathbf w_{-}) \\
\beta = \frac{1}{2} \mathbf w^T (\mathbf w_{+} + \mathbf w_{-}) + \log (\frac{N_{-}}{N_{+}})
$$&lt;/p&gt;
&lt;h2 id=&#34;ordinary-least-squares&#34;&gt;Ordinary Least Squares&lt;/h2&gt;
&lt;p&gt;OLS，普通最小二乘法，用于线性回归。输入一组数据\((\mathbf x, y)\)，得到线性函数\(f(\mathbf x) = \mathbf \omega^T \mathbf x\)，把其中的\(\mathbf x\)替换为合适的核函数（Kernel Function）\(\mathbf \phi (\mathbf x)\)可以用于非线性回归。&lt;/p&gt;
&lt;p&gt;引入概念最小二乘误差（Least Square Error），用于衡量线性函数与样本的契合度：&lt;/p&gt;
&lt;p&gt;$$
E(\mathbf \omega) = \sum_{t=1}^T (y_t - \mathbf \omega^T \mathbf x_t)^2
$$&lt;/p&gt;
&lt;p&gt;求导，置零，推导过程略过，得到结果为：&lt;/p&gt;
&lt;p&gt;$$
\mathbf \omega = (\mathbf X \mathbf X^T)^{-1}\mathbf X y^T
$$&lt;/p&gt;
&lt;p&gt;可以把\(y\)扩展到多维，这样得到的是Linear Mapping。&lt;/p&gt;
&lt;p&gt;很多时候时候线性函数\(f(\mathbf x) = \mathbf \omega^T \mathbf x\)不满足需求，需要添加一个偏移量，也就是把函数变成\(f(\mathbf x) = \mathbf \omega^T \mathbf x + bias\)，可以通过在样本\(\mathbf X\)的最上方加入一行\(1\)来实现。&lt;/p&gt;
&lt;h2 id=&#34;ridge-regression&#34;&gt;Ridge Regression&lt;/h2&gt;
&lt;p&gt;有时候仅仅使用OLS会导致过拟合(Overfitting)的问题，即模型使用过多的参数过度适应样本而不能反应真实情况，所以控制\(\mathbf \omega\)的复杂度是有必要的。&lt;/p&gt;
&lt;p&gt;扩展一下最小二乘误差：&lt;/p&gt;
&lt;p&gt;$$
E_{RR} (\mathbf \omega) = (y - \mathbf \omega^T \mathbf X)^2 + \lambda \mathbf \omega^2
$$&lt;/p&gt;
&lt;p&gt;其中\(\lambda\)被称为Ridge，控制它的大小以控制\(\mathbf \omega\)的复杂度，不能太大也不能太小，太大会导致欠拟合，太小会导致过拟合。&lt;/p&gt;
&lt;p&gt;推导过程略过，得到的结果为：&lt;/p&gt;
&lt;p&gt;$$
\mathbf \omega = (\mathbf X \mathbf X^T + \lambda \mathbf I)^{-1}\mathbf X y^T
$$&lt;/p&gt;
&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;
&lt;p&gt;以上差不多是这门课里学到的全部算法，主要用于线性归类和回归，可以使用适当的&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Kernel_method&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;核函数&lt;/a&gt;（Kernel Function）扩展到非线性的范围。另外还可以使用&lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Cross Validation&lt;/a&gt;来判断一个模型的好坏。关于这两个话题之后再做总结。&lt;/p&gt;</description>
        </item>
        <item>
        <title>利率期权</title>
        <link>https://blog.xpgreat.com/p/intrest_option/</link>
        <pubDate>Tue, 25 Dec 2018 11:02:13 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/intrest_option/</guid>
        <description>&lt;p&gt;利率期权(interest-rate option)是一项关于利率变化的权利。买方支付一定金额的期权费后，就可以获得这项权利：在到期日按预先约定的利率，按一定的期限借入或贷出一定金额的货币。这样当市场利率向不利方向变化时，买方可固定其利率水平；当市场利率向有利方向变化时，买方可获得利率变化的好处。利率期权的卖方向买方收取期权费，同时承担相应的责任。&lt;/p&gt;
&lt;h2 id=&#34;put--call&#34;&gt;put &amp;amp; call&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;put&lt;/strong&gt;：约定在到期日买方有按预定利率贷出一定金额的货币的权利，如果到期日利率下跌，买方可以获得到期日利率和预定利率之间的差值作为获利；如果到期日利率上涨，高于预定利率，这时行权则不值得，不会行权。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;call&lt;/strong&gt;：约定在到期日买方有按预定利率借入一定金额的货币的权利，如果到期日利率上涨，买方可以获得到期日利率和预定利率之间的差值作为获利；如果到期日利率下跌，低于预定利率，这时行权则不值得，不会行权。&lt;/p&gt;
&lt;h2 id=&#34;利率封顶interest-rate-cap&#34;&gt;利率封顶（Interest Rate Cap）&lt;/h2&gt;
&lt;p&gt;又称利率上限。客户同银行达成一项协议，指定某一种市场参考利率，同时确定一个利率上限水平。在此基础之上，利率封顶的卖出方向买入方承诺：在规定的期限内，如果市场参考利率（一般参考LIBOR(伦敦同业拆借利率),美国的联邦基金利率(Federal Funds Rate)）高于协定的利率上限水平，卖方向买方支付市场利率高于利率上限的差额部分；如果市场参考利率低于或等于协定的利率上限水平，则卖方无任何支付义务。买方由于获得了上述权利，必须向卖方支付一定数额的期权费。&lt;/p&gt;
&lt;p&gt;例子：发行一份浮动利率票据（Floating Rate Note, FRN），购入一份利率call。到期时市场参考利率如果低于预定利率，call不执行，利率是浮动的；如果市场参考利率高于预定利率，执行call，call的获利与市场参考利率上升的部分对冲，以达到将利率封顶的目的。&lt;/p&gt;
&lt;p&gt;可以用于控制利率上升的风险，比如某公司现有金额为美元1000万，期限6个月，以LIBOR计息的浮动债务，公司既希望在市场利率降低时能享有低利率的好处，又想避免市场利率上涨时利息成本增加的风险。这时，公司支付一定的期权费，向银行买入6个月，协定利率为5%的利率封顶。6个月后，如果LIBOR上升为6%（利率大于等于5%），公司选择行使该期权，即银行向公司支付市场利率和协议利率的差价（6%-5%=1%），公司有效地固定了其债务利息；如果LIBOR利率低于5%，公司可选择不实施该权利，而以较低的市场利率支付债务利息。这样，对于买方，有效地控制了利率上升的风险，而卖方则收取一笔期权费。&lt;/p&gt;
&lt;p&gt;利率封顶的期权费与利率上限水平和协议期限有关。相对而言，利率上限水平越高，期权费率越低；期限越短，期权费率也越低。&lt;/p&gt;
&lt;h2 id=&#34;利率封底-interest-rate-floor&#34;&gt;利率封底 （Interest Rate Floor）&lt;/h2&gt;
&lt;p&gt;又称利率下限，与利率封顶相反，利率封底是客户与银行达成一项协议，指定某一种市场参考利率，同时确定一个利率下限水平。在此基础之上，利率封底的卖出方向买入方承诺：在规定的期限内，如果市场参考利率低于协定的利率下限水平，卖方向买方支付市场利率低于利率下限的差额部分；如果市场参考利率高于或等于协定的利率下限水平，则卖方无任何支付义务。买方由于获得了上述权利，必须向卖方支付一定数额的期权手续费。&lt;/p&gt;
&lt;p&gt;例子：购入一份浮动利率票据，购入一份利率put。到期时市场参考利率如果高于预定利率，put不执行，利率是浮动的；如果市场参考利率低于预定利率，执行put，put的获利与市场参考利率低于预期的部分对冲，以达到将利率封底的目的。&lt;/p&gt;
&lt;h2 id=&#34;利率两头封interest-rate-collar&#34;&gt;利率两头封（Interest Rate Collar）&lt;/h2&gt;
&lt;p&gt;又称利率上下限，是将利率封顶和利率封底两种金融工具合成的产品。具体地说，购买一项利率两头封，就是在买进一项Cap的同时，卖出一项Floor，以收入的期权费来部分抵销需要支出的期权费，达到既规避利率风险又降低费用成本的目的（自筹资金，self-financing）。卖出一项利率两头封，则是指在卖出一项Cap的同时，买入一项Floor。若Cap和Floor的预定利率相同，可以完全消除市场利率变化的影响，然而也没有获利空间，所以没有意义。Cap和Floor的预定利率不同时，可以在一定范围内从市场利率的浮动获利。当借款人预计市场利率会上涨时，可以考虑购买一项利率两头封。&lt;/p&gt;</description>
        </item>
        <item>
        <title>期权的风险指标</title>
        <link>https://blog.xpgreat.com/p/finance_greeks/</link>
        <pubDate>Mon, 24 Dec 2018 17:06:00 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/finance_greeks/</guid>
        <description>&lt;p&gt;期权的风险指标通常用希腊字母来表示，包括：Delta值(\(\Delta\))、Gamma值(\(\Gamma\))、Omega值(\(\Omega\))、Theta值(\(\theta\))、Vega值、Rho(\(\rho\))值等。&lt;/p&gt;
&lt;p&gt;结合&lt;a class=&#34;link&#34; href=&#34;https://blog.xpgreat.com/post/bs_model/&#34; &gt;布莱克-舒尔斯期权定价模型&lt;/a&gt;的公式来看：&lt;/p&gt;
&lt;p&gt;$$
c=SN(d_1)-Xe^{-r(T-t)}N(d_2)
$$&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;p&gt;$$
d_1 = \frac{\ln \frac{S}{X} + (r+\frac{\sigma^2}{2})(T-t)}{\sigma \sqrt{T-t}}
$$
$$
d_2 = \frac{\ln \frac{S}{X} + (r-\frac{\sigma^2}{2})(T-t)}{\sigma \sqrt{T-t}}
$$&lt;/p&gt;
&lt;p&gt;\(N(x)\)是标准正态分布变量的累计概率分布函数。&lt;/p&gt;
&lt;h2 id=&#34;deltadelta&#34;&gt;Delta(\(\Delta\))&lt;/h2&gt;
&lt;p&gt;又称&lt;strong&gt;对冲值&lt;/strong&gt;，是衡量标的资产价格变动时，期权价格的变化幅度。&lt;/p&gt;
&lt;p&gt;$$
\Delta = \frac{dc}{dS}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特性&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;买权的Delta一定要是正值，因为期货价格上涨（下跌），期权价格随之上涨（下跌），二者始终保持同向变化。&lt;/li&gt;
&lt;li&gt;卖权的Delta一定要是负值，因为看跌期权价格的变化与期货价格相反。&lt;/li&gt;
&lt;li&gt;Delta的范围介乎-1到1之间。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;运用&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;衡量头寸风险：如看涨期权的Delta为0.4，意味着期货价格每变动一元，期权的价格则变动0.4元。Delta具有可加性，如果投资者持有以下投资组合：&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;持仓头寸&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Delta&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;数量（张）&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;买入小麦期货&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;买入看涨期权&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0.47&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;买入看跌期权&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;-0.53&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;该交易者的总体持仓的Delta值为\(1+2&lt;em&gt;0.47-3&lt;/em&gt;0.53=0.35\)，也就是说这是一个偏多的头寸，相当于0.35手期货多头。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Delta中性套期保值&lt;/strong&gt;（Delta Hedging）：如果投资者希望对冲期权或期货头寸的风险，Delta就是套期保值比率。只要使头寸的整体 Delta值保持为0，就建立了一个中性的套期策略。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gammagamma&#34;&gt;Gamma(\(\Gamma\))&lt;/h2&gt;
&lt;p&gt;反映期货价格对Delta值的影响程度。如某一期权的Delta为0.6，Gamma值为0.05，则表示期货价格上升1元，所引起Delta增加量为0.05。Delta将从0.6增加到0.65。&lt;/p&gt;
&lt;p&gt;$$
\Gamma = \frac{d\Delta}{dS}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特性&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gamma值均为正值，因为若期货价格上涨，看涨期权的Delta值由0向1移动，看跌期权的Delta值从-1向0移动，即期权的Delta值从小到大移动，Gamma值为正。&lt;/li&gt;
&lt;li&gt;Gamma值越大，Delta值变化越快。&lt;/li&gt;
&lt;li&gt;平值期权的Gamma值最大。&lt;/li&gt;
&lt;li&gt;随着到期日的临近，平值期权Gamma值会急剧增加。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;应用&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;进行Delta中性套期保值，Gamma绝对值越大的头寸，风险程度也越高，因为进行中性对冲需要调整的频率高；相反，Gamma绝对值越小的头寸，风险程度越低。对期权的买方来说，风险有限，所以Gamma值越高，Delta越不稳定，对投资者越有利。&lt;/p&gt;
&lt;h2 id=&#34;omegaomega&#34;&gt;Omega(\(\Omega\))&lt;/h2&gt;
&lt;p&gt;又称期权价格的无风险利率粘性，表示期权价格变动量和期货价格变动量的比值。&lt;/p&gt;
&lt;p&gt;$$
\Omega = \frac{\frac{dc}{c}}{\frac{dS}{S}}
$$&lt;/p&gt;
&lt;h2 id=&#34;rhorho&#34;&gt;Rho(\(\rho\))&lt;/h2&gt;
&lt;p&gt;表示期权价格对无风险利率变化的敏感程度。&lt;/p&gt;
&lt;p&gt;$$
\rho = \frac{dc}{dr}
$$&lt;/p&gt;
&lt;h2 id=&#34;thetatheta&#34;&gt;Theta(\(\theta\))&lt;/h2&gt;
&lt;p&gt;表示时间变化对期权价格的影响。衡量期权的时间价值的折损的速度。&lt;/p&gt;
&lt;p&gt;$$
\theta = - \frac{dc}{dT}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特性&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;Theta为负值，意味着期权的价格随着时间流逝会越来越低，折损的速度则可由Theta来衡量。例如还有60天到期的平价期权的Theta为-0.0002126，这意味着每天期权价值失去0.0002126美元。&lt;/p&gt;
&lt;h2 id=&#34;vega&#34;&gt;Vega&lt;/h2&gt;
&lt;p&gt;表示期货价格的波动率变化对期权价格的影响。&lt;/p&gt;
&lt;p&gt;$$
Vega = \frac{dc}{d\sigma}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特性&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;Vega为正值，意味着期权的价格随着波动率的增加而增加。当期权处于平价状态时，Vega值最大；当期权处于较深的价内或者价外时，Vega值接近于零。（相关：&lt;a class=&#34;link&#34; href=&#34;https://wiki.mbalib.com/wiki/%E4%BB%B7%E5%86%85%E6%9D%83%E8%AF%81&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;价内权证&lt;/a&gt;）&lt;/p&gt;</description>
        </item>
        <item>
        <title>Linux下压缩命令</title>
        <link>https://blog.xpgreat.com/p/linux_compress/</link>
        <pubDate>Sun, 16 Dec 2018 13:30:23 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/linux_compress/</guid>
        <description>&lt;p&gt;Linux下压缩解压缩命令的归纳。&lt;/p&gt;
&lt;h4 id=&#34;tar&#34;&gt;.tar&lt;/h4&gt;
&lt;p&gt;解包：&lt;code&gt;tar xvf FileName.tar&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;打包：&lt;code&gt;tar cvf FileName.tar DirName&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;（注：tar是打包，不是压缩！）&lt;/p&gt;
&lt;h4 id=&#34;gz&#34;&gt;.gz&lt;/h4&gt;
&lt;p&gt;解压1：&lt;code&gt;gunzip FileName.gz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;解压2：&lt;code&gt;gzip -d FileName.gz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：&lt;code&gt;gzip FileName&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;targz-和-tgz&#34;&gt;.tar.gz 和 .tgz&lt;/h4&gt;
&lt;p&gt;解压：&lt;code&gt;tar zxvf FileName.tar.gz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：&lt;code&gt;tar zcvf FileName.tar.gz DirName&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;bz2&#34;&gt;.bz2&lt;/h4&gt;
&lt;p&gt;解压1：&lt;code&gt;bzip2 -d FileName.bz2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;解压2：&lt;code&gt;bunzip2 FileName.bz2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：&lt;code&gt;bzip2 -z FileName&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;tarbz2&#34;&gt;.tar.bz2&lt;/h4&gt;
&lt;p&gt;解压：&lt;code&gt;tar jxvf FileName.tar.bz2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：&lt;code&gt;tar jcvf FileName.tar.bz2 DirName&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;bz&#34;&gt;.bz&lt;/h4&gt;
&lt;p&gt;解压1：&lt;code&gt;bzip2 -d FileName.bz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;解压2：&lt;code&gt;bunzip2 FileName.bz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：未知&lt;/p&gt;
&lt;h4 id=&#34;tarbz&#34;&gt;.tar.bz&lt;/h4&gt;
&lt;p&gt;解压：&lt;code&gt;tar jxvf FileName.tar.bz&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：未知&lt;/p&gt;
&lt;h4 id=&#34;z&#34;&gt;.Z&lt;/h4&gt;
&lt;p&gt;解压：&lt;code&gt;uncompress FileName.Z&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：&lt;code&gt;compress FileName&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;tarz&#34;&gt;.tar.Z&lt;/h4&gt;
&lt;p&gt;解压：&lt;code&gt;tar Zxvf FileName.tar.Z&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：&lt;code&gt;tar Zcvf FileName.tar.Z DirName&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;zip&#34;&gt;.zip&lt;/h4&gt;
&lt;p&gt;解压：&lt;code&gt;unzip FileName.zip&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：&lt;code&gt;zip FileName.zip DirName&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;rar&#34;&gt;.rar&lt;/h4&gt;
&lt;p&gt;解压：&lt;code&gt;rar x FileName.rar&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：&lt;code&gt;rar a FileName.rar DirName&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;lha&#34;&gt;.lha&lt;/h4&gt;
&lt;p&gt;解压：&lt;code&gt;lha -e FileName.lha&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：&lt;code&gt;lha -a FileName.lha FileName&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;rpm&#34;&gt;.rpm&lt;/h4&gt;
&lt;p&gt;解包：&lt;code&gt;rpm2cpio FileName.rpm | cpio -div&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;deb&#34;&gt;.deb&lt;/h4&gt;
&lt;p&gt;解包：&lt;code&gt;ar p FileName.deb data.tar.gz | tar zxf -&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;用sex&#34;&gt;用sEx&lt;/h4&gt;
&lt;p&gt;解压：&lt;code&gt;sEx x FileName.*&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;压缩：&lt;code&gt;sEx a FileName.* FileName&lt;/code&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>布莱克-舒尔斯期权定价模型</title>
        <link>https://blog.xpgreat.com/p/bs_model/</link>
        <pubDate>Tue, 11 Dec 2018 22:05:54 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/bs_model/</guid>
        <description>&lt;p&gt;在Risikomanagement课上讲到了布莱克-舒尔斯期权定价模型（Black-Scholes Option Pricing Model），但只是给出了公式，没有推导。在网上查找之后终于大致弄懂，把推导过程尽可能的详细的写一写。&lt;/p&gt;
&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;布莱克-舒尔斯模型（英语：Black-Scholes Model），简称BS模型，又称布莱克-舒尔斯-墨顿模型（Black–Scholes–Merton model），是一种为期权或权证等金融衍生工具定价的数学模型，由美国经济学家迈伦·舒尔斯与费雪·布莱克首先提出，并由罗伯特·C·墨顿修改模型于有派发股利时亦可使用而更完善。由此模型可以推导出布莱克-舒尔斯公式，并由此公式估算出欧式期权的理论价格。此公式问世后带来了期权市场的繁荣。该公式被广泛使用，虽然在很多情况下被使用者进行一定的改动和修正。很多经验测试表明这个公式足够贴近市场价格，然而也有会出现差异的时候，如著名的“波动率的微笑”。&lt;/p&gt;
&lt;p&gt;该模型就是以迈伦·舒尔斯和费雪·布莱克命名的。1997年迈伦·舒尔斯和罗伯特·墨顿凭借该模型获得诺贝尔经济学奖。然而它假设价格的变动，会符合高斯分布（即俗称的钟形曲线），但在财务市场上经常出现符合统计学肥尾现象的事件，这影响此公式的有效性。（摘自&lt;a class=&#34;link&#34; href=&#34;https://zh.wikipedia.org/wiki/%E5%B8%83%E8%8E%B1%E5%85%8B-%E8%88%92%E5%B0%94%E5%85%B9%E6%A8%A1%E5%9E%8B&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;维基百科&lt;/a&gt;）&lt;/p&gt;
&lt;h2 id=&#34;效率市场假说&#34;&gt;效率市场假说&lt;/h2&gt;
&lt;p&gt;1965年法玛（Fama）提出，认为投资者都试图利用可获得的信息获得更多报酬；证券价格对新的市场信息的反应应该是迅速且准确的，证券价格可以反应全部信息。证券价格在市场竞争下从一个均衡水平过渡到另一个均衡水平，而与新信息对应的价格变动是相互独立的。&lt;/p&gt;
&lt;p&gt;效率市场假说可以分为三类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;弱式&lt;/strong&gt;：目前股票价格已充分反映了过去股票价格所提供的各项情报。所以，投资人无法再运用各种方法对&lt;strong&gt;过去股票价格&lt;/strong&gt;进行分析，再利用分析结果来预测未来股票价格，基于随机游走假说，未来消息是随机而来的。意即投资者无法再利用&lt;strong&gt;过去资讯&lt;/strong&gt;来获得高额报酬。所以，弱势效率越高，若以过去价量为基础的技术分析来进行预测效果将会十分不准确。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;半强式&lt;/strong&gt;：目前股票价格已充分反应了所有公开资讯，所以，投资者无法利用&lt;strong&gt;情报&lt;/strong&gt;分析结果来进行股票价格预测而获取高额报酬。因此，半强式效率越高，依赖&lt;strong&gt;公开的&lt;/strong&gt;财务报表、经济情况及政治情势来进行基本面分析，然后再预测股票价格是徒劳无功。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强式&lt;/strong&gt;：目前股票价格充分反应了所有&lt;strong&gt;已公开和未公开&lt;/strong&gt;之所有情报。虽然情报未公开，但投资者能利用各种管道来获得资讯，所以，所谓未公开的消息，实际上是已公开的资讯且已反应于股票价格上。此种情形下，投资者也无法因拥有某些股票内幕消息而获取高额报酬。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;区分点在于投资者可利用的信息对预测价格有没有用。此假说发布后，许多学者进行了实证分析，发现发达国家的证券市场大致符合弱式效率市场。&lt;/p&gt;
&lt;h2 id=&#34;随机过程&#34;&gt;随机过程&lt;/h2&gt;
&lt;p&gt;随机过程是指某变量的值以某种不确定的方式随时间变化的过程。根据变量是否连续和时间是否连续分为4种：连续变量在连续时间的随机过程、连续变量在离散时间的随机过程、离散变量在连续时间的随机过程，和离散变量在离散时间的随机过程。严格来说，证券价格的变化应该属于离散变量在离散时间的随机过程，然而在时间间隔很小的时候，我们可以近似的把它看作连续变量在连续时间的随机过程。&lt;/p&gt;
&lt;h3 id=&#34;马尔可夫过程&#34;&gt;马尔可夫过程&lt;/h3&gt;
&lt;p&gt;随机过程，在这个过程中只有变量的当前值才与其将来值有关系，过去的值与未来的预测无关。&lt;/p&gt;
&lt;h3 id=&#34;布朗运动&#34;&gt;布朗运动&lt;/h3&gt;
&lt;p&gt;又称维纳过程。由英国的植物学家罗伯特·布朗发现并命名，维纳（Wiener）给出的其随机过程的定义。&lt;/p&gt;
&lt;h4 id=&#34;标准布朗运动&#34;&gt;标准布朗运动&lt;/h4&gt;
&lt;p&gt;设\(\Delta t\)代表一个小的时间间隔长度，\(\Delta z\)代表变量\(z\)在\(\Delta t\)时间内的变化，遵循标准布朗运动的\(\Delta z\)具有两个特征：&lt;/p&gt;
&lt;p&gt;特征1：\(\Delta z\)和\(\Delta t\)的关系满足：&lt;/p&gt;
&lt;p&gt;$$
\Delta z = \varepsilon \sqrt{\Delta t}
$$&lt;/p&gt;
&lt;p&gt;其中\(\varepsilon \sim N(0,1)\)。&lt;/p&gt;
&lt;p&gt;特征2：对于任何两个不同时间，\(\Delta z\)和\(\Delta t\)相互独立。因此标准布朗运动是一个特殊的马尔可夫过程。&lt;/p&gt;
&lt;p&gt;考虑在一段时间\(T\)内\(z\)的变化：&lt;/p&gt;
&lt;p&gt;$$
z(T) - z(0) = \sum^N_{i=1} \varepsilon_i \sqrt{\Delta t}
$$&lt;/p&gt;
&lt;p&gt;当\(\Delta t \to 0\)时，可以得到极限的布朗运动：&lt;/p&gt;
&lt;p&gt;$$
dz = \varepsilon \sqrt{dt}
$$&lt;/p&gt;
&lt;h4 id=&#34;普通布朗运动&#34;&gt;普通布朗运动&lt;/h4&gt;
&lt;p&gt;引入两个概念：漂移率（Drift Rate），表示单位时间内\(z\)均值的变化值；方差率（Variance Rate），表示单位时间内的方差。令漂移率的期望为\(a\)，方差率的期望为\(b^2\)，可以得到变量\(x\)的普通布朗运动：&lt;/p&gt;
&lt;p&gt;$$
dx = adt + bdz = adt + b\varepsilon \sqrt{dt}
$$&lt;/p&gt;
&lt;p&gt;其中\(dz\)遵循标准布朗运动。&lt;/p&gt;
&lt;h3 id=&#34;伊藤过程&#34;&gt;伊藤过程&lt;/h3&gt;
&lt;p&gt;将普通布朗运动中的\(a\)和\(b\)当作随时间\(t\)和状态\(x\)的函数，可以得到伊藤过程（Ito Process）：&lt;/p&gt;
&lt;p&gt;$$
dx = a(x,t)dt + b(x,t)dz
$$&lt;/p&gt;
&lt;h2 id=&#34;证券价格的变动过程&#34;&gt;证券价格的变动过程&lt;/h2&gt;
&lt;p&gt;&lt;del&gt;终于又回到证券上了。&lt;/del&gt;（无收益）证券价格的变化可以用漂移率为\(\mu S\)、方差率为\((\sigma S)^2\)的伊藤过程表示：&lt;/p&gt;
&lt;p&gt;$$
dS = \mu Sdt + \sigma Sdz
$$&lt;/p&gt;
&lt;p&gt;两边同除以\(S\)有：&lt;/p&gt;
&lt;p&gt;$$
\frac{dS}{S} = \mu dt + \sigma dz
$$&lt;/p&gt;
&lt;p&gt;其中\(S\)表示证券价格，\(\mu\)表示单位时间内以连续复利表示的预期收益率，\(\sigma^2\)表示证券收益率在单位时间内的方差，\(\sigma\)简称证券价格的波动率（Volatility）。&lt;/p&gt;
&lt;p&gt;在短时间\(\Delta t\)后，证券价格比率的变化值\(\frac{\Delta S}{S}\)为：&lt;/p&gt;
&lt;p&gt;$$
\frac{dS}{S} = \mu \Delta t + \varepsilon \sigma \sqrt{\Delta t} \sim N(\mu \Delta t, \sigma \sqrt{\Delta t})
$$&lt;/p&gt;
&lt;h3 id=&#34;伊藤引理&#34;&gt;伊藤引理&lt;/h3&gt;
&lt;p&gt;伊藤进一步推导出（过程查论文吧）：若变量\(x\)遵循伊藤过程，则变量\(x\)和\(t\)的函数\(G\)遵循下列过程：&lt;/p&gt;
&lt;p&gt;$$
dG = (\frac{\partial G}{\partial x}a + \frac{\partial G}{\partial t} + \frac{\partial^2 G}{2\partial x^2}b^2)dt + \frac{\partial G}{\partial x}bdz
$$&lt;/p&gt;
&lt;p&gt;比较上面的伊藤过程，可以发现函数\(G\)是遵循伊藤过程的，漂移率：\(\frac{\partial G}{\partial x}a + \frac{\partial G}{\partial t} + \frac{\partial^2 G}{2\partial x^2}b^2\)，方差率：\((\frac{\partial G}{\partial x})^2b^2\)。根据上面的推理，我们有：&lt;/p&gt;
&lt;p&gt;$$
dS = \mu Sdt + \sigma Sdz
$$&lt;/p&gt;
&lt;p&gt;我们知道，衍生证券的价格是标的证券价格和时间的函数。根据伊藤引理，衍生证券的价格\(G\)应遵循过程：&lt;/p&gt;
&lt;p&gt;$$
dG = (\frac{\partial G}{\partial S}\mu S + \frac{\partial G}{\partial t} + \frac{\partial^2 G}{2\partial S^2}\sigma^2S^2)dt + \frac{\partial G}{\partial S}\sigma Sdz
$$&lt;/p&gt;
&lt;p&gt;比较前面两个式子可以知道，\(S\)和\(G\)都受同一个不确定性来源\(dz\)的影响，这个特点十分重要。&lt;/p&gt;
&lt;h2 id=&#34;证券价格的自然对数变化过程&#34;&gt;证券价格的自然对数变化过程&lt;/h2&gt;
&lt;p&gt;令\(G=\ln S\)，代入上式，则：&lt;/p&gt;
&lt;p&gt;$$
dG = (\mu - \frac{\sigma^2}{2})dt+\sigma dz
$$&lt;/p&gt;
&lt;p&gt;可以发现证券价格对数\(G\)遵循&lt;strong&gt;普通布朗运动&lt;/strong&gt;，有恒定的漂移率\(\mu - \frac{\sigma^2}{2}\)和方差率\(\sigma^2\)！&lt;/p&gt;
&lt;p&gt;令\(t\)时刻的\(G\)值为\(\ln S\)，\(T\)时刻的\(G\)值为\(\ln S_T\)，则\(T - t\)时间内\(G\)的变化为：&lt;/p&gt;
&lt;p&gt;$$
\ln S_T - \ln S \sim N[(\mu - \frac{\sigma^2}{2})(T-t), \sigma \sqrt{T-t}]
$$&lt;/p&gt;
&lt;p&gt;因为\(\ln S\)是定值，所以可以得到：&lt;/p&gt;
&lt;p&gt;$$
\ln S_T \sim N[\ln S + (\mu - \frac{\sigma^2}{2})(T-t), \sigma \sqrt{T-t}]
$$&lt;/p&gt;
&lt;p&gt;这表明\(S_T\)服从对数正态分布，证券价格对数的不确定性（标准差）与时间长度的平方根成正比。&lt;/p&gt;
&lt;h2 id=&#34;布莱克-舒尔斯微分方程&#34;&gt;布莱克-舒尔斯微分方程&lt;/h2&gt;
&lt;p&gt;由于衍生证券价格和标的证券价格都受同一种不确定性影响（\(dz\)），若匹配适当，这种不确定性可以相互抵消。因此布莱克和舒尔斯建立了一个包括一单位衍生证券空头和若干单位标的证券多头的投资组合。若数量适当，衍生和标的证券的盈利和亏损是可以抵消的，短时间内该投资组合无风险。在&lt;strong&gt;无套利机会&lt;/strong&gt;的情况下，该投资组合的短期收益率一定等于无风险利率。&lt;/p&gt;
&lt;p&gt;推导布莱克-舒尔斯微分方程需要以下假设：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;证券价格遵循几何布朗运动（可以放松为伊藤过程）&lt;/li&gt;
&lt;li&gt;允许卖空标的证券&lt;/li&gt;
&lt;li&gt;没有交易费用和税收&lt;/li&gt;
&lt;li&gt;所有证券都是完全可分的&lt;/li&gt;
&lt;li&gt;在衍生证券的有效期内的标的证券没有收益&lt;/li&gt;
&lt;li&gt;不存在无风险套利的机会&lt;/li&gt;
&lt;li&gt;证券交易和价格变动都是连续的&lt;/li&gt;
&lt;li&gt;在衍生证券有效期内，无风险利率\(r\)为常数&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由假设1，有：&lt;/p&gt;
&lt;p&gt;$$
dS = \mu Sdt + \sigma Sdz
$$&lt;/p&gt;
&lt;p&gt;在时间间隔\(\Delta t\)中，&lt;/p&gt;
&lt;p&gt;$$
\Delta S = \mu S\Delta t + \sigma S\Delta z
$$&lt;/p&gt;
&lt;p&gt;假设\(f\)是依赖于\(S\)的衍生证券的价格，则\(f\)是\(S\)和\(t\)的函数。由伊藤引理可得：&lt;/p&gt;
&lt;p&gt;$$
df = (\frac{\partial f}{\partial S}\mu S + \frac{\partial f}{\partial t} + \frac{\partial^2 f}{2\partial S^2}\sigma^2S^2)dt + \frac{\partial f}{\partial S}\sigma Sdz
$$&lt;/p&gt;
&lt;p&gt;在时间间隔\(\Delta t\)中，&lt;/p&gt;
&lt;p&gt;$$
\Delta f = (\frac{\partial f}{\partial S}\mu S + \frac{\partial f}{\partial t} + \frac{\partial^2 f}{2\partial S^2}\sigma^2S^2)\Delta t + \frac{\partial f}{\partial S}\sigma S\Delta z
$$&lt;/p&gt;
&lt;p&gt;其中\(\Delta z = \varepsilon \sqrt{\Delta t}\)。为了消除\(\Delta z\)，构建一个包括一单位衍生证券空头和\(\frac{\partial f}{\partial S}\)单位标的证券多头的组合。令\(\Pi\)代表该投资组合的价值，则：&lt;/p&gt;
&lt;p&gt;$$
\Pi = -f + \frac{\partial f}{\partial S}S
$$&lt;/p&gt;
&lt;p&gt;在时间间隔\(\Delta t\)中，&lt;/p&gt;
&lt;p&gt;$$
\Delta \Pi = -\Delta f + \frac{\partial f}{\partial S}\Delta S
$$&lt;/p&gt;
&lt;p&gt;代入\(\Delta S\)和\(\Delta f\)，可得：&lt;/p&gt;
&lt;p&gt;$$
\Delta \Pi = (-\frac{\partial f}{\partial S} - \frac{\partial^2 f}{2\partial S^2}\sigma^2S^2)\Delta t
$$&lt;/p&gt;
&lt;p&gt;不含有\(\Delta z\)，所以该组合在\(\Delta t\)内没有风险。因为不存在无风险套利的机会，所以在\(\Delta t\)内的瞬时收益率一定等于\(\Delta t\)中的无风险收益率。因此：&lt;/p&gt;
&lt;p&gt;$$
\Delta \Pi = r \Pi \Delta t
$$&lt;/p&gt;
&lt;p&gt;代入\(\Delta \Pi\)和\(\Pi\)，得：&lt;/p&gt;
&lt;p&gt;$$
(\frac{\partial f}{\partial t} + \frac{\partial^2 f}{2\partial S^2}\sigma^2S^2)\Delta t = r (f - \frac{\partial f}{\partial S}S \Delta t
$$&lt;/p&gt;
&lt;p&gt;化简得：&lt;/p&gt;
&lt;p&gt;$$
\frac{\partial f}{\partial t} + rS\frac{\partial f}{\partial S} + \frac{1}{2}\sigma^2S^2\frac{\partial^2 f}{\partial S^2} = rf
$$&lt;/p&gt;
&lt;p&gt;这就是著名的布莱克-舒尔斯微分方程，适用于其价格取决于标的证券价格\(S\)的所有衍生证券的定价。&lt;/p&gt;
&lt;h2 id=&#34;布莱克-舒尔斯期权定价公式&#34;&gt;布莱克-舒尔斯期权定价公式&lt;/h2&gt;
&lt;p&gt;在风险中性的条件下，&lt;strong&gt;欧式&lt;/strong&gt;看涨期权到期时（\(T\)）的期望值为：&lt;/p&gt;
&lt;p&gt;$$
\hat E[max(S_T-X, 0)]
$$&lt;/p&gt;
&lt;p&gt;根据风险中性定价原理，欧式看涨期权的价格\(c\)等于将此期望值按无风险利率进行贴现后的现值：&lt;/p&gt;
&lt;p&gt;$$
c = e^{-r(T-t)} \hat E[max(S_T-X, 0)]
$$&lt;/p&gt;
&lt;p&gt;根据前面，\(\ln S_T\)符合分布&lt;/p&gt;
&lt;p&gt;$$
\ln S_T \sim N[\ln S + (\mu - \frac{\sigma^2}{2})(T-t), \sigma \sqrt{T-t}]
$$&lt;/p&gt;
&lt;p&gt;在风险中性条件下，我们可以用\(r\)取代\(\mu\)，即：&lt;/p&gt;
&lt;p&gt;$$
\ln S_T \sim N[\ln S + (r - \frac{\sigma^2}{2})(T-t), \sigma \sqrt{T-t}]
$$&lt;/p&gt;
&lt;p&gt;对\(c\)式右边求值是一种积分过程（过程略），结果为：&lt;/p&gt;
&lt;p&gt;$$
c=SN(d_1)-Xe^{-r(T-t)}N(d_2)
$$&lt;/p&gt;
&lt;p&gt;其中，&lt;/p&gt;
&lt;p&gt;$$
d_1 = \frac{\ln \frac{S}{X} + (r+\frac{\sigma^2}{2})(T-t)}{\sigma \sqrt{T-t}}
$$
$$
d_2 = \frac{\ln \frac{S}{X} + (r-\frac{\sigma^2}{2})(T-t)}{\sigma \sqrt{T-t}}
$$&lt;/p&gt;
&lt;p&gt;\(N(x)\)是标准正态分布变量的累计概率分布函数。&lt;/p&gt;
&lt;p&gt;这就是无收益资产欧式看涨期权的定价公式。&lt;/p&gt;
&lt;p&gt;由&lt;a class=&#34;link&#34; href=&#34;https://blog.xpgreat.com/post/put_call_parity/&#34; &gt;售出-购进平价理论&lt;/a&gt;可以进一步导出欧式看跌期权的定价公式：&lt;/p&gt;
&lt;p&gt;$$
p=-SN(-d_1)+Xe^{-r(T-t)}N(-d_2)
$$&lt;/p&gt;
&lt;p&gt;美式看涨期权不会提前行权，所以其定价与欧式看涨期权一致。而由于美式看跌期权与看涨期权之间不存在严密的平价关系，因此美式看跌期权的代价还没有得到一个精确的解析公式，但可用蒙特卡罗模拟、二叉树和有限差分三种数值方法以及解析近似方法求出。&lt;/p&gt;
&lt;h2 id=&#34;有收益资产的期权定价公式&#34;&gt;有收益资产的期权定价公式&lt;/h2&gt;
&lt;p&gt;在收益已知的情况下，可以把标的证券的价格分解成两个部分：期权有效期内已知的现金收益的现值部分，和一个有风险部分。当期权到期时，这部分现值将由于标的资产支付现金收益而消失。因此，我们只要用\(S\)表示有风险部分的证券价格，\(\sigma\)表示风险部分遵循随机过程的波动率，就能直接套用公式了。&lt;/p&gt;
&lt;p&gt;当标的证券的已知收益的现值为\(I\)时，需要用\((S-I)\)代替公式的\(S\)。&lt;/p&gt;
&lt;p&gt;当标的证券的收益为按连续复利计算的固定收益率\(q\)时，需要用\(Se^{-q(T-t)}\)代替公式的\(S\)即可。&lt;/p&gt;</description>
        </item>
        <item>
        <title>售出-购进平价理论</title>
        <link>https://blog.xpgreat.com/p/put_call_parity/</link>
        <pubDate>Fri, 07 Dec 2018 21:48:39 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/put_call_parity/</guid>
        <description>&lt;p&gt;售出-购进平价理论（Put-Call Parity）指在无套利原则下（Non-Arbitrage），欧式看涨期权（Call Option）和欧式看跌期权（Put Option）定价之间存在的基本关系。&lt;/p&gt;
&lt;h2 id=&#34;看涨看跌期权&#34;&gt;看涨/看跌期权&lt;/h2&gt;
&lt;p&gt;看涨期权，Call Option，也被称为买进期权，买方期权，买权，延买期权或“敲进”（Knock In），是指期权的购买者拥有在期权合约规定有效期内按照&lt;strong&gt;执行价格&lt;/strong&gt;&lt;em&gt;购买&lt;/em&gt; 一定数量的&lt;strong&gt;标的物&lt;/strong&gt;的权利。&lt;/p&gt;
&lt;p&gt;看跌期权，Put Option，也被称为认沽期权、卖出期权、出售期权、卖权选择权、卖方期权、卖权、延卖期权或“敲出”（Knock Out），是指期权的购买者拥有在期权合约规定有效期内按照&lt;strong&gt;执行价格&lt;/strong&gt;&lt;em&gt;卖出&lt;/em&gt; 一定数量的&lt;strong&gt;标的物&lt;/strong&gt;的权利。&lt;/p&gt;
&lt;p&gt;举个例子，买家以20元购买了卖家的看&lt;strong&gt;涨&lt;/strong&gt;期权，约定在一年后执行，执行价格为1500元，标的物是一吨铜。一年后铜的价格&lt;strong&gt;涨&lt;/strong&gt;至2000元一吨，买家行使期权，以执行价格1500从卖家购买一吨铜（价值2000），获利2000-1500-20*K（K为贴现系数），卖家亏损2000-1500-20*K。如果一年后铜的价格&lt;strong&gt;跌&lt;/strong&gt;至1300元一吨， 买家不行使期权，亏损20*K，卖家获利20*K。&lt;/p&gt;
&lt;p&gt;改动一下，买家以20元购买了卖家的看&lt;strong&gt;跌&lt;/strong&gt;期权，约定在一年后执行，执行价格为1500元，标的物是一吨铜。一年后铜的价格&lt;strong&gt;跌&lt;/strong&gt;至1300元一吨，买家行使期权，以执行价格1500向卖家售出一吨铜（价值1300），获利1500-1300-20*K（K为贴现系数），卖家亏损1500-1300-20*K。如果一年后铜的价格&lt;strong&gt;涨&lt;/strong&gt;至2000元一吨， 买家不行使期权，亏损20*K，卖家获利20*K。&lt;/p&gt;
&lt;p&gt;推广可得，在期权行使时间看涨期权的获利/亏损可以表示为：&lt;/p&gt;
&lt;p&gt;$$
\max (0, S_T - E) - q^np
$$&lt;/p&gt;
&lt;p&gt;其中，\(S_T\)是现价，\(E\)是执行价格，\(q\)是利率，\(p\)是购买权力费用，\(n = T-t\)是执行时间。&lt;/p&gt;
&lt;p&gt;看跌期权的获利/亏损则为：&lt;/p&gt;
&lt;p&gt;$$
\max (0, E - S_T) - q^np
$$&lt;/p&gt;
&lt;p&gt;由此可见，期权买方有权利无义务，风险有限，最大损失是权利金，对于看涨期权，理论获利无上限，对于看跌期权，获利是有限的。而期权卖方有义务无权利，获利是有限的，而对于看涨期权，风险无限，对于看跌期权，风险有限。&lt;/p&gt;
&lt;p&gt;另外，期权分为美式期权和欧式期权，区别在于美式期权可以在到期日前任何时间行使权力，而欧式期权只能在到期日行使权力。但欧式期权可以在任何时间转让期权。&lt;/p&gt;
&lt;h2 id=&#34;售出-购进平价理论&#34;&gt;售出-购进平价理论&lt;/h2&gt;
&lt;p&gt;考虑两个投资组合：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;买入一份欧式看涨期权，卖出一份欧式看跌期权，执行价格\(E\)，无股息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;买入一单位标的物股权，借入\(\frac{E}{q}\)债券。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于1，如果到期标的物价格下跌，不行使看涨期权，看跌期权买家行使权力，需要以执行价格\(E(&amp;gt;S_T)\)买入；如果价格上涨，行使看涨期权，看跌期权买家不行使权力，以执行价格\(E(&amp;lt;S_T)\)买入，两个情况结果一致，相当于支付价值\(S_T-E\)。&lt;/p&gt;
&lt;p&gt;对于2，期权到期（\(T\)）时，一单位标的物股权和债券的支付价值也为\(S_T - q\frac{E}{q} = S_T - E\)。&lt;/p&gt;
&lt;p&gt;基于无套利原则，在\(T\)时两个投资组合的支付价值一致，在之前的每个时间的价值也应该是一致的。所以在初始状态\(t\)时，有：&lt;/p&gt;
&lt;p&gt;$$
c-p=S_t-\frac{E}{q}
$$&lt;/p&gt;
&lt;p&gt;其中\(c\)，\(p\)分别表示看跌/看涨期权的现价，\(S_t\)标的物的现价，\(E\)执行价格，\(q\)利率。&lt;/p&gt;
&lt;h2 id=&#34;美式期权的提前行权&#34;&gt;美式期权的提前行权&lt;/h2&gt;
&lt;p&gt;美式期权可以在到期日前任何时间行使权力。根据售出-购进平价理论有：&lt;/p&gt;
&lt;p&gt;$$
c=p+S_t-\frac{E}{q^n} \geq S_t-\frac{E}{q^n} \geq S_t-E
$$&lt;/p&gt;
&lt;p&gt;因为现价永远大于执行价格，所以提前行使期权不值得。美式看涨期权基本不可能提前行权。这样可以得出美式看涨期权的定价和欧式一致。&lt;/p&gt;
&lt;p&gt;而：&lt;/p&gt;
&lt;p&gt;$$
p=c-S_t+\frac{E}{q^n} \ ? \  E - S_t
$$&lt;/p&gt;
&lt;p&gt;大小关系未知，所以美式看跌期权有可能提前行权。&lt;/p&gt;</description>
        </item>
        <item>
        <title>在Hugo博客中加入代码高亮</title>
        <link>https://blog.xpgreat.com/p/hugo_add_highlight/</link>
        <pubDate>Wed, 05 Dec 2018 19:31:24 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/hugo_add_highlight/</guid>
        <description>&lt;p&gt;相信自己搭建博客的人中有一大半都是玩代码的，所以代码高亮可以说是博客的必备功能。本文提供一个在基于Hugo的博客上使用&lt;code&gt;highlight.js&lt;/code&gt;的代码高亮方案。&lt;/p&gt;
&lt;h2 id=&#34;实施步骤&#34;&gt;实施步骤&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;在&lt;a class=&#34;link&#34; href=&#34;https://highlightjs.org/download/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;highlight.js的官方网站&lt;/a&gt;上复制HTML的&lt;code&gt;link&lt;/code&gt;和&lt;code&gt;script&lt;/code&gt;标签。例如：&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/styles/default.min.css&amp;quot;&amp;gt;
&amp;lt;script src=&amp;quot;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;打开&lt;code&gt;themes/yourtheme/layouts/_default&lt;/code&gt;目录，打开&lt;code&gt;baseof.html&lt;/code&gt;，（可能对不同主题文件和路径会有不同，如果找不到可以试试查找&lt;code&gt;head.html&lt;/code&gt;或&lt;code&gt;header.html&lt;/code&gt;。这个html文件是网页的head部分的模板。在合适的地方粘贴第一步中的代码。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;大功告成。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;
&lt;p&gt;用&lt;!-- raw HTML omitted --&gt;```&lt;!-- raw HTML omitted --&gt;包裹代码块，保险起见，在&lt;!-- raw HTML omitted --&gt;```&lt;!-- raw HTML omitted --&gt;后加上语言名字。例如（不包括方括号内）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;[START HERE]
```c
int hash(char * str, int length) { // hash function
    int hash = 0;
    for (int i = 0; i &amp;lt; length; i++) {
        hash = ((hash + str[i]) * 31) % MAX_ID; // maximum of ID?
    }
    return hash;
}
```[END HERE]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;显示效果如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;hash&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// hash function
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hash&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;length&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;hash&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hash&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MAX_ID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// maximum of ID?
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hash&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;注意事项&#34;&gt;注意事项&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;如果要使用的语言很生僻，链接内的js和css可能无法满足需求，可以添加新的script，比如：&lt;code&gt;https://cdn.bootcss.com/highlight.js/9.12.0/languages/yaml.min.js&lt;/code&gt;。或者在&lt;a class=&#34;link&#34; href=&#34;https://highlightjs.org/download/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;highlight.js的官方网站&lt;/a&gt;上自定义并下载下来，用本地引用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果需要修改颜色、背景色等样式，可以把css下载下来，修改后本地引用。修改后的css放在&lt;code&gt;themes/yourtheme/static/css&lt;/code&gt;里，用&lt;code&gt;href=&amp;quot;/css/highlight.css&amp;quot;&lt;/code&gt;引用。&lt;strong&gt;强烈建议&lt;/strong&gt;在css里把背景色去除。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</description>
        </item>
        <item>
        <title>Markdown里图片并列显示</title>
        <link>https://blog.xpgreat.com/p/md_display_img_parallel/</link>
        <pubDate>Wed, 05 Dec 2018 17:42:53 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/md_display_img_parallel/</guid>
        <description>&lt;p&gt;在编辑博文的时候，有时会想把两张图片并列在一起显示，参考了网上内容，分享一下。&lt;/p&gt;
&lt;p&gt;首先记住一点，在markdown里是可以直接写html代码的。这个前提下很自然的有下面的方法：&lt;/p&gt;
&lt;h2 id=&#34;调整图片宽度高度&#34;&gt;调整图片宽度/高度：&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;img src=&amp;quot;http://xxx.jpg&amp;quot; title=&amp;quot;xxx&amp;quot; width=&amp;quot;100&amp;quot; height=&amp;quot;100&amp;quot;/&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;宽度是 &lt;code&gt;width&lt;/code&gt;，高度是 &lt;code&gt;height&lt;/code&gt;，&lt;code&gt;title&lt;/code&gt; 为图片描述。&lt;/p&gt;
&lt;h2 id=&#34;单张居中显示&#34;&gt;单张居中显示：&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;center&amp;gt;
    &amp;lt;img src=&amp;quot;http://example.com/xxx.png&amp;quot;&amp;gt;
&amp;lt;/center&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;figure&amp;gt;
    &amp;lt;img src=&amp;quot;http://xxx.jpg&amp;quot;&amp;gt;
&amp;lt;/figure&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;两张并排显示&#34;&gt;两张并排显示：&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;figure class=&amp;quot;half&amp;quot;&amp;gt;
    &amp;lt;img src=&amp;quot;http://xxx.jpg&amp;quot;&amp;gt;
    &amp;lt;img src=&amp;quot;http://xxx.jpg&amp;quot;&amp;gt;
&amp;lt;/figure&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;三张并排显示&#34;&gt;三张并排显示：&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;figure class=&amp;quot;third&amp;quot;&amp;gt;
    &amp;lt;img src=&amp;quot;http://xxx.jpg&amp;quot;&amp;gt;
    &amp;lt;img src=&amp;quot;http://xxx.jpg&amp;quot;&amp;gt;
    &amp;lt;img src=&amp;quot;http://xxx.jpg&amp;quot;&amp;gt;
&amp;lt;/figure&amp;gt;
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>Hugo和WordPress比较</title>
        <link>https://blog.xpgreat.com/p/hugo_wp/</link>
        <pubDate>Wed, 05 Dec 2018 17:38:52 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/hugo_wp/</guid>
        <description>&lt;p&gt;对比以前搭建WordPress博客的经历，Hugo有这样的一些不同：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Hugo生成的是&lt;strong&gt;静态页面&lt;/strong&gt;，所以没有内置的评论、搜索、登陆等等一系列的需要后台的功能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;没有WordPress上那么多五花八门的插件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;主题偏少，但足够。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用起来有些门槛，比如必须会markdown，最好有一点HTML、 CSS的知识。但比起WordPress，学习起来快很多。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;备份起来比WordPress简单（？），WordPress得用插件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;管理简单，&lt;del&gt;因为只有基本的功能&lt;/del&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以如果要搭建个人博客，Hugo十分足够。如果是喜欢图形化的编辑，喜欢尝试各种各样的主题插件（&lt;del&gt;喜新厌旧&lt;/del&gt;），需要能方便的互动的，推荐WordPress。如果搭建组织网站，比如社团网站，需要多人、多层管理维护的，上WordPress，不用考虑Hugo了。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>使用Hugo的一些注意事项</title>
        <link>https://blog.xpgreat.com/p/tips_hugo/</link>
        <pubDate>Wed, 05 Dec 2018 16:32:54 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/tips_hugo/</guid>
        <description>&lt;p&gt;一些在使用Hugo时发现的小技巧或问题及解决方法，会持续更新。&lt;/p&gt;
&lt;h2 id=&#34;如何插入本地图片&#34;&gt;如何插入本地图片？&lt;/h2&gt;
&lt;p&gt;一直不知道怎样插入本地图片，查找后得知，在&lt;code&gt;site&lt;/code&gt;目录下的&lt;code&gt;static&lt;/code&gt;目录就是存放静态文件的地方，可以在下面创建一个&lt;code&gt;media&lt;/code&gt;目录，用于保存图片等媒体文件，引用的话，使用&lt;code&gt;/media/123.png&lt;/code&gt;即可。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：不要使用大写的后缀名，如&lt;code&gt;123.PNG&lt;/code&gt;，生成的静态页面引用的是小写后缀，会出现找不到文件。&lt;/p&gt;
&lt;h2 id=&#34;搜索引擎无法搜索到博客内容&#34;&gt;搜索引擎无法搜索到博客内容。&lt;/h2&gt;
&lt;p&gt;这是因为搜索引擎还没有收录我们的URL，可以在搜索引擎提交一下自己的网址，比如&lt;a class=&#34;link&#34; href=&#34;https://www.google.com/webmasters/tools/home&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;谷歌&lt;/a&gt;，并过一段时间再试。&lt;/p&gt;
&lt;h2 id=&#34;hugo不支持站内搜索和内置评论&#34;&gt;Hugo不支持站内搜索和内置评论。&lt;/h2&gt;
&lt;p&gt;这是因为Hugo生成的是静态网站，没有服务器后台，没有数据库，所以当然不能搜索和评论啦。如果你的网站被搜索引擎收录了，可以使用搜索引擎的限制搜索，比如&lt;code&gt;serchword site:https://yoursite.com/&lt;/code&gt;，有的主题提供搜索框工具，基本也是靠这种方法。评论可以使用外置的评论系统，比如&lt;code&gt;Discus&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;git-push的时候会发生冲突&#34;&gt;Git push的时候会发生冲突。&lt;/h2&gt;
&lt;p&gt;可能是在GitHub上配置自定义页面的时候（我就是在这时遇到的），repository里创建了新的文件（我碰到的是CNAME文件）而本地没有。解决方法是先&lt;code&gt;pull&lt;/code&gt;再&lt;code&gt;push&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;hugo报错&#34;&gt;Hugo报错。&lt;/h2&gt;
&lt;p&gt;肯定是在配置文件或者是文章头部配置有问题，具体查看报错信息进行修改即可，Hugo的报错做的不错，很好理解。&lt;/p&gt;
&lt;h2 id=&#34;访问的时候浏览器提示不安全&#34;&gt;访问的时候浏览器提示“不安全”。&lt;/h2&gt;
&lt;p&gt;这是因为没有启用HTTPS，如果是托管在GitHub上的话可以使用GitHub提供的HTTPS福利。开启步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;找到处于不可选状态的Enforce HTTPS选项，旁边会提示 &lt;em&gt;Unavailable for your site because your domain is not properly configured to support HTTPS&lt;/em&gt; 。&lt;/li&gt;
&lt;li&gt;将填在Custom domain里的自定义域名清空，保存，然后重新填上自定义域名，再保存。&lt;/li&gt;
&lt;li&gt;现在可以勾选Enforce HTTPS选项了，这时会提示正在签发证书: &lt;em&gt;Not yet available for your site because the certificate has not finished being issued&lt;/em&gt; 。&lt;/li&gt;
&lt;li&gt;证书签发成功后，可以使用 https 链接访问自定义域名了。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;dns配置完成后还不能访问页面或者有的设备可以访问而有的不行&#34;&gt;DNS配置完成后，还不能访问页面，或者有的设备可以访问，而有的不行。&lt;/h2&gt;
&lt;p&gt;这是因为DNS需要一定时间传播，等待一段时间就好了。有兴趣可以阅读一下DNS的&lt;a class=&#34;link&#34; href=&#34;https://zh.wikipedia.org/wiki/%E5%9F%9F%E5%90%8D%E7%B3%BB%E7%BB%9F&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;维基百科&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;svg绘图标&#34;&gt;svg绘图标&lt;/h2&gt;
&lt;p&gt;左边栏的Utils图标代码：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;svg xmlns=&amp;quot;http://www.w3.org/2000/svg&amp;quot; class=&amp;quot;icon icon-tabler icon-tabler-search&amp;quot; width=&amp;quot;24&amp;quot; height=&amp;quot;24&amp;quot; viewBox=&amp;quot;0 0 24 24&amp;quot; stroke-width=&amp;quot;2&amp;quot; stroke=&amp;quot;currentColor&amp;quot; fill=&amp;quot;none&amp;quot; stroke-linecap=&amp;quot;round&amp;quot; stroke-linejoin=&amp;quot;round&amp;quot;&amp;gt;
	&amp;lt;path stroke=&amp;quot;none&amp;quot; d=&amp;quot;M0 0h24v24H0z&amp;quot;&amp;gt;&amp;lt;/path&amp;gt;
	&amp;lt;line x1=&amp;quot;10&amp;quot; y1=&amp;quot;18&amp;quot; x2=&amp;quot;23&amp;quot; y2=&amp;quot;5&amp;quot;&amp;gt;&amp;lt;/line&amp;gt;
	&amp;lt;line x1=&amp;quot;10&amp;quot; y1=&amp;quot;18&amp;quot; x2=&amp;quot;3&amp;quot; y2=&amp;quot;9&amp;quot;&amp;gt;&amp;lt;/line&amp;gt;
&amp;lt;/svg&amp;gt;
&lt;/code&gt;&lt;/pre&gt;</description>
        </item>
        <item>
        <title>博客第一步</title>
        <link>https://blog.xpgreat.com/p/blog_building/</link>
        <pubDate>Mon, 03 Dec 2018 19:58:50 +0100</pubDate>
        
        <guid>https://blog.xpgreat.com/p/blog_building/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://gohugo.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hugo&lt;/a&gt;是一个用Go编写的静态站点生成器，由于具有丰富的主题资源和惊人的生成速度而备受青睐。博文采用&lt;code&gt;Markdown&lt;/code&gt;这一轻量的标记语言编写，速度快，美观。本博客即是基于Hugo搭建，下面讲述一下我的建站历程。&lt;/p&gt;
&lt;h2 id=&#34;安装hugo&#34;&gt;安装Hugo&lt;/h2&gt;
&lt;p&gt;如果你是&lt;code&gt;macOS&lt;/code&gt;用户，请使用&lt;code&gt;Homebrew&lt;/code&gt;快速安装&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;brew install hugo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果你是&lt;code&gt;Windows&lt;/code&gt;用户，请使用&lt;code&gt;Chocolatey&lt;/code&gt;或者&lt;code&gt;Scoop&lt;/code&gt;快速安装，取决于你使用什么包管理&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;choco install hugo -confirm
scoop install hugo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果你是&lt;code&gt;Debian&lt;/code&gt;或&lt;code&gt;Ubuntu&lt;/code&gt;用户，请使用&lt;code&gt;apt&lt;/code&gt;快速安装&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo apt-get install hugo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;基本上使用单行命令都可以成功安装Hugo，具体请移步&lt;a class=&#34;link&#34; href=&#34;https://gohugo.io/getting-started/installing&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;生成第一篇文章&#34;&gt;生成第一篇文章&lt;/h2&gt;
&lt;p&gt;使用如下命令新建一个名为“mysite”的网站：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;hugo new site mysite
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;接下来，在&lt;a class=&#34;link&#34; href=&#34;https://themes.gohugo.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;这里&lt;/a&gt;找到一个漂亮的网站主题。主题极其之多，找到一个满意的并不难。本博客采用的是&lt;a class=&#34;link&#34; href=&#34;https://github.com/Vimux/Mainroad/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Mainroad&lt;/a&gt;主题。如果对主题的配色之类的不满意，可以通过修改&lt;code&gt;style.css&lt;/code&gt;这一文件来达到想要的效果。&lt;/p&gt;
&lt;p&gt;以Mainroad为例，将主题&lt;code&gt;clone&lt;/code&gt;到本地的&lt;code&gt;themes&lt;/code&gt;文件夹内：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cd themes
git clone https://github.com/Vimux/Mainroad.git
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;mysite/content&lt;/code&gt;是用来存放文档的地方，我们在其下建立一个新的&lt;code&gt;Markdown&lt;/code&gt;文件：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;hugo new post/first.md
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在&lt;code&gt;first.md&lt;/code&gt;中写入一些内容，使用如下命令进行本地预览：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;hugo server -t mainroad -D
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;打开网址 &lt;a class=&#34;link&#34; href=&#34;http://localhost:1313/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://localhost:1313/&lt;/a&gt; 即可查看本地生成的静态网站。&lt;/p&gt;
&lt;h2 id=&#34;适配主题&#34;&gt;适配主题&lt;/h2&gt;
&lt;p&gt;Hugo的每个主题都会有不同的参数配置，而这些配置被存放在根目录下的&lt;code&gt;config.toml&lt;/code&gt;文件中，以Mainroad为例，&lt;a class=&#34;link&#34; href=&#34;https://github.com/Vimux/Mainroad/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;官方的GitHub&lt;/a&gt;里已经做了说明：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;baseurl = &amp;quot;/&amp;quot;
title = &amp;quot;Mainroad&amp;quot;
languageCode = &amp;quot;en-us&amp;quot;
paginate = &amp;quot;10&amp;quot; # Number of posts per page
theme = &amp;quot;mainroad&amp;quot;
disqusShortname = &amp;quot;&amp;quot; # Enable comments by entering your Disqus shortname
googleAnalytics = &amp;quot;&amp;quot; # Enable Google Analytics by entering your tracking id

[Author] # Used in authorbox
  name = &amp;quot;John Doe&amp;quot;
  bio = &amp;quot;John Doe&#39;s true identity is unknown. Maybe he is a successful blogger or writer. Nobody knows it.&amp;quot;
  avatar = &amp;quot;img/avatar.png&amp;quot;

[Params]
  subtitle = &amp;quot;Just another site&amp;quot; # Subtitle of your site. Used in site header
  description = &amp;quot;John Doe&#39;s Personal blog about everything&amp;quot; # Site description. Used in meta description
  #copyright = &amp;quot;John Doe&amp;quot; # copyright holder, otherwise will use site title
  opengraph = true # Enable OpenGraph if true
  twitter_cards = true # Enable Twitter Cards if true
  readmore = false # Show &amp;quot;Read more&amp;quot; button in list if true
  authorbox = true # Show authorbox at bottom of pages if true
  toc = true # Enable Table of Contents
  post_navigation = true # Show post navigation at bottom of pages if true
  # post_meta = [&amp;quot;date&amp;quot;, &amp;quot;categories&amp;quot;] # Order of post meta information. Use [&amp;quot;none&amp;quot;] to turn off completely.
  postSections = [&amp;quot;post&amp;quot;] # the section pages to show on home page and the &amp;quot;Recent articles&amp;quot; widget
  #postSections = [&amp;quot;blog&amp;quot;, &amp;quot;news&amp;quot;] # alternative that shows more than one section&#39;s pages
  #dateformat = &amp;quot;2006-01-02&amp;quot; # change the format of dates
  #mathjax = true # Enable MathJax
  #mathjaxPath = &amp;quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js&amp;quot; # Specify MathJax path
  #mathjaxConfig = &amp;quot;TeX-AMS-MML_HTMLorMML&amp;quot; # Specify MathJax config

[Params.sidebar]
  home = &amp;quot;right&amp;quot; # Configure layout for home page
  list = &amp;quot;left&amp;quot;  # Configure layout for list pages
  single = false # Configure layout for single pages
  # Enable widgets in given order
  widgets = [&amp;quot;search&amp;quot;, &amp;quot;recent&amp;quot;, &amp;quot;categories&amp;quot;, &amp;quot;taglist&amp;quot;, &amp;quot;social&amp;quot;]

[Params.widgets]
  recent_num = 5 # Set the number of articles in the &amp;quot;Recent articles&amp;quot; widget
  tags_counter = false # Enable counter for each tag in &amp;quot;Tags&amp;quot; widget (disabled by default)

[Params.widgets.social]
  # Enable parts of social widget
  facebook = &amp;quot;username&amp;quot;
  twitter = &amp;quot;username&amp;quot;
  instagram = &amp;quot;username&amp;quot;
  linkedin = &amp;quot;username&amp;quot;
  telegram = &amp;quot;username&amp;quot;
  github = &amp;quot;username&amp;quot;
  gitlab = &amp;quot;username&amp;quot;
  bitbucket = &amp;quot;username&amp;quot;
  email = &amp;quot;example@example.com&amp;quot;
  google_plus = &amp;quot;profileid&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在每一篇博文前也有一个&lt;code&gt;Header&lt;/code&gt;，官方提供的样例为：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;---
title: &amp;quot;Example article title&amp;quot;
date: &amp;quot;2017-08-21&amp;quot;
description: &amp;quot;Example article description&amp;quot;
thumbnail: &amp;quot;img/placeholder.jpg&amp;quot; # Optional, thumbnail
lead: &amp;quot;Example lead - highlighted near the title&amp;quot;
disable_comments: false # Optional, disable Disqus comments if true
authorbox: true # Optional, enable authorbox for specific post
toc: true # Optional, enable Table of Contents for specific post
mathjax: true # Optional, enable MathJax for specific post
categories:
  - &amp;quot;Category 1&amp;quot;
  - &amp;quot;Category 2&amp;quot;
tags:
  - &amp;quot;Test&amp;quot;
  - &amp;quot;Another test&amp;quot;
menu: main # Optional, add page to a menu. Options: main, side, footer
---
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以把&lt;code&gt;Header&lt;/code&gt;的内容复制到&lt;code&gt;mysite//archetypes/default.md&lt;/code&gt;中，这样新建新文档的时候才会适应你的主题的结构。如果在写草稿，为了不让它在博客里显示，需要在&lt;code&gt;Header&lt;/code&gt;里加入：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;draft: true
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;发布网站到github-pages&#34;&gt;发布网站到GitHub Pages&lt;/h2&gt;
&lt;p&gt;在使用&lt;code&gt;hugo server -D&lt;/code&gt;预览网站无误后可正式发布网站到域名供大家浏览。将要发布的文章内&lt;code&gt;draft&lt;/code&gt;改为&lt;code&gt;false&lt;/code&gt;后执行命令&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;hugo
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可看到根目录下多出&lt;code&gt;/public&lt;/code&gt;文件夹出来，该文件夹的内容即Hugo生成的整个静态网站。最终我们需要把这些静态网站的文件部署到一个地方，免费且稳定的GitHub Pages是一个很好的选择。具体操作如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在GitHub新建一个Repository命名为Example.github.io，其中Example改成自己的GitHub账户名；&lt;/li&gt;
&lt;li&gt;在/mysite外建立一个平行的文件夹，此处假设也命名为/Example.github.io；&lt;/li&gt;
&lt;li&gt;进入/public文件夹将内容复制到/Example.github.io；&lt;/li&gt;
&lt;li&gt;将/Example.github.io的内容push到远程仓库。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以上命令可在命令行通过以下语句实现：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;mkdir Example.github.io
cd mysite/public
cp -r . ../../Example.github.io
cd ../../Example.github.io
git init
git add .
git commit -m &amp;quot;commit message&amp;quot;
git remote add origin https://github.com/Example/Example.github.io.git
git push -u origin master
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;完成以上命令后，等待一分钟左右即可在 &lt;a class=&#34;link&#34; href=&#34;https://Example.github.io/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://Example.github.io/&lt;/a&gt; 访问你的网站。
以后每次更新文章后只用将生成的&lt;code&gt;/public&lt;/code&gt;文件夹的静态网站内容复制到&lt;code&gt;/Example.github.io&lt;/code&gt;，然后再&lt;code&gt;push&lt;/code&gt;到远程仓库即可。也可将步骤写为Shell脚本，此处不再赘述。&lt;/p&gt;
&lt;h2 id=&#34;使用自己的域名&#34;&gt;使用自己的域名&lt;/h2&gt;
&lt;p&gt;当然，GitHub的域名怎么能满足装*的心理，这时可以将网站设置为自己的域名。购买域名的地方很多，如国外知名网站GoDaddy，简体中文页面、支持支付宝付款、不用备案等都带来了很多方便。关于如何将域名定向到自己的页面，请&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/34426950&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;参阅&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;将网页托管到GitHub还有一个好处，可以设置强制https协议，这样你的网站就不会被识别成不安全啦。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Markdown&lt;/code&gt;可以用很多编辑器编辑，我使用的是&lt;a class=&#34;link&#34; href=&#34;https://www.sublimetext.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Sublime Text&lt;/a&gt;，十分好用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果嫌打开Bash麻烦可以把命令写成&lt;code&gt;.sh&lt;/code&gt;或&lt;code&gt;.bat&lt;/code&gt;文件方便操作。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
        </item>
        
    </channel>
</rss>
